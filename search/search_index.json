{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Warning Converting and updating this documentation to the new system is still a work in progress, and pages may change content or organisation as we work on it. Please bear with us while we complete the move. Research Computing Services \u00a7 This documentation is maintained by the research computing team for the purpose of sharing information about our services, including user guides, service updates and account request and renewal support. Documentation \u00a7 General User Information: Getting an Account Guide for New Users Guide for Experienced Users Planned Outages Clusters: Myriad Grace Kathleen Legion Thomas Young Michael Email Support \u00a7 For support for any of our services, contact us at: rc-support@ucl.ac.uk We will endeavour to answer queries on any aspect of computing related to your research whatever your skill level or requirements. How to... \u00a7 Connect to the Research Data Storage service Report a problem with one of our computing platforms Access services from outside UCL Apply for access to national GPU clusters Get more resources (more storage, more compute time, longer run time, etc) Training \u00a7 Due to the on-going COVID-19 pandemic, our ordinary training courses have been suspended indefinitely. We have an online Moodle course \"Introduction to the Unix Shell\" and infrequently provide a training course aimed at getting users up and running on one of our main clusters. Please see our Research IT Training page for details.","title":"Home"},{"location":"#research-computing-services","text":"This documentation is maintained by the research computing team for the purpose of sharing information about our services, including user guides, service updates and account request and renewal support.","title":"Research Computing Services"},{"location":"#documentation","text":"General User Information: Getting an Account Guide for New Users Guide for Experienced Users Planned Outages Clusters: Myriad Grace Kathleen Legion Thomas Young Michael","title":"Documentation"},{"location":"#email-support","text":"For support for any of our services, contact us at: rc-support@ucl.ac.uk We will endeavour to answer queries on any aspect of computing related to your research whatever your skill level or requirements.","title":"Email Support"},{"location":"#how-to","text":"Connect to the Research Data Storage service Report a problem with one of our computing platforms Access services from outside UCL Apply for access to national GPU clusters Get more resources (more storage, more compute time, longer run time, etc)","title":"How to..."},{"location":"#training","text":"Due to the on-going COVID-19 pandemic, our ordinary training courses have been suspended indefinitely. We have an online Moodle course \"Introduction to the Unix Shell\" and infrequently provide a training course aimed at getting users up and running on one of our main clusters. Please see our Research IT Training page for details.","title":"Training"},{"location":"Account_Services/","text":"Account Services \u00a7 Cluster access application \u00a7 There is a single online form for applying to the majority of our clusters. Use of these services is subject to a common set of terms and conditions . All granted applications give you a Myriad account while access to other systems depends on what types of work you tell us you will be doing. Account sponsors \u00a7 If you are a student or postdoctoral researcher, your application must be approved by a permanent member of staff (normally your supervisor or PI). This will be automatically determined when you begin your application, and when you submit it, an email will be sent to this person asking them to approve the application before the account can be created. Please note that the form requires your sponsor's UCL username and not their UPI. Permanent members of staff do not need a sponsor and their accounts will be automatically approved. Apply for an account \u00a7 The online application form can be found here: User account application form (UCL login required via Shibboleth) You will need a UCL user name and password. These are the same credentials used to access core services like Portico and HR systems. Application process \u00a7 The application process has these steps: Enter your UCL username and password to access the application form. Complete the application form, reading the instructions carefully. Tip: Hover your mouse over text boxes for more information. When you successfully submit the form, your sponsor will be sent an email asking them to approve your application. If you do not require a sponsor, your application will be automatically approved. Your sponsor should click on the link in the email, log in and approve the account. You will then receive an email when your account is approved if you have a sponsor. (This does not mean your account has been created yet). You should receive an email once we have created your account. Please note that there may be a delay of up to one working day between an application being approved and your account being created. If your sponsor does not receive the email to approve your account, send them the link to your application directly (it will look like dashboard.rc.ucl.ac.uk/computing/requests/xxxx) and they will have a button at the top to approve it. Please note: most delays in the application process are caused by sponsors missing or not receiving the approval email, so check with your sponsor first if your application is stuck waiting for approval. Research Computing can only create your account after it is approved. Accounts for visitors \u00a7 UCL visitors are welcome to apply for accounts on Research Computing services. Please note that: Applicants must have a central UCL account. UCL Visitor accounts can be arranged by your Departmental Administrator, so you should speak to them first. Account applications should specify the UCL grant under which the work is being carried out, if possible, as well as an associated UCL group or researcher. Account applications may not be submitted on behalf of another except to cover accessibility requirements, as the account application process includes agreeing to relevant legal terms and conditions. Accounts for honorary staff \u00a7 UCL Staff Members may nominate Honorary members (named individuals) to be provided with access to Research Computing services where this is beneficial to UCL's research interests. Nomination should be made via the CRAG , explaining the benefit arising to UCL. Proposals will be reviewed by the CRAG at their monthly meeting and on a case by case basis. All accounts thus provided are subject to all other 'standard' T&C's relating to their use of Research Computing services. Charges for use of Research Computing services \u00a7 Research Computing services are free at point of use by default. There are no direct charges for your usage under standard resource allocation policy as defined by the CRAG . Several methods are available to researchers who wish to gain access to additional resources, or obtain 'priority' use, including chargeable options. Details are available at Additional Resource_Requests and Purchasing in Myriad .","title":"Account Services"},{"location":"Account_Services/#account-services","text":"","title":"Account Services"},{"location":"Account_Services/#cluster-access-application","text":"There is a single online form for applying to the majority of our clusters. Use of these services is subject to a common set of terms and conditions . All granted applications give you a Myriad account while access to other systems depends on what types of work you tell us you will be doing.","title":"Cluster access application"},{"location":"Account_Services/#account-sponsors","text":"If you are a student or postdoctoral researcher, your application must be approved by a permanent member of staff (normally your supervisor or PI). This will be automatically determined when you begin your application, and when you submit it, an email will be sent to this person asking them to approve the application before the account can be created. Please note that the form requires your sponsor's UCL username and not their UPI. Permanent members of staff do not need a sponsor and their accounts will be automatically approved.","title":"Account sponsors"},{"location":"Account_Services/#apply-for-an-account","text":"The online application form can be found here: User account application form (UCL login required via Shibboleth) You will need a UCL user name and password. These are the same credentials used to access core services like Portico and HR systems.","title":"Apply for an account"},{"location":"Account_Services/#application-process","text":"The application process has these steps: Enter your UCL username and password to access the application form. Complete the application form, reading the instructions carefully. Tip: Hover your mouse over text boxes for more information. When you successfully submit the form, your sponsor will be sent an email asking them to approve your application. If you do not require a sponsor, your application will be automatically approved. Your sponsor should click on the link in the email, log in and approve the account. You will then receive an email when your account is approved if you have a sponsor. (This does not mean your account has been created yet). You should receive an email once we have created your account. Please note that there may be a delay of up to one working day between an application being approved and your account being created. If your sponsor does not receive the email to approve your account, send them the link to your application directly (it will look like dashboard.rc.ucl.ac.uk/computing/requests/xxxx) and they will have a button at the top to approve it. Please note: most delays in the application process are caused by sponsors missing or not receiving the approval email, so check with your sponsor first if your application is stuck waiting for approval. Research Computing can only create your account after it is approved.","title":"Application process"},{"location":"Account_Services/#accounts-for-visitors","text":"UCL visitors are welcome to apply for accounts on Research Computing services. Please note that: Applicants must have a central UCL account. UCL Visitor accounts can be arranged by your Departmental Administrator, so you should speak to them first. Account applications should specify the UCL grant under which the work is being carried out, if possible, as well as an associated UCL group or researcher. Account applications may not be submitted on behalf of another except to cover accessibility requirements, as the account application process includes agreeing to relevant legal terms and conditions.","title":"Accounts for visitors"},{"location":"Account_Services/#accounts-for-honorary-staff","text":"UCL Staff Members may nominate Honorary members (named individuals) to be provided with access to Research Computing services where this is beneficial to UCL's research interests. Nomination should be made via the CRAG , explaining the benefit arising to UCL. Proposals will be reviewed by the CRAG at their monthly meeting and on a case by case basis. All accounts thus provided are subject to all other 'standard' T&C's relating to their use of Research Computing services.","title":"Accounts for honorary staff"},{"location":"Account_Services/#charges-for-use-of-research-computing-services","text":"Research Computing services are free at point of use by default. There are no direct charges for your usage under standard resource allocation policy as defined by the CRAG . Several methods are available to researchers who wish to gain access to additional resources, or obtain 'priority' use, including chargeable options. Details are available at Additional Resource_Requests and Purchasing in Myriad .","title":"Charges for use of Research Computing services"},{"location":"Additional_Resource_Requests/","text":"Additional Resource Requests \u00a7 We recognise that researchers may sometimes require a higher throughput of work than it is possible to achieve with free \u2018fair share\u2019 usage of Myriad , Grace and Kathleen . There a couple of ways of obtaining additional Legion resource beyond this fair share: Make a special request to the CRAG for free access to additional resources \u00a7 Users who wish to request additional resources or reserve resources beyond those provided can complete the additional resource request form in collaboration with your supervisor or the project's principal investigator. This includes requests for increased storage quotas. The completed form should be sent to the Research Computing Platforms team at rc-support@ucl.ac.uk , for technical review. If successful, your case will be presented to the CRAG for consideration at the next meeting of the Group. The CRAG meets monthly, usually on the third Friday of the month, and users will be informed of the Group\u2019s decision as soon as possible after their next meeting. Note that an application to the CRAG for additional resources is only likely to be approved if the impact on other users is not deemed to be significant, or of long duration. Additional resource request form Request hosting of shared datasets \u00a7 We have provision for hosting shared datasets for users on Myriad. These can be datasets that are freely accessible by all users, or ones limited to groups. Hosted datasets: Will not be backed up. Must have a named primary contact. Must be reapplied for every 12 months to make sure they are still current and required. Will have an associated quota. Will be removed when renewal lapses (notice will be given). They are likely to be managed by a role account - access to the role account will be by ssh key. To apply for a hosted dataset, please send this form to rc-support@ucl.ac.uk . Hosted dataset request form Purchase dedicated compute nodes within Legion or Myriad \u00a7 There has previously been a programme allowing researchers to purchase compute nodes to be attached to the Legion cluster. This has been discontinued, as the Myriad cluster replaces the Legion cluster for the majority of users. There are plans to allow the purchase of nodes for Myriad in future but these have not yet been finalised. Further information \u00a7 For further advice or information on future hardware options, please contact rits@ucl.ac.uk .","title":"Additional Resource Requests"},{"location":"Additional_Resource_Requests/#additional-resource-requests","text":"We recognise that researchers may sometimes require a higher throughput of work than it is possible to achieve with free \u2018fair share\u2019 usage of Myriad , Grace and Kathleen . There a couple of ways of obtaining additional Legion resource beyond this fair share:","title":"Additional Resource Requests"},{"location":"Additional_Resource_Requests/#make-a-special-request-to-the-crag-for-free-access-to-additional-resources","text":"Users who wish to request additional resources or reserve resources beyond those provided can complete the additional resource request form in collaboration with your supervisor or the project's principal investigator. This includes requests for increased storage quotas. The completed form should be sent to the Research Computing Platforms team at rc-support@ucl.ac.uk , for technical review. If successful, your case will be presented to the CRAG for consideration at the next meeting of the Group. The CRAG meets monthly, usually on the third Friday of the month, and users will be informed of the Group\u2019s decision as soon as possible after their next meeting. Note that an application to the CRAG for additional resources is only likely to be approved if the impact on other users is not deemed to be significant, or of long duration. Additional resource request form","title":"Make a special request to the CRAG for free access to additional resources"},{"location":"Additional_Resource_Requests/#request-hosting-of-shared-datasets","text":"We have provision for hosting shared datasets for users on Myriad. These can be datasets that are freely accessible by all users, or ones limited to groups. Hosted datasets: Will not be backed up. Must have a named primary contact. Must be reapplied for every 12 months to make sure they are still current and required. Will have an associated quota. Will be removed when renewal lapses (notice will be given). They are likely to be managed by a role account - access to the role account will be by ssh key. To apply for a hosted dataset, please send this form to rc-support@ucl.ac.uk . Hosted dataset request form","title":"Request hosting of shared datasets"},{"location":"Additional_Resource_Requests/#purchase-dedicated-compute-nodes-within-legion-or-myriad","text":"There has previously been a programme allowing researchers to purchase compute nodes to be attached to the Legion cluster. This has been discontinued, as the Myriad cluster replaces the Legion cluster for the majority of users. There are plans to allow the purchase of nodes for Myriad in future but these have not yet been finalised.","title":"Purchase dedicated compute nodes within Legion or Myriad"},{"location":"Additional_Resource_Requests/#further-information","text":"For further advice or information on future hardware options, please contact rits@ucl.ac.uk .","title":"Further information"},{"location":"Contact_Us/","text":"Contact and Support \u00a7 Users should direct any queries relating to their use of Research Computing services to the Research Computing Support Team at rc-support@ucl.ac.uk (see below). The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided. Availability: 9:30am - 4:30pm, Monday - Friday, except on Bank Holidays and College Closures. We aim to provide to you with a useful response within 24 hours. Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided. Drop-In Sessions \u00a7 Research IT Services holds drop-in sessions roughly every two weeks which at least one member of the Research Computing team usually attends. More details and dates for these sessions are available on the the RITS pages . If you have a particularly complex problem, it may be useful to email the support address, rc-support@ucl.ac.uk , beforehand so that the person attending can prepare. Location \u00a7 The Research Computing Team are located at: 1-19 Torrington Place Floor 6 London WC1E 7HB Please note that this is a busy, mixed-group office, and people attempting to gain support by walking in uninvited may not be made welcome.","title":"Contact and Support"},{"location":"Contact_Us/#contact-and-support","text":"Users should direct any queries relating to their use of Research Computing services to the Research Computing Support Team at rc-support@ucl.ac.uk (see below). The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided. Availability: 9:30am - 4:30pm, Monday - Friday, except on Bank Holidays and College Closures. We aim to provide to you with a useful response within 24 hours. Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided.","title":"Contact and Support"},{"location":"Contact_Us/#drop-in-sessions","text":"Research IT Services holds drop-in sessions roughly every two weeks which at least one member of the Research Computing team usually attends. More details and dates for these sessions are available on the the RITS pages . If you have a particularly complex problem, it may be useful to email the support address, rc-support@ucl.ac.uk , beforehand so that the person attending can prepare.","title":"Drop-In Sessions"},{"location":"Contact_Us/#location","text":"The Research Computing Team are located at: 1-19 Torrington Place Floor 6 London WC1E 7HB Please note that this is a busy, mixed-group office, and people attempting to gain support by walking in uninvited may not be made welcome.","title":"Location"},{"location":"Example_Jobscripts/","text":"On this page we describe some basic example scripts to submit jobs to our clusters. After creating your script, submit it to the scheduler with: qsub my_script.sh Service Differences \u00a7 These scripts are applicable to all our clusters, but node sizes (core count, memory, and temporary storage sizes) differ between machines, so please check those details on the cluster-specific pages. Working Directories and Output \u00a7 The parallel filesystems we use to provide the home and scratch filesystems perform best when reading or writing single large files, and worst when operating on many different small files. To avoid causing problems, many of the scripts below are written to create all their files in the temporary $TMPDIR storage, and compress and copy them to the scratch area at the end of the job. This can be a problem if your job is not finishing and you need to see the output, or if your job is crashing or failing to produce what you expected. Feel free to modify the scripts to read from or write to Scratch directly, however, your performance will generally not be as good as writing to $TMPDIR , and you may impact the general performance of the machine if you do this with many jobs simultaneously. This is particularly the case with single-core jobs, because that core is guaranteed to be writing out data. Note that there is also the option of using the Local2Scratch process ( see below ), which takes place after the job has finished, in the clean-up step. This gives you the option of always getting the contents of $TMPDIR back, at the cost of possibly getting incomplete files and not having any control over where the files go. Note about Projects \u00a7 Projects are a system used in the scheduler and the accounting system to track budgets and access controls. Most users of UCL's internal clusters will not need to specify a project and will default to the AllUsers project. Users of the Thomas and Michael services should refer to the specific pages for those machines, and the information they were given when they registered. To specify a project ID in a job script, use the -P object as below: #$ -P <your_project_id> Resources \u00a7 The lines starting with #$ -l are where you are requesting resources like wallclock time (how long your job is allowed to run), memory, and possibly tmpfs (local hard disk space on the node, if it has one). If you have no notion of how much you should request for any of these, have a look at How do I estimate what resources to request in my jobscript? Serial Job Script Example \u00a7 The most basic type of job a user can submit is a serial job. These jobs run on a single processor (core) with a single thread. Shown below is a simple job script that runs /bin/date (which prints the current date) on the compute node, and puts the output into a file. #!/bin/bash -l # Batch script to run a serial job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Serial_Job # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID. #$ -wd /home/<your_UCL_id>/Scratch/workspace # Your work should be done in $TMPDIR cd $TMPDIR # Run the application and put the output into a file called date.txt /bin/date > date.txt # Preferably, tar-up (archive) all output files onto the shared scratch area tar -zcvf $HOME /Scratch/files_from_job_ $JOB_ID .tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete! Multi-threaded Job Example \u00a7 For programs that can use multiple threads, you can request multiple processor cores using the -pe smp <number> option. One common method for using multiple threads in a program is OpenMP, and the $OMP_NUM_THREADS environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though ( via a mechanism called cgroups ). Note that this job script works directly in scratch instead of in the temporary $TMPDIR storage. #!/bin/bash -l # Batch script to run an OpenMP threaded job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM for each core/thread (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Multi-threaded Job # Request 16 cores. #$ -pe smp 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/output # 8. Run the application. $HOME /my_program/example MPI Job Script Example \u00a7 The default MPI implementation on our clusters is the Intel MPI stack. MPI programs don\u2019t use a shared memory model so they can be run across multiple nodes. This script differs considerably from the serial and OpenMP jobs in that MPI programs need to be invoked by a program called gerun. This is our wrapper for mpirun and takes care of passing the number of processors and a file called a machine file. '''Important''': If you wish to pass a file or stream of data to the standard input (stdin) of an MPI program, there are specific command-line options you need to use to control which MPI tasks are able to receive it. ( -s for Intel MPI, --stdin for OpenMPI.) Please consult the help output of the mpirun command for further information. The gerun launcher does not automatically handle this. If you use OpenMPI, you need to make sure the Intel MPI modules are removed and the OpenMPI modules are loaded, either in your jobscript or in your shell start-up files (e.g. `~/.bashrc). #!/bin/bash -l # Batch script to run an MPI parallel job under SGE with Intel MPI. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N MadScience_1_16 # Select the MPI parallel environment and 16 processes. #$ -pe mpi 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID : #$ -wd /home/<your_UCL_id>/Scratch/output # Run our MPI job. GERun is a wrapper that launches MPI jobs on our clusters. gerun $HOME /src/science/simulate Array Job Script Example \u00a7 If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the -t option to get Sun Grid Engine to run 10,000 copies of this job numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the $SGE_TASK_ID environment variable in each task. All the usual SGE output files have the task ID appended. MPI jobs and parallel shared memory jobs can also be submitted as arrays. #!/bin/bash -l # Batch script to run a serial array job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N MyArrayJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Run the application. echo \" $JOB_NAME $SGE_TASK_ID \" Array Job Script Example Using Parameter File \u00a7 Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces. For example: 0001 1.5 3 aardvark 0002 1.1 13 guppy 0003 1.23 5 elephant 0004 1.112 23 panda 0005 ... Assuming that this file is stored in ~/Scratch/input/params.txt (you can call this file anything you want) then the user can use awk/sed to get the appropriate variables out of the file. The script below does this and stores them in $index , $variable1 , $variable2 and $variable3 . So for example in task 4, $index = 0004 , $variable1 = 1.112 , $variable2 = 23 and $variable3 = panda . Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks. #!/bin/bash -l # Batch script to run an array job. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 1000 tasks # numbered 1 to 1000. #$ -t 1-1000 # Set the name of the job. #$ -N array-params # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Parse parameter file to get variables. number = $SGE_TASK_ID paramfile = /home/<your_UCL_id>/Scratch/input/params.txt index = \"`sed -n ${ number } p $paramfile | awk '{print $1 }'`\" variable1 = \"`sed -n ${ number } p $paramfile | awk '{print $2 }'`\" variable2 = \"`sed -n ${ number } p $paramfile | awk '{print $3 }'`\" variable3 = \"`sed -n ${ number } p $paramfile | awk '{print $4 }'`\" # Run the program (replace echo with your binary and options). echo \" $index \" \" $variable1 \" \" $variable2 \" \" $variable3 \" Example Array Job Using Local2Scratch \u00a7 Users can automate the transfer of data from $TMPDIR to their scratch space by adding the text #Local2Scratch to their script on a line alone as a special comment. During the clean-up phase of the job, a tool checks whether the script contains that text, and if so, files are transferred from $TMPDIR to a directory in scratch with the structure <job id>/<job id>.<task id>.<queue>/ . The example below does this for a job array, but this works for any job type. #!/bin/bash -l # Batch script to run an array job under SGE and # transfer the output to Scratch from local. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N local2scratcharray # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Run the application in TMPDIR. cd $TMPDIR hostname > hostname.txt Array Job Script with a Stride \u00a7 If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task. Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs. For example, a stride of 10 will give you these task IDs: 1, 11, 21... Your script can then have a loop that runs task IDs from $SGE_TASK_ID to $SGE_TASK_ID + 9 , so each task is doing ten times as many runs as it was before. #!/bin/bash -l # Batch script to run an array job with strided task IDs under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested task IDs # numbered 1 to 10000 with a stride of 10. #$ -t 1-10000:10 # Set the name of the job. #$ -N arraystride # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Do your work in $TMPDIR cd $TMPDIR # 10. Loop through the IDs covered by this stride and run the application if # the input file exists. (This is because the last stride may not have that # many inputs available). Or you can leave out the check and get an error. for (( i = $SGE_TASK_ID ; i< $SGE_TASK_ID +10 ; i++ )) do if [ -f \"input. $i \" ] then echo \" $JOB_NAME \" \" $SGE_TASK_ID \" \"input. $i \" fi done GPU Job Script Example \u00a7 To use NVIDIA GPUs with the CUDA libraries, you need to load the CUDA runtime libraries module or else set up the environment yourself. The script below shows what you'll need to unload and load the appropriate modules. You also need to use the -l gpu=<number> option to request the GPUs from the scheduler. #!/bin/bash -l # Batch script to run a GPU job under SGE. # Request a number of GPU cards, in this case 2 (the maximum) #$ -l gpu=2 # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Change into temporary directory to run work cd $TMPDIR # load the cuda module (in case you are running a CUDA program) module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18/gnu-4.9.2 # Run the application - the line below is just a random example. mygpucode # 10. Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME /Scratch/files_from_job_ $JOB_ID .tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete! Job using MPI and GPUs \u00a7 It is possible to run MPI programs that use GPUs but our clusters currently only support this within a single node. The script below shows how to run a program using 2 gpus and 12 cpus. #!/bin/bash -l # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 12 cores, 2 GPUs, 1 gigabyte of RAM per CPU, 15 gigabyte of TMPDIR space #$ -l mem=1G #$ -l gpu=2 #$ -pe mpi 12 #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUMPIrun # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your user id>/Scratch/output/ # Run our MPI job. You can choose OpenMPI or IntelMPI for GCC. module unload compilers mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.10.1/gnu-4.9.2 module load cuda/7.5.18/gnu-4.9.2 gerun myGPUapp","title":"Example Jobscripts"},{"location":"Example_Jobscripts/#service-differences","text":"These scripts are applicable to all our clusters, but node sizes (core count, memory, and temporary storage sizes) differ between machines, so please check those details on the cluster-specific pages.","title":"Service Differences"},{"location":"Example_Jobscripts/#working-directories-and-output","text":"The parallel filesystems we use to provide the home and scratch filesystems perform best when reading or writing single large files, and worst when operating on many different small files. To avoid causing problems, many of the scripts below are written to create all their files in the temporary $TMPDIR storage, and compress and copy them to the scratch area at the end of the job. This can be a problem if your job is not finishing and you need to see the output, or if your job is crashing or failing to produce what you expected. Feel free to modify the scripts to read from or write to Scratch directly, however, your performance will generally not be as good as writing to $TMPDIR , and you may impact the general performance of the machine if you do this with many jobs simultaneously. This is particularly the case with single-core jobs, because that core is guaranteed to be writing out data. Note that there is also the option of using the Local2Scratch process ( see below ), which takes place after the job has finished, in the clean-up step. This gives you the option of always getting the contents of $TMPDIR back, at the cost of possibly getting incomplete files and not having any control over where the files go.","title":"Working Directories and Output"},{"location":"Example_Jobscripts/#note-about-projects","text":"Projects are a system used in the scheduler and the accounting system to track budgets and access controls. Most users of UCL's internal clusters will not need to specify a project and will default to the AllUsers project. Users of the Thomas and Michael services should refer to the specific pages for those machines, and the information they were given when they registered. To specify a project ID in a job script, use the -P object as below: #$ -P <your_project_id>","title":"Note about Projects"},{"location":"Example_Jobscripts/#resources","text":"The lines starting with #$ -l are where you are requesting resources like wallclock time (how long your job is allowed to run), memory, and possibly tmpfs (local hard disk space on the node, if it has one). If you have no notion of how much you should request for any of these, have a look at How do I estimate what resources to request in my jobscript?","title":"Resources"},{"location":"Example_Jobscripts/#serial-job-script-example","text":"The most basic type of job a user can submit is a serial job. These jobs run on a single processor (core) with a single thread. Shown below is a simple job script that runs /bin/date (which prints the current date) on the compute node, and puts the output into a file. #!/bin/bash -l # Batch script to run a serial job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Serial_Job # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID. #$ -wd /home/<your_UCL_id>/Scratch/workspace # Your work should be done in $TMPDIR cd $TMPDIR # Run the application and put the output into a file called date.txt /bin/date > date.txt # Preferably, tar-up (archive) all output files onto the shared scratch area tar -zcvf $HOME /Scratch/files_from_job_ $JOB_ID .tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete!","title":"Serial Job Script Example"},{"location":"Example_Jobscripts/#multi-threaded-job-example","text":"For programs that can use multiple threads, you can request multiple processor cores using the -pe smp <number> option. One common method for using multiple threads in a program is OpenMP, and the $OMP_NUM_THREADS environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though ( via a mechanism called cgroups ). Note that this job script works directly in scratch instead of in the temporary $TMPDIR storage. #!/bin/bash -l # Batch script to run an OpenMP threaded job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM for each core/thread (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Multi-threaded Job # Request 16 cores. #$ -pe smp 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/output # 8. Run the application. $HOME /my_program/example","title":"Multi-threaded Job Example"},{"location":"Example_Jobscripts/#mpi-job-script-example","text":"The default MPI implementation on our clusters is the Intel MPI stack. MPI programs don\u2019t use a shared memory model so they can be run across multiple nodes. This script differs considerably from the serial and OpenMP jobs in that MPI programs need to be invoked by a program called gerun. This is our wrapper for mpirun and takes care of passing the number of processors and a file called a machine file. '''Important''': If you wish to pass a file or stream of data to the standard input (stdin) of an MPI program, there are specific command-line options you need to use to control which MPI tasks are able to receive it. ( -s for Intel MPI, --stdin for OpenMPI.) Please consult the help output of the mpirun command for further information. The gerun launcher does not automatically handle this. If you use OpenMPI, you need to make sure the Intel MPI modules are removed and the OpenMPI modules are loaded, either in your jobscript or in your shell start-up files (e.g. `~/.bashrc). #!/bin/bash -l # Batch script to run an MPI parallel job under SGE with Intel MPI. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N MadScience_1_16 # Select the MPI parallel environment and 16 processes. #$ -pe mpi 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID : #$ -wd /home/<your_UCL_id>/Scratch/output # Run our MPI job. GERun is a wrapper that launches MPI jobs on our clusters. gerun $HOME /src/science/simulate","title":"MPI Job Script Example"},{"location":"Example_Jobscripts/#array-job-script-example","text":"If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the -t option to get Sun Grid Engine to run 10,000 copies of this job numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the $SGE_TASK_ID environment variable in each task. All the usual SGE output files have the task ID appended. MPI jobs and parallel shared memory jobs can also be submitted as arrays. #!/bin/bash -l # Batch script to run a serial array job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N MyArrayJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Run the application. echo \" $JOB_NAME $SGE_TASK_ID \"","title":"Array Job Script Example"},{"location":"Example_Jobscripts/#array-job-script-example-using-parameter-file","text":"Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces. For example: 0001 1.5 3 aardvark 0002 1.1 13 guppy 0003 1.23 5 elephant 0004 1.112 23 panda 0005 ... Assuming that this file is stored in ~/Scratch/input/params.txt (you can call this file anything you want) then the user can use awk/sed to get the appropriate variables out of the file. The script below does this and stores them in $index , $variable1 , $variable2 and $variable3 . So for example in task 4, $index = 0004 , $variable1 = 1.112 , $variable2 = 23 and $variable3 = panda . Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks. #!/bin/bash -l # Batch script to run an array job. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 1000 tasks # numbered 1 to 1000. #$ -t 1-1000 # Set the name of the job. #$ -N array-params # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Parse parameter file to get variables. number = $SGE_TASK_ID paramfile = /home/<your_UCL_id>/Scratch/input/params.txt index = \"`sed -n ${ number } p $paramfile | awk '{print $1 }'`\" variable1 = \"`sed -n ${ number } p $paramfile | awk '{print $2 }'`\" variable2 = \"`sed -n ${ number } p $paramfile | awk '{print $3 }'`\" variable3 = \"`sed -n ${ number } p $paramfile | awk '{print $4 }'`\" # Run the program (replace echo with your binary and options). echo \" $index \" \" $variable1 \" \" $variable2 \" \" $variable3 \"","title":"Array Job Script Example Using Parameter File"},{"location":"Example_Jobscripts/#example-array-job-using-local2scratch","text":"Users can automate the transfer of data from $TMPDIR to their scratch space by adding the text #Local2Scratch to their script on a line alone as a special comment. During the clean-up phase of the job, a tool checks whether the script contains that text, and if so, files are transferred from $TMPDIR to a directory in scratch with the structure <job id>/<job id>.<task id>.<queue>/ . The example below does this for a job array, but this works for any job type. #!/bin/bash -l # Batch script to run an array job under SGE and # transfer the output to Scratch from local. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N local2scratcharray # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Run the application in TMPDIR. cd $TMPDIR hostname > hostname.txt","title":"Example Array Job Using Local2Scratch"},{"location":"Example_Jobscripts/#array-job-script-with-a-stride","text":"If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task. Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs. For example, a stride of 10 will give you these task IDs: 1, 11, 21... Your script can then have a loop that runs task IDs from $SGE_TASK_ID to $SGE_TASK_ID + 9 , so each task is doing ten times as many runs as it was before. #!/bin/bash -l # Batch script to run an array job with strided task IDs under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested task IDs # numbered 1 to 10000 with a stride of 10. #$ -t 1-10000:10 # Set the name of the job. #$ -N arraystride # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Do your work in $TMPDIR cd $TMPDIR # 10. Loop through the IDs covered by this stride and run the application if # the input file exists. (This is because the last stride may not have that # many inputs available). Or you can leave out the check and get an error. for (( i = $SGE_TASK_ID ; i< $SGE_TASK_ID +10 ; i++ )) do if [ -f \"input. $i \" ] then echo \" $JOB_NAME \" \" $SGE_TASK_ID \" \"input. $i \" fi done","title":"Array Job Script with a Stride"},{"location":"Example_Jobscripts/#gpu-job-script-example","text":"To use NVIDIA GPUs with the CUDA libraries, you need to load the CUDA runtime libraries module or else set up the environment yourself. The script below shows what you'll need to unload and load the appropriate modules. You also need to use the -l gpu=<number> option to request the GPUs from the scheduler. #!/bin/bash -l # Batch script to run a GPU job under SGE. # Request a number of GPU cards, in this case 2 (the maximum) #$ -l gpu=2 # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Change into temporary directory to run work cd $TMPDIR # load the cuda module (in case you are running a CUDA program) module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18/gnu-4.9.2 # Run the application - the line below is just a random example. mygpucode # 10. Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME /Scratch/files_from_job_ $JOB_ID .tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete!","title":"GPU Job Script Example"},{"location":"Example_Jobscripts/#job-using-mpi-and-gpus","text":"It is possible to run MPI programs that use GPUs but our clusters currently only support this within a single node. The script below shows how to run a program using 2 gpus and 12 cpus. #!/bin/bash -l # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 12 cores, 2 GPUs, 1 gigabyte of RAM per CPU, 15 gigabyte of TMPDIR space #$ -l mem=1G #$ -l gpu=2 #$ -pe mpi 12 #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUMPIrun # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your user id>/Scratch/output/ # Run our MPI job. You can choose OpenMPI or IntelMPI for GCC. module unload compilers mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.10.1/gnu-4.9.2 module load cuda/7.5.18/gnu-4.9.2 gerun myGPUapp","title":"Job using MPI and GPUs"},{"location":"Experienced_Users/","text":"Quick Start Guide for Experienced HPC Users \u00a7 What Services are available? \u00a7 UCL \u00a7 Grace/Kathleen - HPC, large parallel MPI jobs. Myriad - High Throughput, GPU or large memory jobs. External \u00a7 Thomas - MMM Hub Tier 2 Michael - Faraday Institution Tier 2 How do I get access? \u00a7 UCL services: Fill in the sign-up form Tier 2 services: Contact your point of contact. How do I connect? \u00a7 All connections are via SSH, and you use your UCL credentials to log in (external users should use the mmmXXXX account with the SSH key they have provided to their point of contact). UCL services can only be connected to by users inside the UCL network which may mean using the institutional VPN or \"bouncing\" off another UCL machine when accessing them from outside the UCL network . The Tier 2 services (Thomas and Michael) are externally accessible. Login hosts \u00a7 Service General alias Direct login node addresses Grace grace.rc.ucl.ac.uk login01.ext.grace.ucl.ac.uk login02.ext.grace.ucl.ac.uk Kathleen kathleen.rc.ucl.ac.uk login01.kathleen.rc.ucl.ac.uk login02.kathleen.rc.ucl.ac.uk Myriad myriad.rc.ucl.ac.uk login12.myriad.rc.ucl.ac.uk login13.myriad.rc.ucl.ac.uk Thomas thomas.rc.ucl.ac.uk login03.thomas.rc.ucl.ac.uk login04.thomas.rc.ucl.ac.uk Michael michael.rc.ucl.ac.uk login10.michael.rc.ucl.ac.uk login11.michael.rc.ucl.ac.uk Generally you should connect to the general alias as this is load-balanced across the available login nodes, however if you use screen or tmux you will want to use the direct hostname so that you can reconnect to your session. Software stack \u00a7 All UCL services use the same software stack based upon RHEL 7.x with a standardised set of packages, exposed to the user through environment modules (the module command). By default this has a set of useful tools loaded, as well as the Intel compilers and MPI but users are free to change their own environment. Batch System \u00a7 UCL services use Grid Engine to manage jobs. This install is somewhat customised and so scripts for non-UCL services may not work . We recommend that when launching MPI jobs you use our gerun parallel launcher instead of mpirun as it inherits settings from the job and launches the appropriate number of processes with the MPI implementation you have chosen. It abstracts away a lot of the complexity between different version of MPI. # using gerun gerun myMPIprogram # using mpirun when a machinefile is needed (eg Intel MPI) mpirun -np $NSLOTS -machinefile $PE_HOSTFILE myMPIprogram $NSLOTS is an environment variable containing the value you gave to -pe mpi so you do not need to re-specify it. Troubleshooting gerun \u00a7 If you need to see what gerun is doing because something is not working as expected, look at the error file for your job, default name $JOBNAME.e$JOB_ID . It will contain debug information from gerun about where it ran and the exact mpirun command it used. You may need to use mpirun directly with different options if your program has sufficiently complex process placement requirements, or is using something like GlobalArrays and requires a different process layout than it is being given. Script sections \u00a7 Shebang \u00a7 It's important that you add the -l option to bash in the shebang so that login scripts are parsed and the environment modules environment is set up. #! must be the first two characters in the file, no previous white space. #!/bin/bash -l Resources you can request \u00a7 Number of cores \u00a7 For MPI: #$ -pe mpi <number of cores> For threads: #$ -pe smp <number of cores> For single core jobs you don't need to request a number of cores. For hybrid codes use the MPI example and set OMP_NUM_THREADS to control the number of threads per node. gerun will launch the right number of processes in the right place if you use it. Amount of RAM per core \u00a7 #$ -l mem=<integer amount of RAM in G or M> e.g. #$ -l mem=4G requests 4 gigabytes of RAM per core. Run time \u00a7 #$ -l h_rt=<hours:minutes:seconds> e.g. #$ -l h_rt=48:00:00 requests 48 hours. Working directory \u00a7 Either a specific working directory: #$ -wd /path/to/working/directory or the directory the script was submitted from: #$ -cwd GPUs (Myriad only) \u00a7 #$ -l gpu=<number of GPUs> Enable Hyperthreads (Kathleen only) \u00a7 #$ -l threads=1 With Hyperthreads enabled you need to request twice as many cores and then control threads vs MPI ranks with OMP_NUM_THREADS . E.g. #$ -pe mpi 160 #$ -l threads=1 export OMP_NUM_THREADS = 2 Would use 80 cores, with two threads (on Hyperthreads) per core. If you use gerun to launch your MPI process, it will take care of the division for you, but if you're using mpirun or mpiexec directly, you'll have to take care to use the correct number of MPI ranks per node yourself. Temporary local disk (every machine EXCEPT Kathleen) \u00a7 #$ -l tmpdir=<size in G> e.g. #$ -l tmpdir=10G requests 10 gigabytes of temporary local disk. The rest of the script \u00a7 You need to load any module dependencies, set up any custom environment variables or paths you need and then run the rest of your workflow. Submitting your jobscript \u00a7 Job scripts can be submitted with qsub , jobs can be monitored with qstat and deleted with qdel . Interactive jobs \u00a7 If you need to run an interactive job, possibly with X forwarding, you can do so using qrsh . Please see our page on interactive jobs for more details.","title":"Quick Start Guide for Experienced HPC Users"},{"location":"Experienced_Users/#quick-start-guide-for-experienced-hpc-users","text":"","title":"Quick Start Guide for Experienced HPC Users"},{"location":"Experienced_Users/#what-services-are-available","text":"","title":"What Services are available?"},{"location":"Experienced_Users/#ucl","text":"Grace/Kathleen - HPC, large parallel MPI jobs. Myriad - High Throughput, GPU or large memory jobs.","title":"UCL"},{"location":"Experienced_Users/#external","text":"Thomas - MMM Hub Tier 2 Michael - Faraday Institution Tier 2","title":"External"},{"location":"Experienced_Users/#how-do-i-get-access","text":"UCL services: Fill in the sign-up form Tier 2 services: Contact your point of contact.","title":"How do I get access?"},{"location":"Experienced_Users/#how-do-i-connect","text":"All connections are via SSH, and you use your UCL credentials to log in (external users should use the mmmXXXX account with the SSH key they have provided to their point of contact). UCL services can only be connected to by users inside the UCL network which may mean using the institutional VPN or \"bouncing\" off another UCL machine when accessing them from outside the UCL network . The Tier 2 services (Thomas and Michael) are externally accessible.","title":"How do I connect?"},{"location":"Experienced_Users/#login-hosts","text":"Service General alias Direct login node addresses Grace grace.rc.ucl.ac.uk login01.ext.grace.ucl.ac.uk login02.ext.grace.ucl.ac.uk Kathleen kathleen.rc.ucl.ac.uk login01.kathleen.rc.ucl.ac.uk login02.kathleen.rc.ucl.ac.uk Myriad myriad.rc.ucl.ac.uk login12.myriad.rc.ucl.ac.uk login13.myriad.rc.ucl.ac.uk Thomas thomas.rc.ucl.ac.uk login03.thomas.rc.ucl.ac.uk login04.thomas.rc.ucl.ac.uk Michael michael.rc.ucl.ac.uk login10.michael.rc.ucl.ac.uk login11.michael.rc.ucl.ac.uk Generally you should connect to the general alias as this is load-balanced across the available login nodes, however if you use screen or tmux you will want to use the direct hostname so that you can reconnect to your session.","title":"Login hosts"},{"location":"Experienced_Users/#software-stack","text":"All UCL services use the same software stack based upon RHEL 7.x with a standardised set of packages, exposed to the user through environment modules (the module command). By default this has a set of useful tools loaded, as well as the Intel compilers and MPI but users are free to change their own environment.","title":"Software stack"},{"location":"Experienced_Users/#batch-system","text":"UCL services use Grid Engine to manage jobs. This install is somewhat customised and so scripts for non-UCL services may not work . We recommend that when launching MPI jobs you use our gerun parallel launcher instead of mpirun as it inherits settings from the job and launches the appropriate number of processes with the MPI implementation you have chosen. It abstracts away a lot of the complexity between different version of MPI. # using gerun gerun myMPIprogram # using mpirun when a machinefile is needed (eg Intel MPI) mpirun -np $NSLOTS -machinefile $PE_HOSTFILE myMPIprogram $NSLOTS is an environment variable containing the value you gave to -pe mpi so you do not need to re-specify it.","title":"Batch System"},{"location":"Experienced_Users/#troubleshooting-gerun","text":"If you need to see what gerun is doing because something is not working as expected, look at the error file for your job, default name $JOBNAME.e$JOB_ID . It will contain debug information from gerun about where it ran and the exact mpirun command it used. You may need to use mpirun directly with different options if your program has sufficiently complex process placement requirements, or is using something like GlobalArrays and requires a different process layout than it is being given.","title":"Troubleshooting gerun"},{"location":"Experienced_Users/#script-sections","text":"","title":"Script sections"},{"location":"Experienced_Users/#shebang","text":"It's important that you add the -l option to bash in the shebang so that login scripts are parsed and the environment modules environment is set up. #! must be the first two characters in the file, no previous white space. #!/bin/bash -l","title":"Shebang"},{"location":"Experienced_Users/#resources-you-can-request","text":"","title":"Resources you can request"},{"location":"Experienced_Users/#number-of-cores","text":"For MPI: #$ -pe mpi <number of cores> For threads: #$ -pe smp <number of cores> For single core jobs you don't need to request a number of cores. For hybrid codes use the MPI example and set OMP_NUM_THREADS to control the number of threads per node. gerun will launch the right number of processes in the right place if you use it.","title":"Number of cores"},{"location":"Experienced_Users/#amount-of-ram-per-core","text":"#$ -l mem=<integer amount of RAM in G or M> e.g. #$ -l mem=4G requests 4 gigabytes of RAM per core.","title":"Amount of RAM per core"},{"location":"Experienced_Users/#run-time","text":"#$ -l h_rt=<hours:minutes:seconds> e.g. #$ -l h_rt=48:00:00 requests 48 hours.","title":"Run time"},{"location":"Experienced_Users/#working-directory","text":"Either a specific working directory: #$ -wd /path/to/working/directory or the directory the script was submitted from: #$ -cwd","title":"Working directory"},{"location":"Experienced_Users/#gpus-myriad-only","text":"#$ -l gpu=<number of GPUs>","title":"GPUs (Myriad only)"},{"location":"Experienced_Users/#enable-hyperthreads-kathleen-only","text":"#$ -l threads=1 With Hyperthreads enabled you need to request twice as many cores and then control threads vs MPI ranks with OMP_NUM_THREADS . E.g. #$ -pe mpi 160 #$ -l threads=1 export OMP_NUM_THREADS = 2 Would use 80 cores, with two threads (on Hyperthreads) per core. If you use gerun to launch your MPI process, it will take care of the division for you, but if you're using mpirun or mpiexec directly, you'll have to take care to use the correct number of MPI ranks per node yourself.","title":"Enable Hyperthreads (Kathleen only)"},{"location":"Experienced_Users/#temporary-local-disk-every-machine-except-kathleen","text":"#$ -l tmpdir=<size in G> e.g. #$ -l tmpdir=10G requests 10 gigabytes of temporary local disk.","title":"Temporary local disk (every machine EXCEPT Kathleen)"},{"location":"Experienced_Users/#the-rest-of-the-script","text":"You need to load any module dependencies, set up any custom environment variables or paths you need and then run the rest of your workflow.","title":"The rest of the script"},{"location":"Experienced_Users/#submitting-your-jobscript","text":"Job scripts can be submitted with qsub , jobs can be monitored with qstat and deleted with qdel .","title":"Submitting your jobscript"},{"location":"Experienced_Users/#interactive-jobs","text":"If you need to run an interactive job, possibly with X forwarding, you can do so using qrsh . Please see our page on interactive jobs for more details.","title":"Interactive jobs"},{"location":"Interactive_Jobs/","text":"Interactive Job Sessions \u00a7 For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for live visualisation, software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete. Please note that time limits are restricted to two hours for interactive sessions, and available core counts are limited. Requesting Access \u00a7 You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. It typically takes the form: qrsh -pe mpi 8 -l mem=512M,h_rt=2:00:00 -now no In this example you are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours (the maximum allowed for interactive sessions). All job types we support on the system are supported via an interactive session (see our examples section ). Likewise, all qsub options are supported like regular job submission with the difference that with qrsh they must be given at the command line, and not with any job script (or via -@). In addition the -now option is useful when a cluster is busy. By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The -now no option tells it to keep waiting until it gets scheduled. Pressing Ctrl+C (i.e. the control key and the C key at the same time) will safely cancel the request if it doesn't seem to be able to get you a session. Interactive X sessions \u00a7 You can get an interactive X session from the head node of the job back to the login node. The way to do this is to run the qrsh command in the following generic fashion: qrsh <options> <command> <arguments to <command>> Where <command> is either a command to launch an X terminal like Xterm or Mrxvt or a GUI application like XMGrace or GaussView. To make effective use of the X forwarding you will need to have logged in to the login node with ssh -X or some equivalent method. Here is an example of how you can get a X terminal session with the qrsh command: qrsh -l mem=512M,h_rt=0:30:0 \\ /shared/ucl/apps/mrxvt/0.5.4/bin/mrxvt -title 'User Test Node' Working on the nodes \u00a7 If you want to run a command on one of your allocated nodes which is not the headnode, you can use a standard ssh command: ssh <hostname> <command> [args] Where <hostname> can be obtained by inspecting the file $TMPDIR/machines . GPU test nodes \u00a7 You can also run GPU jobs interactively simply by adding the -l gpu=1 or -l gpu=2 options to the qrsh command as normal. For more information, please contact us on rc-support@ucl.ac.uk","title":"Interactive Job Sessions"},{"location":"Interactive_Jobs/#interactive-job-sessions","text":"For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for live visualisation, software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete. Please note that time limits are restricted to two hours for interactive sessions, and available core counts are limited.","title":"Interactive Job Sessions"},{"location":"Interactive_Jobs/#requesting-access","text":"You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. It typically takes the form: qrsh -pe mpi 8 -l mem=512M,h_rt=2:00:00 -now no In this example you are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours (the maximum allowed for interactive sessions). All job types we support on the system are supported via an interactive session (see our examples section ). Likewise, all qsub options are supported like regular job submission with the difference that with qrsh they must be given at the command line, and not with any job script (or via -@). In addition the -now option is useful when a cluster is busy. By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The -now no option tells it to keep waiting until it gets scheduled. Pressing Ctrl+C (i.e. the control key and the C key at the same time) will safely cancel the request if it doesn't seem to be able to get you a session.","title":"Requesting Access"},{"location":"Interactive_Jobs/#interactive-x-sessions","text":"You can get an interactive X session from the head node of the job back to the login node. The way to do this is to run the qrsh command in the following generic fashion: qrsh <options> <command> <arguments to <command>> Where <command> is either a command to launch an X terminal like Xterm or Mrxvt or a GUI application like XMGrace or GaussView. To make effective use of the X forwarding you will need to have logged in to the login node with ssh -X or some equivalent method. Here is an example of how you can get a X terminal session with the qrsh command: qrsh -l mem=512M,h_rt=0:30:0 \\ /shared/ucl/apps/mrxvt/0.5.4/bin/mrxvt -title 'User Test Node'","title":"Interactive X sessions"},{"location":"Interactive_Jobs/#working-on-the-nodes","text":"If you want to run a command on one of your allocated nodes which is not the headnode, you can use a standard ssh command: ssh <hostname> <command> [args] Where <hostname> can be obtained by inspecting the file $TMPDIR/machines .","title":"Working on the nodes"},{"location":"Interactive_Jobs/#gpu-test-nodes","text":"You can also run GPU jobs interactively simply by adding the -l gpu=1 or -l gpu=2 options to the qrsh command as normal. For more information, please contact us on rc-support@ucl.ac.uk","title":"GPU test nodes"},{"location":"Job_Results/","text":"Where do my results go? \u00a7 After submitting your job, you can use the command qstat to view the status of all the jobs you have submitted. Once you can no longer see your job on the list, this means your job has completed. To view details on jobs that have completed, you can run jobhist . There are various ways of monitoring the output of your job. Output and error files \u00a7 When writing your job script you can either tell it to start in the directory you submit it from ( -cwd ), or from a particular directory ( -wd <dir> ), or from your home directory (the default). When your job runs, it will create files in this directory for the job's output and errors: File Name Contents myscript.sh Your job script. myscript.sh.o12345 Output from the job. ( stdout ) myscript.sh.e12345 Errors, warnings, and other messages from the job that aren't mixed into the output. ( stderr ) myscript.sh.po12345 Output from the setup script run before a job. (\"prolog\") myscript.sh.pe12345 Output from the clean-up script run after a job. (\"epilog\") Normally there should be nothing in the .po and .pe files, and that's fine. If you change the name of the job in the queue, using the -N option, your output and error files will use that as the filename stem instead. Most programs will also produce separate output files, in a way that is particular to that program. Often these will be in the same directory, but that depends on the program and how you ran it. Grid Engine commands \u00a7 The following commands can be used to submit and monitor a job. Qsub \u00a7 This command submits your job to the batch queue. You can also use options on the command-line to override options you have put in your job script. Command Action qsub myscript.sh Submit the script as-is qsub -N NewName myscript.sh Submit the script but change the job's name qsub -l h_rt=24:0:0 myscript.sh Submit the script but change the maximum run-time qsub -hold_jid 12345 myscript.sh Submit the script but make it wait for job 12345 to finish qsub -ac allow=XYZ myscript.sh Submit the script but only let it run on node classes X, Y, and Z Qstat \u00a7 This command shows the status of your jobs. When you run qstat with no options, all of your jobs currently running will be displayed. By adding in the option -f -j <job-ID> you will get more detail on the specified job. Qdel \u00a7 This command deletes your job from the queue. When deleting a job will need to run qdel <job-ID> , however qdel '*' can be used to delete all jobs. To delete a batch of jobs, creating a file with the list of job IDs that you would like to delete and placing it in the following commands will delete the following jobs: cat <filename> | xargs qdel Qsub emailing \u00a7 We also have a mailing system that can be implemented to send emails with reminders of the status of your job through qsub . In your jobscript, or when you use qsub to submit your job, you can use the option -m . You can specify when you want an email sent to you by using the below options after qsub -m : b Mail is sent at the beginning of the job. e Mail is sent at the end of the job. a Mail is sent when the job is aborted or rescheduled. s Mail is sent when the job is suspended. n No mail is sent. (The default.) You can use more than one of these options by putting them together after the -m option; for example, adding the following to your job script would mean you get an email when the job begins and when it ends: #$ -m be","title":"Where do my results go?"},{"location":"Job_Results/#where-do-my-results-go","text":"After submitting your job, you can use the command qstat to view the status of all the jobs you have submitted. Once you can no longer see your job on the list, this means your job has completed. To view details on jobs that have completed, you can run jobhist . There are various ways of monitoring the output of your job.","title":"Where do my results go?"},{"location":"Job_Results/#output-and-error-files","text":"When writing your job script you can either tell it to start in the directory you submit it from ( -cwd ), or from a particular directory ( -wd <dir> ), or from your home directory (the default). When your job runs, it will create files in this directory for the job's output and errors: File Name Contents myscript.sh Your job script. myscript.sh.o12345 Output from the job. ( stdout ) myscript.sh.e12345 Errors, warnings, and other messages from the job that aren't mixed into the output. ( stderr ) myscript.sh.po12345 Output from the setup script run before a job. (\"prolog\") myscript.sh.pe12345 Output from the clean-up script run after a job. (\"epilog\") Normally there should be nothing in the .po and .pe files, and that's fine. If you change the name of the job in the queue, using the -N option, your output and error files will use that as the filename stem instead. Most programs will also produce separate output files, in a way that is particular to that program. Often these will be in the same directory, but that depends on the program and how you ran it.","title":"Output and error files"},{"location":"Job_Results/#grid-engine-commands","text":"The following commands can be used to submit and monitor a job.","title":"Grid Engine commands"},{"location":"Job_Results/#qsub","text":"This command submits your job to the batch queue. You can also use options on the command-line to override options you have put in your job script. Command Action qsub myscript.sh Submit the script as-is qsub -N NewName myscript.sh Submit the script but change the job's name qsub -l h_rt=24:0:0 myscript.sh Submit the script but change the maximum run-time qsub -hold_jid 12345 myscript.sh Submit the script but make it wait for job 12345 to finish qsub -ac allow=XYZ myscript.sh Submit the script but only let it run on node classes X, Y, and Z","title":"Qsub"},{"location":"Job_Results/#qstat","text":"This command shows the status of your jobs. When you run qstat with no options, all of your jobs currently running will be displayed. By adding in the option -f -j <job-ID> you will get more detail on the specified job.","title":"Qstat"},{"location":"Job_Results/#qdel","text":"This command deletes your job from the queue. When deleting a job will need to run qdel <job-ID> , however qdel '*' can be used to delete all jobs. To delete a batch of jobs, creating a file with the list of job IDs that you would like to delete and placing it in the following commands will delete the following jobs: cat <filename> | xargs qdel","title":"Qdel"},{"location":"Job_Results/#qsub-emailing","text":"We also have a mailing system that can be implemented to send emails with reminders of the status of your job through qsub . In your jobscript, or when you use qsub to submit your job, you can use the option -m . You can specify when you want an email sent to you by using the below options after qsub -m : b Mail is sent at the beginning of the job. e Mail is sent at the end of the job. a Mail is sent when the job is aborted or rescheduled. s Mail is sent when the job is suspended. n No mail is sent. (The default.) You can use more than one of these options by putting them together after the -m option; for example, adding the following to your job script would mean you get an email when the job begins and when it ends: #$ -m be","title":"Qsub emailing"},{"location":"New_Users/","text":"Guide for New Users \u00a7 What is a cluster? Which service(s) at UCL are right for me? How do I connect to the clusters? How do I transfer my data? How do I run a job? What does a jobscript look like? Where do my results go?","title":"Guide for New Users"},{"location":"New_Users/#guide-for-new-users","text":"What is a cluster? Which service(s) at UCL are right for me? How do I connect to the clusters? How do I transfer my data? How do I run a job? What does a jobscript look like? Where do my results go?","title":"Guide for New Users"},{"location":"Planned_Outages/","text":"Planned Outages \u00a7 Full details of outages are emailed to the cluster-specific user lists. Generally speaking, an outage will last from the morning of the first date listed until mid-morning of the end date listed. The nodes may need to be emptied of jobs in advance ('drained'), so jobs may remain in the queue for longer before an outage begins. If there is a notable delay in bringing the system back we will contact you after approximately midday - please don't email us at 9am on the listed end days! After an outage, the first day or two back should be considered 'at risk'; that is, things are more likely to go wrong without warning and we might need to make adjustments. Date Service Status Reason 02 Jul 2020 -> 09 Jul 2020 Michael Rollback Lustre software upgrade to fix bug. Full outage, no access. Upgrade was unusable, downgrading to previous. 01 May 2020 -> 11 May 2020 Myriad Completed Storage upgrade. 20 Mar 2020 -> 30 Mar 2020 Myriad Stopped, Postponed Issues found with new storage during outage. Myriad returned to service 24th. Sync data; switch to new storage. (Important: new metadata servers). Begins midday. Full outage, no access. 16 Mar 2020 -> (was 23) 26 Mar 2020 Michael Completed Firmware upgrades to bridge old and new networks. Extended to get jobs working on new nodes. 2 Mar 2020 -> 9 Mar 2020 Michael Completed Installation of phase 2 hardware, network bridging 10 Feb 2020 -> 17 Feb 2020 Myriad Cancelled Storage upgrade to 3PB","title":"Planned Outages"},{"location":"Planned_Outages/#planned-outages","text":"Full details of outages are emailed to the cluster-specific user lists. Generally speaking, an outage will last from the morning of the first date listed until mid-morning of the end date listed. The nodes may need to be emptied of jobs in advance ('drained'), so jobs may remain in the queue for longer before an outage begins. If there is a notable delay in bringing the system back we will contact you after approximately midday - please don't email us at 9am on the listed end days! After an outage, the first day or two back should be considered 'at risk'; that is, things are more likely to go wrong without warning and we might need to make adjustments. Date Service Status Reason 02 Jul 2020 -> 09 Jul 2020 Michael Rollback Lustre software upgrade to fix bug. Full outage, no access. Upgrade was unusable, downgrading to previous. 01 May 2020 -> 11 May 2020 Myriad Completed Storage upgrade. 20 Mar 2020 -> 30 Mar 2020 Myriad Stopped, Postponed Issues found with new storage during outage. Myriad returned to service 24th. Sync data; switch to new storage. (Important: new metadata servers). Begins midday. Full outage, no access. 16 Mar 2020 -> (was 23) 26 Mar 2020 Michael Completed Firmware upgrades to bridge old and new networks. Extended to get jobs working on new nodes. 2 Mar 2020 -> 9 Mar 2020 Michael Completed Installation of phase 2 hardware, network bridging 10 Feb 2020 -> 17 Feb 2020 Myriad Cancelled Storage upgrade to 3PB","title":"Planned Outages"},{"location":"Remote_Access/","text":"Warning This document describes a service that is being trialled and is not available for general use yet. Until it goes into service please continue to use Socrates for remote access. Remote Access to Research Computing Resources \u00a7 UCL's Research Computing services are accessible from inside the UCL firewall. If you wish to connect from outside, you need to either connect through a VPN or use SSH to log in to a machine accessible from outside and use that to \"jump\" through into the UCL network. Previously the recommended host for doing this was Socrates. Due to the degree to which this service is overloaded due to the prevailing pandemic increasing its use, ISD has built a new set of \"jump\" boxes which are designed to be fast, scalable and secure for access to Research Computing services. Connecting to the new jump boxes \u00a7 You can connect to the new jump boxes by connecting with your SSH client to: ssh.rc.ucl.ac.uk Once connected you can then log on to the UCL RC service you are using as normal. You can configure your ssh client to automatically connect via these jump boxes so that you make the connection in one step. Single-step logins using tunnelling \u00a7 Linux / Unix / Mac OS X \u00a7 On the command line \u00a7 # Log in to Grace, jumping via jump box ssh -o ProxyJump=ssh.rc.ucl.ac.uk grace.rc.ucl.ac.uk or # Copy 'my_file' from the machine you are logged in to into your Scratch on Grace scp -o ProxyJump=ssh.rc.ucl.ac.uk my_file grace.rc.ucl.ac.uk:~/Scratch/ This tunnels through the jump box service in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files. You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way. Using a config file \u00a7 You can create a config which does this without you needing to type it every time. Inside your ~/.ssh directory on your local machine, add the below to your config file (or create a file called config if you don't already have one). Generically, it should be of this form where <name> can be anything you want to call this entry. Host <name> User <remote_user_id> HostName <remote_hostname> proxyCommand ssh -W <remote_hostname>:22 <remote_user_id>@ssh.rc.ucl.ac.uk This causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host. Here are some examples - you can have as many of these as you need in your config file. Host myriad User ccxxxxx HostName myriad.rc.ucl.ac.uk proxyCommand ssh -W myriad.rc.ucl.ac.uk:22 ccxxxxx@ssh.rcucl.ac.uk Host login05 User ccxxxxx HostName login05.external.legion.ucl.ac.uk proxyCommand ssh -W login05.external.legion.ucl.ac.uk:22 ccxxxxx@ssh.rc.ucl.ac.uk Host aristotle User ccxxxxx HostName aristotle.rc.ucl.ac.uk proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 ccxxxxx@ssh.rc.ucl.ac.uk You can now just type ssh myriad or scp file1 aristotle:~ and you will go through Socrates. You'll be asked for login details twice since you're logging in to two machines, Socrates and your endpoint. File storage \u00a7 The jump boxes have extremely limited file storage space, intentionally, and should not be used for storing files - if you need to transfer files you should use the two-step process above. This storage should only be used for SSH configuration files. This storage is not mirrored across the jump boxes which means if you write a file to your home directory, you will not be able to read it if you are allocated to another jump box next time you log in. Key management \u00a7 !!! If you use SSH keys you absolutely MUST NOT STORE UNENCRYPTED PRIVATE KEYS ON THIS OR ANY OTHER MULTI-USER COMPUTER . We will be running regular scans of the filesystem to identify and then block unencrypted public keys across our services. There are currently two servers in the pool, ejp-gateway01 and ejp-gateway02 . Because the /home filesystem is not shared across the jump boxes, you need to sync SSH configuration files like ~/.ssh/authorized_keys across all the available jump boxes so that the change takes effect whichever jump box you are allocated to. You can see which machine you are logged into by the bash prompt. So for example, if on ejp-gateway02 then do: [ccaaxxx@ad.ucl.ac.uk@ejp-gateway02 ~]$ scp -r ~/.ssh ejp-gateway01: Password: known_hosts 100% 196 87.1KB/s 00:00 authorized_keys 100% 0 0.0KB/s 00:00 [ccaaxxx@ad.ucl.ac.uk@ejp-gateway02 ~]$ and similarly if on ejp-gateway01 do scp -r ~/.ssh ejp-gateway02:","title":"Remote Access to Research Computing Resources"},{"location":"Remote_Access/#remote-access-to-research-computing-resources","text":"UCL's Research Computing services are accessible from inside the UCL firewall. If you wish to connect from outside, you need to either connect through a VPN or use SSH to log in to a machine accessible from outside and use that to \"jump\" through into the UCL network. Previously the recommended host for doing this was Socrates. Due to the degree to which this service is overloaded due to the prevailing pandemic increasing its use, ISD has built a new set of \"jump\" boxes which are designed to be fast, scalable and secure for access to Research Computing services.","title":"Remote Access to Research Computing Resources"},{"location":"Remote_Access/#connecting-to-the-new-jump-boxes","text":"You can connect to the new jump boxes by connecting with your SSH client to: ssh.rc.ucl.ac.uk Once connected you can then log on to the UCL RC service you are using as normal. You can configure your ssh client to automatically connect via these jump boxes so that you make the connection in one step.","title":"Connecting to the new jump boxes"},{"location":"Remote_Access/#single-step-logins-using-tunnelling","text":"","title":"Single-step logins using tunnelling"},{"location":"Remote_Access/#linux-unix-mac-os-x","text":"","title":"Linux / Unix / Mac OS X"},{"location":"Remote_Access/#on-the-command-line","text":"# Log in to Grace, jumping via jump box ssh -o ProxyJump=ssh.rc.ucl.ac.uk grace.rc.ucl.ac.uk or # Copy 'my_file' from the machine you are logged in to into your Scratch on Grace scp -o ProxyJump=ssh.rc.ucl.ac.uk my_file grace.rc.ucl.ac.uk:~/Scratch/ This tunnels through the jump box service in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files. You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way.","title":"On the command line"},{"location":"Remote_Access/#using-a-config-file","text":"You can create a config which does this without you needing to type it every time. Inside your ~/.ssh directory on your local machine, add the below to your config file (or create a file called config if you don't already have one). Generically, it should be of this form where <name> can be anything you want to call this entry. Host <name> User <remote_user_id> HostName <remote_hostname> proxyCommand ssh -W <remote_hostname>:22 <remote_user_id>@ssh.rc.ucl.ac.uk This causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host. Here are some examples - you can have as many of these as you need in your config file. Host myriad User ccxxxxx HostName myriad.rc.ucl.ac.uk proxyCommand ssh -W myriad.rc.ucl.ac.uk:22 ccxxxxx@ssh.rcucl.ac.uk Host login05 User ccxxxxx HostName login05.external.legion.ucl.ac.uk proxyCommand ssh -W login05.external.legion.ucl.ac.uk:22 ccxxxxx@ssh.rc.ucl.ac.uk Host aristotle User ccxxxxx HostName aristotle.rc.ucl.ac.uk proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 ccxxxxx@ssh.rc.ucl.ac.uk You can now just type ssh myriad or scp file1 aristotle:~ and you will go through Socrates. You'll be asked for login details twice since you're logging in to two machines, Socrates and your endpoint.","title":"Using a config file"},{"location":"Remote_Access/#file-storage","text":"The jump boxes have extremely limited file storage space, intentionally, and should not be used for storing files - if you need to transfer files you should use the two-step process above. This storage should only be used for SSH configuration files. This storage is not mirrored across the jump boxes which means if you write a file to your home directory, you will not be able to read it if you are allocated to another jump box next time you log in.","title":"File storage"},{"location":"Remote_Access/#key-management","text":"!!! If you use SSH keys you absolutely MUST NOT STORE UNENCRYPTED PRIVATE KEYS ON THIS OR ANY OTHER MULTI-USER COMPUTER . We will be running regular scans of the filesystem to identify and then block unencrypted public keys across our services. There are currently two servers in the pool, ejp-gateway01 and ejp-gateway02 . Because the /home filesystem is not shared across the jump boxes, you need to sync SSH configuration files like ~/.ssh/authorized_keys across all the available jump boxes so that the change takes effect whichever jump box you are allocated to. You can see which machine you are logged into by the bash prompt. So for example, if on ejp-gateway02 then do: [ccaaxxx@ad.ucl.ac.uk@ejp-gateway02 ~]$ scp -r ~/.ssh ejp-gateway01: Password: known_hosts 100% 196 87.1KB/s 00:00 authorized_keys 100% 0 0.0KB/s 00:00 [ccaaxxx@ad.ucl.ac.uk@ejp-gateway02 ~]$ and similarly if on ejp-gateway01 do scp -r ~/.ssh ejp-gateway02:","title":"Key management"},{"location":"Terms_and_Conditions/","text":"Terms and Conditions \u00a7 All use of Research Computing Platforms is subject to the UCL Computing Regulations. All users will be required to renew their account once per year. Users will receive a reminder one month prior to suspension of their Myriad account sent to their Live@UCL e-mail address. Funding information will need to be provided upon application, and publication information upon renewal. Users are forbidden from performing production runs on the login nodes. The Research Computing Platform Services Team reserve the right to suspend or ban without prior warning any use of the system which impairs its operation. With the exception of in cases where there is imminent harm or risk to the service, the Research Computing Platform Services Team will not access your files without permission. Official service notifications are sent to the myriad-users (or the equivalent for other services) mailing list. Users are automatically subscribed to this list using their Live@UCL e-mail address and should read notices sent there. The Research Computing Platform Services Team reserve the right to suspend users' accounts, without notice, in the event of a user being the subject of any UCL disciplinary procedure, or where a user is found to be in breach of UCL\u2019s Computing Regulations or best practice guidelines regarding password management, as provided by Information Services Division. Users are required to acknowledge their use of Myriad and associated research computing services in any publications describing research that has been conducted, in any part, on Myriad. The following words should be used: \"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work\". All support requests should be sent by e-mail to rc-support@ucl.ac.uk.","title":"Terms and Conditions"},{"location":"Terms_and_Conditions/#terms-and-conditions","text":"All use of Research Computing Platforms is subject to the UCL Computing Regulations. All users will be required to renew their account once per year. Users will receive a reminder one month prior to suspension of their Myriad account sent to their Live@UCL e-mail address. Funding information will need to be provided upon application, and publication information upon renewal. Users are forbidden from performing production runs on the login nodes. The Research Computing Platform Services Team reserve the right to suspend or ban without prior warning any use of the system which impairs its operation. With the exception of in cases where there is imminent harm or risk to the service, the Research Computing Platform Services Team will not access your files without permission. Official service notifications are sent to the myriad-users (or the equivalent for other services) mailing list. Users are automatically subscribed to this list using their Live@UCL e-mail address and should read notices sent there. The Research Computing Platform Services Team reserve the right to suspend users' accounts, without notice, in the event of a user being the subject of any UCL disciplinary procedure, or where a user is found to be in breach of UCL\u2019s Computing Regulations or best practice guidelines regarding password management, as provided by Information Services Division. Users are required to acknowledge their use of Myriad and associated research computing services in any publications describing research that has been conducted, in any part, on Myriad. The following words should be used: \"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work\". All support requests should be sent by e-mail to rc-support@ucl.ac.uk.","title":"Terms and Conditions"},{"location":"UCL_Service_For_Me/","text":"Which service(s) at UCL are right for me? \u00a7 Depending on the type of jobs you would like to run each cluster can have different requirements that your jobs must meet. When you submit your user account application form you will be given access to the cluster depending on resources selected. Currently we categorise intended workloads into ones that use: Individual single core jobs Large numbers (>1000) of single core jobs Multithreaded jobs Extremely large quantities of RAM (>64GB) Small MPI jobs (<32 cores) Medium-sized MPI jobs (32-256 cores) Large-sized MPI jobs (>256 cores) At least one GPGPU At least ten GPGPUs Each cluster has its own specifications for the types of jobs that they run which is all dependable on list above. The cluster machines we have available are: Myriad Myriad is designed to be most suitable for serial work, including large numbers of serial jobs, and multi-threaded jobs (using e.g. OpenMP). It also includes a small number of GPUs for development or testing work. It went into service in July 2018. See Myriad . Kathleen Kathleen is intended for multi-node jobs (using e.g. MPI) and went into service in Feb 2020 as a replacement for Grace. We recommend using Kathleen if you intend to use more than 36 cores per job. See Kathleen . Grace Grace is intended for multi-node jobs (using e.g. MPI) and went into service in Dec 2015. We recommend using Grace if you intend to use more than 36 cores per job. Grace will run until around the first quarter of 2021. See Grace . Legion The Legion service was an older multipurpose cluster, including capability for serial jobs, multi-threaded jobs, and small multi-node jobs. It is currently being decommissioned and is not running new jobs for general users. Data access ceases entirely by 1 May 2020. Thomas Thomas is the UK's Tier 2 Materials and Molecular Modelling Hub. It is accessible by members of partner institutions and relevant consortia, and is for materials and molecular modelling work only. It has separate access procedures from UCL's central clusters. Access is managed by a Point of Contact from the relevant institution or consortia, not by Research Computing. See Thomas . Michael Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, an EPSRC-funded Tier 2 system providing large scale computation to UK researchers; and delivers computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology. Access is managed by a Point of Contact from the Faraday Institution, not by Research Computing. See Michael .","title":"UCL Service For Me"},{"location":"UCL_Service_For_Me/#which-services-at-ucl-are-right-for-me","text":"Depending on the type of jobs you would like to run each cluster can have different requirements that your jobs must meet. When you submit your user account application form you will be given access to the cluster depending on resources selected. Currently we categorise intended workloads into ones that use: Individual single core jobs Large numbers (>1000) of single core jobs Multithreaded jobs Extremely large quantities of RAM (>64GB) Small MPI jobs (<32 cores) Medium-sized MPI jobs (32-256 cores) Large-sized MPI jobs (>256 cores) At least one GPGPU At least ten GPGPUs Each cluster has its own specifications for the types of jobs that they run which is all dependable on list above. The cluster machines we have available are: Myriad Myriad is designed to be most suitable for serial work, including large numbers of serial jobs, and multi-threaded jobs (using e.g. OpenMP). It also includes a small number of GPUs for development or testing work. It went into service in July 2018. See Myriad . Kathleen Kathleen is intended for multi-node jobs (using e.g. MPI) and went into service in Feb 2020 as a replacement for Grace. We recommend using Kathleen if you intend to use more than 36 cores per job. See Kathleen . Grace Grace is intended for multi-node jobs (using e.g. MPI) and went into service in Dec 2015. We recommend using Grace if you intend to use more than 36 cores per job. Grace will run until around the first quarter of 2021. See Grace . Legion The Legion service was an older multipurpose cluster, including capability for serial jobs, multi-threaded jobs, and small multi-node jobs. It is currently being decommissioned and is not running new jobs for general users. Data access ceases entirely by 1 May 2020. Thomas Thomas is the UK's Tier 2 Materials and Molecular Modelling Hub. It is accessible by members of partner institutions and relevant consortia, and is for materials and molecular modelling work only. It has separate access procedures from UCL's central clusters. Access is managed by a Point of Contact from the relevant institution or consortia, not by Research Computing. See Thomas . Michael Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, an EPSRC-funded Tier 2 system providing large scale computation to UK researchers; and delivers computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology. Access is managed by a Point of Contact from the Faraday Institution, not by Research Computing. See Michael .","title":"Which service(s) at UCL are right for me?"},{"location":"howto/","text":"How do I? \u00a7 I have an account, now: How do I log in? \u00a7 Logging in is most straightforward if you are inside the UCL firewall. If you are logging in from home or other external networks then you first have to get on to the UCL network . Linux / Unix / Mac OS X \u00a7 Use the terminal and type the below command to secure shell (ssh) into the machine you wish to access. Replace <your_UCL_user_id> with your central UCL username, and <system_name> with the name of the machine you want to log in to, eg. legion , grace , aristotle . ssh <your_UCL_user_id>@<system_name>.rc.ucl.ac.uk Windows \u00a7 On Windows you need something that will give you a suitable terminal and ssh - usually PuTTY, although you could also use Cygwin if you wanted a full Linux-like environment. Using PuTTY \u00a7 PuTTY is a common SSH client on Windows and is available on Desktop@UCL. You can find it under: Start > All Programs > Applications O-P > PuTTY You will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY. In newer versions of PuTTY, it looks like this. TODO: new putty You will then be asked to enter your username and password. Only enter your username, not @legion.rc.ucl.ac.uk . The password field will remain entirely blank when you type in to it - it does not show placeholders to indicate you have typed something. Logging in from outside the UCL firewall \u00a7 You will need to either use the UCL Virtual Private Network or ssh in to UCL's gateway socrates.ucl.ac.uk first. From Socrates you can then ssh in to our systems. ssh <your_UCL_user_id>@socrates.ucl.ac.uk ssh <your_UCL_user_id>@<system_name>.rc.ucl.ac.uk Note: the default shell (what you are typing commands into) on Socrates is csh rather than bash and so your default prompt will look different and use a % rather than a $ as the dividing character: 25 % ls html.pub/ 26 % Advanced: If you find you need to go via Socrates often, you can set up this jump automatically, see Single-step logins using tunnelling Login problems \u00a7 If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you have recently updated your password, it takes some hours to propagate to all UCL systems. If you still cannot get access but can access other UCL services like Socrates, please contact us on rc-support@ucl.ac.uk. Your account may have expired, or you may have gone over quota. If you cannot access anything, please see UCL MyAccount - you may need to request a password reset from the Service Desk. If you get a host key error message, you will need to delete old host keys - continue reading! Remote host identification has changed \u00a7 When you log in via SSH, it keeps a record of the host key for the server you logged in to in your .ssh/known_hosts file in your home directory, on the machine you are logging in from. This helps make sure you are connecting directly to the server you think you are, but can cause warnings to show up if the host key on that machine has genuinely changed (usually because of an update or reinstall). Check the host key warning against our current key fingerprints : The error message looks like this if you are using OpenSSH in a terminal: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA. Please contact your system administrator. Add correct host key in /Users/uccaxxx/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /Users/uccaxxx/.ssh/known_hosts:11 ECDSA host key for myriad.rc.ucl.ac.uk has changed and you have requested strict checking. Host key verification failed. Killed by signal 1. This tells you that the old key is in line 11 of your known_hosts file. Sometimes it will give you a direct command you can run to remove that specific key: ssh-keygen -R myriad.rc.ucl.ac.uk or you can manually delete line 11 yourself in a text editor. If you are logging in via Socrates, you will need to remove the old key there too. On Socrates, pico and vi are available text editors. If you are not already familiar with vi , use pico - it has the command shortcuts shown at the bottom, where ^O means press Ctrl and then the letter o . # to open the file for editing in pico pico ~/.ssh/known_hosts Once you have removed the old host key you will be able to ssh in again. The first time you log in to an unknown server you will get a message like this: The authenticity of host 'myriad.rc.ucl.ac.uk can't be established. ECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA. Are you sure you want to continue connecting (yes/no)? Typing yes will allow you to continue logging in. PuTTY will display a warning and you can choose to continue or not. WinSCP will say Server's host key does not match the one that WinSCP has in cache. and you will have the option to update the key. How do I transfer data onto the system? \u00a7 You can transfer data to and from our systems using any program capable of using the Secure Copy (SCP) protocol. This uses the same SSH system as you use to log in to a command line session, but then transfers data over it. This means that if you can use SSH to connect to a system, you can usually use SCP to transfer files to it. Copying files using Linux or Mac OS X \u00a7 You can use the command-line utilities scp, sftp or rsync to copy your data about. You can also use a graphical client (Transmit, CyberDuck, FileZilla). scp \u00a7 This will copy a data file from somewhere on your local machine to a specified location on the remote machine (Legion, Grace etc). scp <local_data_file> <remote_user_id>@<remote_hostname>:<remote_path> # Example: copy myfile from your local current directory into Scratch on Legion scp myfile ccxxxxx@legion.rc.ucl.ac.uk:~/Scratch/ This will do the reverse, copying from the remote machine to your local machine. (This is still run from your local machine). scp <remote_user_id>@<remote_hostname>:<remote_path><remote_data_file> <local_path> # Example: copy myfile from Legion into the Backups directory in your local current directory scp ccxxxxx@legion.rc.ucl.ac.uk:~/Scratch/myfile Backups/ sftp \u00a7 You can use sftp to log in to the remote machine, navigate through directories and use put and get to copy files from and to your local machine. lcd and lls are local equivalents of cd and ls so you can navigate through your local directories as you go. sftp <remote_user_id>@<remote_hostname> cd <remote_path> get <remote_file> lcd <local_path> put <local_file> # Example: download a copy of file1 into your local current directory, # change local directory and upload a copy of file2 sftp ccxxxxx@legion.rc.ucl.ac.uk cd Scratch/files get file1 lcd ../files_to_upload put file2 rsync \u00a7 Rsync is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at man rsync as there are many options. Copying files using Windows and WinSCP \u00a7 WinSCP is a graphical client that you can use for scp or sftp. The login/create new session screen will open if this is the first time you are using WinSCP. You can choose SFTP or SCP as the file protocol. If you have an unstable connection with one, you may wish to try the other. SCP is probably generally better. Fill in the hostname of the machine you wish to connect to, your username and password. Click Save and give your settings a useful name. You'll then be shown your list of Stored sessions, which will have the one you just created. Select the session and click Login. Transferring files from outside the UCL firewall \u00a7 As when logging in, when you are outside the UCL firewall you will need a method to connect inside it before you copy files. (You do not want to be copying files on to Socrates and then on to our systems - this is slow, unnecessary, and it means you need space available on Socrates too). You can use the UCL Virtual Private Network and scp direct to our systems or you can do some form of ssh tunnelling. Single-step logins using tunnelling \u00a7 Linux / Unix / Mac OS X \u00a7 On the command line \u00a7 # Log in to Grace, jumping via Socrates ssh -o ProxyJump=socrates.ucl.ac.uk grace.rc.ucl.ac.uk or # Copy 'my_file' from the machine you are logged in to into your Scratch on Grace scp -o ProxyJump=socrates.ucl.ac.uk my_file grace.rc.ucl.ac.uk:~/Scratch/ This tunnels through Socrates in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files. You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way. Using a config file \u00a7 You can create a config which does this without you needing to type it every time. Inside your ~/.ssh directory on your local machine, add the below to your config file (or create a file called config if you don't already have one). Generically, it should be of this form where <name> can be anything you want to call this entry. Host <name> User <remote_user_id> HostName <remote_hostname> proxyCommand ssh -W <remote_hostname>:22 <remote_user_id>@socrates.ucl.ac.uk This causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host. Here are some examples - you can have as many of these as you need in your config file. Host myriad User ccxxxxx HostName myriad.rc.ucl.ac.uk proxyCommand ssh -W myriad.rc.ucl.ac.uk:22 ccxxxxx@socrates.ucl.ac.uk Host login05 User ccxxxxx HostName login05.external.legion.ucl.ac.uk proxyCommand ssh -W login05.external.legion.ucl.ac.uk:22 ccxxxxx@socrates.ucl.ac.uk Host aristotle User ccxxxxx HostName aristotle.rc.ucl.ac.uk proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 ccxxxxx@socrates.ucl.ac.uk You can now just type ssh myriad or scp file1 aristotle:~ and you will go through Socrates. You'll be asked for login details twice since you're logging in to two machines, Socrates and your endpoint. Windows - WinSCP \u00a7 WinSCP can also set up SSH tunnels. Create a new session as before, and tick the Advanced options box in the bottom left corner. Select Connection > Tunnel from the left pane. Tick the Connect through SSH tunnel box and enter the hostname of the gateway you are tunnelling through, for example socrates.ucl.ac.uk Fill in your username and password for that host. (Central UCL ones for Socrates). Select Session from the left pane and fill in the hostname you want to end up on after the tunnel. Fill in your username and password for that host and set the file protocol to SCP. Save your settings with a useful name. Windows - PuTTY \u00a7 You can use PuTTY for tunnelling when you just want a single-step login and not a file transfer. Managing your quota \u00a7 After using lquota to see your total usage, you may wish to find what is using all your space. du is a command that gives you information about your disk usage. Useful options are: du -ch <dir> du -h --max-depth=1 The first will give you a summary of the sizes of directory tree and subtrees inside the directory you specify, using human-readable sizes with a total at the bottom. The second will show you the totals for all top-level directories relative to where you are, plus the grand total. These can help you track down the locations of large amounts of data if you need to reduce your disk usage. How do I submit a job to the scheduler? \u00a7 To submit a job to the scheduler you need to write a jobscript that contains the resources the job is asking for and the actual commands you want to run. This jobscript is then submitted using the qsub command. qsub myjobscript It will be put in to the queue and will begin running on the compute nodes at some point later when it has been allocated resources. Passing in qsub options on the command line \u00a7 The #$ lines in your jobscript are options to qsub. It will take each line which has #$ as the first two characters and use the contents beyond that as an option. You can also pass options directly to the qsub command and this will override the settings in your script. This can be useful if you are scripting your job submissions in more complicated ways. For example, if you want to change the name of the job for this one instance of the job you can submit your script with: qsub -N NewName myscript.sh Or if you want to increase the wall-clock time to 24 hours: qsub -l h_rt=24:0:0 myscript.sh You can submit jobs with dependencies by using the -hold_jid option. For example, the command below submits a job that won't run until job 12345 has finished: qsub -hold_jid 12345 myscript.sh You may specify node type with the -ac allow= flags as below: qsub -ac allow=XYZ myscript.sh That would restrict the job to running on nodes of type X, Y or Z (the older Legion nodes). Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line whenever possible. We (and you) can see the exact jobscript that was submitted for every job that ran but not what command line options you submitted it with. Checking your previous jobscripts \u00a7 If you want to check what you submitted for a specific job ID, you can do it with the scriptfor utility. scriptfor 12345 As mentioned above, this will not show any command line options you passed in. How do I monitor a job? \u00a7 qstat \u00a7 The qstat command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no-one else\u2019s). This makes it easier to keep track of your jobs. The output will look something like this: job-ID prior name user state submit/start at queue slots ja-task-ID ----------------------------------------------------------------------------------------------------------------- 123454 2.00685 DI_m3 ccxxxxx Eqw 10/13/2017 15:29:11 12 123456 2.00685 DI_m3 ccxxxxx r 10/13/2017 15:29:11 Yorick@node-x02e-006 24 123457 2.00398 DI_m2 ucappka qw 10/12/2017 14:42:12 1 This shows you the job ID, the numeric priority the scheduler has assigned to the job, the name you have given the job, your username, the state the job is in, the date and time it was submitted at (or started at, if it has begun), the head node of the job, the number of 'slots' it is taking up, and if it is an array job the last column shows the task ID. The queue name ( Yorick here) is generally not useful. The head node name ( node-x02e-006 ) is useful - the node-x part tells you this is an X-type node. If you want to get more information on a particular job, note its job ID and then use the -f and -j flags to get full output about that job. Most of this information is not very useful. qstat -f -j 12345 Job states \u00a7 qw : queueing, waiting r : running Rq : a pre-job check on a node failed and this job was put back in the queue Rr : this job was rescheduled but is now running on a new node Eqw : there was an error in this jobscript. This will not run. t : this job is being transferred dr : this job is being deleted Many jobs cycling between Rq and Rr generally means there is a dodgy compute node which is failing pre-job checks, but is free so everything tries to run there. In this case, let us know and we will investigate. If a job stays in t or dr state for a long time, the node it was on is likely to be unresponsive - again let us know and we'll investigate. A job in Eqw will remain in that state until you delete it - you should first have a look at what the error was with qexplain . qexplain \u00a7 This is a utility to show you the non-truncated error reported by your job. qstat -j will show you a truncated version near the bottom of the output. qexplain 123454 qdel \u00a7 You use qdel to delete a job from the queue. qdel 123454 You can delete all your jobs at once: qdel '*' More scheduler commands \u00a7 Have a look at man qstat and note the commands shown in the SEE ALSO section of the manual page. Exit the manual page and then look at the man pages for those. (You will not be able to run all commands). nodesforjob \u00a7 This is a utility that shows you the current percentage load, memory used and swap used on the nodes your job is running on. If your job is sharing the node with other people's jobs, it will show you the total resources in use, not just those used by your job. This is a snapshot of the current time and resource usage may change over the course of your job. Bear in mind that memory use in particular can increase over time as your job runs. If a cluster has hyperthreading enabled and you aren't using it, full load will show as 50% and not 100% - this is normal and not a problem. For a parallel job, very low (or zero) usage of any of the nodes suggests your job is either not capable of running over multiple nodes, or not partitioning its work effectively - you may be asking for more cores than it can use, or asking for a number of cores that doesn't fit well into the node sizes, leaving many idle. [uccacxx@login02 ~]$ nodesforjob 1234 Nodes for job 1234: Primary: node-r99a-238: 103.1 % load, 12.9 % memory used, 0.1% swap used Secondaries: node-r99a-206: 1.7 % load, 1.6 % memory used, 0.1% swap used node-r99a-238: 103.1 % load, 12.9 % memory used, 0.1% swap used node-r99a-292: 103.1 % load, 12.9 % memory used, 0.1% swap used node-r99a-651: 1.6 % load, 3.2 % memory used, 0.1% swap used The above example shows a multi-node job, so all the usage belongs to this job itself. It is running on four nodes, and node-r99a-238 is the head node (the one that launched the job) and shows up in both Primary and Secondaries. The load is very unbalanced - it is using two nodes flat out, and two are mostly doing nothing. Memory use is low. Swap use is essentially zero. jobhist \u00a7 Once a job ends, it no longer shows up in qstat . To see information about your finished jobs - when they started, when they ended, what node they ran on - type jobhist . [uccacxx@login02 ~]$ jobhist FSTIME | FETIME | HOSTNAME | OWNER | JOB NUMBER | TASK NUMBER | EXIT STATUS | JOB NAME ----------------------+---------------------+---------------+---------+------------+-------------+-------------+--------------- 2020-06-17 16:31:12 | 2020-06-17 16:34:19 | node-h00a-010 | uccacxx | 3854822 | 0 | 0 | m_job 2020-06-17 16:56:50 | 2020-06-17 16:56:52 | node-d00a-023 | uccacxx | 3854836 | 0 | 1 | k_job 2020-06-17 17:21:12 | 2020-06-17 17:21:46 | node-d00a-012 | uccacxx | 3854859 | 0 | 0 | k_job FSTIME - when the job started running on the node FETIME - when the job ended HOSTNAME - the head node of the job (if it ran on multiple nodes, it only lists the first) TASK NUMBER - if it was an array job, it will have a different number here for each task This shows jobs that finished in the last 24 hours by default. You can search for longer as well: jobhist --hours=200 If a job ended and didn't create the files you expect, check the start and end times to see whether it ran out of wallclock time. If a job only ran for seconds and didn't produce the expected output, there was probably something wrong in your script - check the .o and .e files in the directory you submitted the job from for errors. How do I run interactive jobs? \u00a7 Sometimes you need to run interactive programs, sometimes with a GUI. This can be achieved through qrsh . We have a detailed guide to running interactive jobs . How do I estimate what resources to request in my jobscript? \u00a7 It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. Run your program as: /usr/bin/time --verbose myprogram myargs where myprogram myargs is however you normally run your program, with whatever options you pass to it. When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is maxrss (maximum resident set size) which roughly tells you the largest amount of memory it used. Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible - if you increase it too much you may end up with a job that cannot be submitted. You can also look at nodesforjob while a job is running to see a snapshot of the memory, swap and load on the nodes your job is running on. How can I see what types of node a cluster has? \u00a7 As well as looking at the cluster-specific page in this documentation for more details (for example Myriad ), you can run nodetypes , which will give you basic information about the nodes that exist in that cluster. [uccacxx@login12 ~]$ nodetypes Unknown node type: util02 Unknown node type: util05 2 type * nodes: 36 cores, 188.4G RAM 7 type B nodes: 36 cores, 1.5T RAM 66 type D nodes: 36 cores, 188.4G RAM 9 type E nodes: 36 cores, 188.4G RAM 1 type F nodes: 36 cores, 188.4G RAM 3 type H nodes: 36 cores, 172.7G RAM 53 type H nodes: 36 cores, 188.4G RAM 3 type I nodes: 36 cores, 1.5T RAM 2 type J nodes: 36 cores, 188.4G RAM This shows how many of each letter-labelled nodetype the cluster has, then the number of cores and amount of memory the node is reporting it has. It also shows the cluster has some utility nodes - those are part of the infrastructure. The * nodes are the login nodes. How do I run a graphical program? \u00a7 To run a graphical program on the cluster and be able to view the user interface on your own local computer, you will need to have an X-Windows Server installed on your local computer and use X-forwarding. X-forwarding on Linux \u00a7 Desktop Linux operating systems already have X-Windows installed, so you just need to ssh in with the correct flags. You need to make sure you use either the -X or -Y (look at man ssh for details) flags on all ssh commands you run to establish a connection to the cluster. For example, connecting from outside of UCL: ssh -X <your_UCL_user_id>@socrates.ucl.ac.uk and then ssh -X <your_UCL_user_id>@myriad.rc.ucl.ac.uk X-forwarding on Mac OS X \u00a7 You will need to install XQuartz to provide an X-Window System for Mac OS X. (Previously known as X11.app). You can then follow the Linux instructions using Terminal.app. X-forwarding on Windows \u00a7 You will need: An SSH client; e.g., PuTTY An X server program; e.g., Exceed, Xming Exceed is available on Desktop@UCL machines and downloadable from the UCL software database . Xming is open source (and mentioned here without testing). Exceed on Desktop@UCL \u00a7 Load Exceed. You can find it under Start > All Programs > Applications O-P > Open Text Exceed 14 > Exceed Open PuTTY (Applications O-P > PuTTY) In PuTTY, set up the connection with the host machine as usual: Host name: myriad.rc.ucl.ac.uk (for example) Port: 22 Connection type: SSH Then, from the Category menu, select Connection > SSH > X11 for 'Options controlling SSH X11 forwarding'. Make sure the box marked 'Enable X11 forwarding' is checked. Return to the session menu and save these settings with a new identifiable name for reuse in future. Click 'Open' and login to the host as usual To test that X-forwarding is working, try running nedit which is a text editor in our default modules. If nedit works, you have successfully enabled X-forwarding for graphical applications. Installing Xming \u00a7 Xming is a popular open source X server for Windows. These are instructions for using it alongside PuTTY. Other SSH clients and X servers are available. We cannot verify how well it may be working. Install both PuTTY and Xming if you have not done so already. During Xming installation, choose not to install an SSH client. Open Xming - the Xming icon should appear on the task bar. Open PuTTY Set up PuTTY as shown in the Exceed section.","title":"How do I?"},{"location":"howto/#how-do-i","text":"I have an account, now:","title":"How do I?"},{"location":"howto/#how-do-i-log-in","text":"Logging in is most straightforward if you are inside the UCL firewall. If you are logging in from home or other external networks then you first have to get on to the UCL network .","title":"How do I log in?"},{"location":"howto/#linux-unix-mac-os-x","text":"Use the terminal and type the below command to secure shell (ssh) into the machine you wish to access. Replace <your_UCL_user_id> with your central UCL username, and <system_name> with the name of the machine you want to log in to, eg. legion , grace , aristotle . ssh <your_UCL_user_id>@<system_name>.rc.ucl.ac.uk","title":"Linux / Unix / Mac OS X"},{"location":"howto/#windows","text":"On Windows you need something that will give you a suitable terminal and ssh - usually PuTTY, although you could also use Cygwin if you wanted a full Linux-like environment.","title":"Windows"},{"location":"howto/#using-putty","text":"PuTTY is a common SSH client on Windows and is available on Desktop@UCL. You can find it under: Start > All Programs > Applications O-P > PuTTY You will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY. In newer versions of PuTTY, it looks like this. TODO: new putty You will then be asked to enter your username and password. Only enter your username, not @legion.rc.ucl.ac.uk . The password field will remain entirely blank when you type in to it - it does not show placeholders to indicate you have typed something.","title":"Using PuTTY"},{"location":"howto/#logging-in-from-outside-the-ucl-firewall","text":"You will need to either use the UCL Virtual Private Network or ssh in to UCL's gateway socrates.ucl.ac.uk first. From Socrates you can then ssh in to our systems. ssh <your_UCL_user_id>@socrates.ucl.ac.uk ssh <your_UCL_user_id>@<system_name>.rc.ucl.ac.uk Note: the default shell (what you are typing commands into) on Socrates is csh rather than bash and so your default prompt will look different and use a % rather than a $ as the dividing character: 25 % ls html.pub/ 26 % Advanced: If you find you need to go via Socrates often, you can set up this jump automatically, see Single-step logins using tunnelling","title":"Logging in from outside the UCL firewall"},{"location":"howto/#login-problems","text":"If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you have recently updated your password, it takes some hours to propagate to all UCL systems. If you still cannot get access but can access other UCL services like Socrates, please contact us on rc-support@ucl.ac.uk. Your account may have expired, or you may have gone over quota. If you cannot access anything, please see UCL MyAccount - you may need to request a password reset from the Service Desk. If you get a host key error message, you will need to delete old host keys - continue reading!","title":"Login problems"},{"location":"howto/#remote-host-identification-has-changed","text":"When you log in via SSH, it keeps a record of the host key for the server you logged in to in your .ssh/known_hosts file in your home directory, on the machine you are logging in from. This helps make sure you are connecting directly to the server you think you are, but can cause warnings to show up if the host key on that machine has genuinely changed (usually because of an update or reinstall). Check the host key warning against our current key fingerprints : The error message looks like this if you are using OpenSSH in a terminal: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA. Please contact your system administrator. Add correct host key in /Users/uccaxxx/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /Users/uccaxxx/.ssh/known_hosts:11 ECDSA host key for myriad.rc.ucl.ac.uk has changed and you have requested strict checking. Host key verification failed. Killed by signal 1. This tells you that the old key is in line 11 of your known_hosts file. Sometimes it will give you a direct command you can run to remove that specific key: ssh-keygen -R myriad.rc.ucl.ac.uk or you can manually delete line 11 yourself in a text editor. If you are logging in via Socrates, you will need to remove the old key there too. On Socrates, pico and vi are available text editors. If you are not already familiar with vi , use pico - it has the command shortcuts shown at the bottom, where ^O means press Ctrl and then the letter o . # to open the file for editing in pico pico ~/.ssh/known_hosts Once you have removed the old host key you will be able to ssh in again. The first time you log in to an unknown server you will get a message like this: The authenticity of host 'myriad.rc.ucl.ac.uk can't be established. ECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA. Are you sure you want to continue connecting (yes/no)? Typing yes will allow you to continue logging in. PuTTY will display a warning and you can choose to continue or not. WinSCP will say Server's host key does not match the one that WinSCP has in cache. and you will have the option to update the key.","title":"Remote host identification has changed"},{"location":"howto/#how-do-i-transfer-data-onto-the-system","text":"You can transfer data to and from our systems using any program capable of using the Secure Copy (SCP) protocol. This uses the same SSH system as you use to log in to a command line session, but then transfers data over it. This means that if you can use SSH to connect to a system, you can usually use SCP to transfer files to it.","title":"How do I transfer data onto the system?"},{"location":"howto/#copying-files-using-linux-or-mac-os-x","text":"You can use the command-line utilities scp, sftp or rsync to copy your data about. You can also use a graphical client (Transmit, CyberDuck, FileZilla).","title":"Copying files using Linux or Mac OS X"},{"location":"howto/#scp","text":"This will copy a data file from somewhere on your local machine to a specified location on the remote machine (Legion, Grace etc). scp <local_data_file> <remote_user_id>@<remote_hostname>:<remote_path> # Example: copy myfile from your local current directory into Scratch on Legion scp myfile ccxxxxx@legion.rc.ucl.ac.uk:~/Scratch/ This will do the reverse, copying from the remote machine to your local machine. (This is still run from your local machine). scp <remote_user_id>@<remote_hostname>:<remote_path><remote_data_file> <local_path> # Example: copy myfile from Legion into the Backups directory in your local current directory scp ccxxxxx@legion.rc.ucl.ac.uk:~/Scratch/myfile Backups/","title":"scp"},{"location":"howto/#sftp","text":"You can use sftp to log in to the remote machine, navigate through directories and use put and get to copy files from and to your local machine. lcd and lls are local equivalents of cd and ls so you can navigate through your local directories as you go. sftp <remote_user_id>@<remote_hostname> cd <remote_path> get <remote_file> lcd <local_path> put <local_file> # Example: download a copy of file1 into your local current directory, # change local directory and upload a copy of file2 sftp ccxxxxx@legion.rc.ucl.ac.uk cd Scratch/files get file1 lcd ../files_to_upload put file2","title":"sftp"},{"location":"howto/#rsync","text":"Rsync is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at man rsync as there are many options.","title":"rsync"},{"location":"howto/#copying-files-using-windows-and-winscp","text":"WinSCP is a graphical client that you can use for scp or sftp. The login/create new session screen will open if this is the first time you are using WinSCP. You can choose SFTP or SCP as the file protocol. If you have an unstable connection with one, you may wish to try the other. SCP is probably generally better. Fill in the hostname of the machine you wish to connect to, your username and password. Click Save and give your settings a useful name. You'll then be shown your list of Stored sessions, which will have the one you just created. Select the session and click Login.","title":"Copying files using Windows and WinSCP"},{"location":"howto/#transferring-files-from-outside-the-ucl-firewall","text":"As when logging in, when you are outside the UCL firewall you will need a method to connect inside it before you copy files. (You do not want to be copying files on to Socrates and then on to our systems - this is slow, unnecessary, and it means you need space available on Socrates too). You can use the UCL Virtual Private Network and scp direct to our systems or you can do some form of ssh tunnelling.","title":"Transferring files from outside the UCL firewall"},{"location":"howto/#single-step-logins-using-tunnelling","text":"","title":"Single-step logins using tunnelling"},{"location":"howto/#linux-unix-mac-os-x_1","text":"","title":"Linux / Unix / Mac OS X"},{"location":"howto/#on-the-command-line","text":"# Log in to Grace, jumping via Socrates ssh -o ProxyJump=socrates.ucl.ac.uk grace.rc.ucl.ac.uk or # Copy 'my_file' from the machine you are logged in to into your Scratch on Grace scp -o ProxyJump=socrates.ucl.ac.uk my_file grace.rc.ucl.ac.uk:~/Scratch/ This tunnels through Socrates in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files. You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way.","title":"On the command line"},{"location":"howto/#using-a-config-file","text":"You can create a config which does this without you needing to type it every time. Inside your ~/.ssh directory on your local machine, add the below to your config file (or create a file called config if you don't already have one). Generically, it should be of this form where <name> can be anything you want to call this entry. Host <name> User <remote_user_id> HostName <remote_hostname> proxyCommand ssh -W <remote_hostname>:22 <remote_user_id>@socrates.ucl.ac.uk This causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host. Here are some examples - you can have as many of these as you need in your config file. Host myriad User ccxxxxx HostName myriad.rc.ucl.ac.uk proxyCommand ssh -W myriad.rc.ucl.ac.uk:22 ccxxxxx@socrates.ucl.ac.uk Host login05 User ccxxxxx HostName login05.external.legion.ucl.ac.uk proxyCommand ssh -W login05.external.legion.ucl.ac.uk:22 ccxxxxx@socrates.ucl.ac.uk Host aristotle User ccxxxxx HostName aristotle.rc.ucl.ac.uk proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 ccxxxxx@socrates.ucl.ac.uk You can now just type ssh myriad or scp file1 aristotle:~ and you will go through Socrates. You'll be asked for login details twice since you're logging in to two machines, Socrates and your endpoint.","title":"Using a config file"},{"location":"howto/#windows-winscp","text":"WinSCP can also set up SSH tunnels. Create a new session as before, and tick the Advanced options box in the bottom left corner. Select Connection > Tunnel from the left pane. Tick the Connect through SSH tunnel box and enter the hostname of the gateway you are tunnelling through, for example socrates.ucl.ac.uk Fill in your username and password for that host. (Central UCL ones for Socrates). Select Session from the left pane and fill in the hostname you want to end up on after the tunnel. Fill in your username and password for that host and set the file protocol to SCP. Save your settings with a useful name.","title":"Windows - WinSCP"},{"location":"howto/#windows-putty","text":"You can use PuTTY for tunnelling when you just want a single-step login and not a file transfer.","title":"Windows - PuTTY"},{"location":"howto/#managing-your-quota","text":"After using lquota to see your total usage, you may wish to find what is using all your space. du is a command that gives you information about your disk usage. Useful options are: du -ch <dir> du -h --max-depth=1 The first will give you a summary of the sizes of directory tree and subtrees inside the directory you specify, using human-readable sizes with a total at the bottom. The second will show you the totals for all top-level directories relative to where you are, plus the grand total. These can help you track down the locations of large amounts of data if you need to reduce your disk usage.","title":"Managing your quota"},{"location":"howto/#how-do-i-submit-a-job-to-the-scheduler","text":"To submit a job to the scheduler you need to write a jobscript that contains the resources the job is asking for and the actual commands you want to run. This jobscript is then submitted using the qsub command. qsub myjobscript It will be put in to the queue and will begin running on the compute nodes at some point later when it has been allocated resources.","title":"How do I submit a job to the scheduler?"},{"location":"howto/#passing-in-qsub-options-on-the-command-line","text":"The #$ lines in your jobscript are options to qsub. It will take each line which has #$ as the first two characters and use the contents beyond that as an option. You can also pass options directly to the qsub command and this will override the settings in your script. This can be useful if you are scripting your job submissions in more complicated ways. For example, if you want to change the name of the job for this one instance of the job you can submit your script with: qsub -N NewName myscript.sh Or if you want to increase the wall-clock time to 24 hours: qsub -l h_rt=24:0:0 myscript.sh You can submit jobs with dependencies by using the -hold_jid option. For example, the command below submits a job that won't run until job 12345 has finished: qsub -hold_jid 12345 myscript.sh You may specify node type with the -ac allow= flags as below: qsub -ac allow=XYZ myscript.sh That would restrict the job to running on nodes of type X, Y or Z (the older Legion nodes). Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line whenever possible. We (and you) can see the exact jobscript that was submitted for every job that ran but not what command line options you submitted it with.","title":"Passing in qsub options on the command line"},{"location":"howto/#checking-your-previous-jobscripts","text":"If you want to check what you submitted for a specific job ID, you can do it with the scriptfor utility. scriptfor 12345 As mentioned above, this will not show any command line options you passed in.","title":"Checking your previous jobscripts"},{"location":"howto/#how-do-i-monitor-a-job","text":"","title":"How do I monitor a job?"},{"location":"howto/#qstat","text":"The qstat command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no-one else\u2019s). This makes it easier to keep track of your jobs. The output will look something like this: job-ID prior name user state submit/start at queue slots ja-task-ID ----------------------------------------------------------------------------------------------------------------- 123454 2.00685 DI_m3 ccxxxxx Eqw 10/13/2017 15:29:11 12 123456 2.00685 DI_m3 ccxxxxx r 10/13/2017 15:29:11 Yorick@node-x02e-006 24 123457 2.00398 DI_m2 ucappka qw 10/12/2017 14:42:12 1 This shows you the job ID, the numeric priority the scheduler has assigned to the job, the name you have given the job, your username, the state the job is in, the date and time it was submitted at (or started at, if it has begun), the head node of the job, the number of 'slots' it is taking up, and if it is an array job the last column shows the task ID. The queue name ( Yorick here) is generally not useful. The head node name ( node-x02e-006 ) is useful - the node-x part tells you this is an X-type node. If you want to get more information on a particular job, note its job ID and then use the -f and -j flags to get full output about that job. Most of this information is not very useful. qstat -f -j 12345","title":"qstat"},{"location":"howto/#job-states","text":"qw : queueing, waiting r : running Rq : a pre-job check on a node failed and this job was put back in the queue Rr : this job was rescheduled but is now running on a new node Eqw : there was an error in this jobscript. This will not run. t : this job is being transferred dr : this job is being deleted Many jobs cycling between Rq and Rr generally means there is a dodgy compute node which is failing pre-job checks, but is free so everything tries to run there. In this case, let us know and we will investigate. If a job stays in t or dr state for a long time, the node it was on is likely to be unresponsive - again let us know and we'll investigate. A job in Eqw will remain in that state until you delete it - you should first have a look at what the error was with qexplain .","title":"Job states"},{"location":"howto/#qexplain","text":"This is a utility to show you the non-truncated error reported by your job. qstat -j will show you a truncated version near the bottom of the output. qexplain 123454","title":"qexplain"},{"location":"howto/#qdel","text":"You use qdel to delete a job from the queue. qdel 123454 You can delete all your jobs at once: qdel '*'","title":"qdel"},{"location":"howto/#more-scheduler-commands","text":"Have a look at man qstat and note the commands shown in the SEE ALSO section of the manual page. Exit the manual page and then look at the man pages for those. (You will not be able to run all commands).","title":"More scheduler commands"},{"location":"howto/#nodesforjob","text":"This is a utility that shows you the current percentage load, memory used and swap used on the nodes your job is running on. If your job is sharing the node with other people's jobs, it will show you the total resources in use, not just those used by your job. This is a snapshot of the current time and resource usage may change over the course of your job. Bear in mind that memory use in particular can increase over time as your job runs. If a cluster has hyperthreading enabled and you aren't using it, full load will show as 50% and not 100% - this is normal and not a problem. For a parallel job, very low (or zero) usage of any of the nodes suggests your job is either not capable of running over multiple nodes, or not partitioning its work effectively - you may be asking for more cores than it can use, or asking for a number of cores that doesn't fit well into the node sizes, leaving many idle. [uccacxx@login02 ~]$ nodesforjob 1234 Nodes for job 1234: Primary: node-r99a-238: 103.1 % load, 12.9 % memory used, 0.1% swap used Secondaries: node-r99a-206: 1.7 % load, 1.6 % memory used, 0.1% swap used node-r99a-238: 103.1 % load, 12.9 % memory used, 0.1% swap used node-r99a-292: 103.1 % load, 12.9 % memory used, 0.1% swap used node-r99a-651: 1.6 % load, 3.2 % memory used, 0.1% swap used The above example shows a multi-node job, so all the usage belongs to this job itself. It is running on four nodes, and node-r99a-238 is the head node (the one that launched the job) and shows up in both Primary and Secondaries. The load is very unbalanced - it is using two nodes flat out, and two are mostly doing nothing. Memory use is low. Swap use is essentially zero.","title":"nodesforjob"},{"location":"howto/#jobhist","text":"Once a job ends, it no longer shows up in qstat . To see information about your finished jobs - when they started, when they ended, what node they ran on - type jobhist . [uccacxx@login02 ~]$ jobhist FSTIME | FETIME | HOSTNAME | OWNER | JOB NUMBER | TASK NUMBER | EXIT STATUS | JOB NAME ----------------------+---------------------+---------------+---------+------------+-------------+-------------+--------------- 2020-06-17 16:31:12 | 2020-06-17 16:34:19 | node-h00a-010 | uccacxx | 3854822 | 0 | 0 | m_job 2020-06-17 16:56:50 | 2020-06-17 16:56:52 | node-d00a-023 | uccacxx | 3854836 | 0 | 1 | k_job 2020-06-17 17:21:12 | 2020-06-17 17:21:46 | node-d00a-012 | uccacxx | 3854859 | 0 | 0 | k_job FSTIME - when the job started running on the node FETIME - when the job ended HOSTNAME - the head node of the job (if it ran on multiple nodes, it only lists the first) TASK NUMBER - if it was an array job, it will have a different number here for each task This shows jobs that finished in the last 24 hours by default. You can search for longer as well: jobhist --hours=200 If a job ended and didn't create the files you expect, check the start and end times to see whether it ran out of wallclock time. If a job only ran for seconds and didn't produce the expected output, there was probably something wrong in your script - check the .o and .e files in the directory you submitted the job from for errors.","title":"jobhist"},{"location":"howto/#how-do-i-run-interactive-jobs","text":"Sometimes you need to run interactive programs, sometimes with a GUI. This can be achieved through qrsh . We have a detailed guide to running interactive jobs .","title":"How do I run interactive jobs?"},{"location":"howto/#how-do-i-estimate-what-resources-to-request-in-my-jobscript","text":"It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. Run your program as: /usr/bin/time --verbose myprogram myargs where myprogram myargs is however you normally run your program, with whatever options you pass to it. When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is maxrss (maximum resident set size) which roughly tells you the largest amount of memory it used. Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible - if you increase it too much you may end up with a job that cannot be submitted. You can also look at nodesforjob while a job is running to see a snapshot of the memory, swap and load on the nodes your job is running on.","title":"How do I estimate what resources to request in my jobscript?"},{"location":"howto/#how-can-i-see-what-types-of-node-a-cluster-has","text":"As well as looking at the cluster-specific page in this documentation for more details (for example Myriad ), you can run nodetypes , which will give you basic information about the nodes that exist in that cluster. [uccacxx@login12 ~]$ nodetypes Unknown node type: util02 Unknown node type: util05 2 type * nodes: 36 cores, 188.4G RAM 7 type B nodes: 36 cores, 1.5T RAM 66 type D nodes: 36 cores, 188.4G RAM 9 type E nodes: 36 cores, 188.4G RAM 1 type F nodes: 36 cores, 188.4G RAM 3 type H nodes: 36 cores, 172.7G RAM 53 type H nodes: 36 cores, 188.4G RAM 3 type I nodes: 36 cores, 1.5T RAM 2 type J nodes: 36 cores, 188.4G RAM This shows how many of each letter-labelled nodetype the cluster has, then the number of cores and amount of memory the node is reporting it has. It also shows the cluster has some utility nodes - those are part of the infrastructure. The * nodes are the login nodes.","title":"How can I see what types of node a cluster has?"},{"location":"howto/#how-do-i-run-a-graphical-program","text":"To run a graphical program on the cluster and be able to view the user interface on your own local computer, you will need to have an X-Windows Server installed on your local computer and use X-forwarding.","title":"How do I run a graphical program?"},{"location":"howto/#x-forwarding-on-linux","text":"Desktop Linux operating systems already have X-Windows installed, so you just need to ssh in with the correct flags. You need to make sure you use either the -X or -Y (look at man ssh for details) flags on all ssh commands you run to establish a connection to the cluster. For example, connecting from outside of UCL: ssh -X <your_UCL_user_id>@socrates.ucl.ac.uk and then ssh -X <your_UCL_user_id>@myriad.rc.ucl.ac.uk","title":"X-forwarding on Linux"},{"location":"howto/#x-forwarding-on-mac-os-x","text":"You will need to install XQuartz to provide an X-Window System for Mac OS X. (Previously known as X11.app). You can then follow the Linux instructions using Terminal.app.","title":"X-forwarding on Mac OS X"},{"location":"howto/#x-forwarding-on-windows","text":"You will need: An SSH client; e.g., PuTTY An X server program; e.g., Exceed, Xming Exceed is available on Desktop@UCL machines and downloadable from the UCL software database . Xming is open source (and mentioned here without testing).","title":"X-forwarding on Windows"},{"location":"howto/#exceed-on-desktopucl","text":"Load Exceed. You can find it under Start > All Programs > Applications O-P > Open Text Exceed 14 > Exceed Open PuTTY (Applications O-P > PuTTY) In PuTTY, set up the connection with the host machine as usual: Host name: myriad.rc.ucl.ac.uk (for example) Port: 22 Connection type: SSH Then, from the Category menu, select Connection > SSH > X11 for 'Options controlling SSH X11 forwarding'. Make sure the box marked 'Enable X11 forwarding' is checked. Return to the session menu and save these settings with a new identifiable name for reuse in future. Click 'Open' and login to the host as usual To test that X-forwarding is working, try running nedit which is a text editor in our default modules. If nedit works, you have successfully enabled X-forwarding for graphical applications.","title":"Exceed on Desktop@UCL"},{"location":"howto/#installing-xming","text":"Xming is a popular open source X server for Windows. These are instructions for using it alongside PuTTY. Other SSH clients and X servers are available. We cannot verify how well it may be working. Install both PuTTY and Xming if you have not done so already. During Xming installation, choose not to install an SSH client. Open Xming - the Xming icon should appear on the task bar. Open PuTTY Set up PuTTY as shown in the Exceed section.","title":"Installing Xming"},{"location":"Background/Cluster_Computing/","text":"Cluster Computing \u00a7 What is a cluster? \u00a7 In this context, a cluster is a collection of computers (often referred to as \"nodes\"). They're networked together with some shared storage and a scheduling system that lets people run programs on them without having to enter commands \"live\". Why would I want to use one? \u00a7 Some researchers have programs that require a lot of compute power, like simulating weather patterns or the quantum behaviour of molecules. Others have a lot of data to process, or need to simulate a lot of things at once, like simulating the spread of disease or assembling parts of DNA into a genome. Often these kinds of work are either impossible or would take far too long to do on a desktop or laptop computer, as well as making the computer unavailable to do everyday tasks like writing documents or reading papers. By running the programs on the computers in a cluster, researchers can use many powerful computers at once, without locking up their own one. How do I use it? \u00a7 Most people use something like the following workflow: connect to the cluster's \"login nodes\" create a script of commands to run programs submit the script to the scheduler wait for the scheduler to find available \"compute nodes\" and run the script look at the results in the files the script created Most people connect using a program called a Secure Shell Client (\"ssh client\" for short), but some programs, like Matlab and Comsol, run on your own computer and can be set up to send work to the cluster automatically. That can be a little tricky to get working, though. The ssh client gives you a command prompt when you can enter text commands, but you can also tell it to pass graphical windows through the network connection, using a system called X-Forwarding. This can be useful for visualising your data without transferring all the files back, but the network connection makes it a bit slower to use than running it on your own computer. You'll need an X server on your own computer to use this: check our page on X-Forwarding for details.","title":"Cluster Computing"},{"location":"Background/Cluster_Computing/#cluster-computing","text":"","title":"Cluster Computing"},{"location":"Background/Cluster_Computing/#what-is-a-cluster","text":"In this context, a cluster is a collection of computers (often referred to as \"nodes\"). They're networked together with some shared storage and a scheduling system that lets people run programs on them without having to enter commands \"live\".","title":"What is a cluster?"},{"location":"Background/Cluster_Computing/#why-would-i-want-to-use-one","text":"Some researchers have programs that require a lot of compute power, like simulating weather patterns or the quantum behaviour of molecules. Others have a lot of data to process, or need to simulate a lot of things at once, like simulating the spread of disease or assembling parts of DNA into a genome. Often these kinds of work are either impossible or would take far too long to do on a desktop or laptop computer, as well as making the computer unavailable to do everyday tasks like writing documents or reading papers. By running the programs on the computers in a cluster, researchers can use many powerful computers at once, without locking up their own one.","title":"Why would I want to use one?"},{"location":"Background/Cluster_Computing/#how-do-i-use-it","text":"Most people use something like the following workflow: connect to the cluster's \"login nodes\" create a script of commands to run programs submit the script to the scheduler wait for the scheduler to find available \"compute nodes\" and run the script look at the results in the files the script created Most people connect using a program called a Secure Shell Client (\"ssh client\" for short), but some programs, like Matlab and Comsol, run on your own computer and can be set up to send work to the cluster automatically. That can be a little tricky to get working, though. The ssh client gives you a command prompt when you can enter text commands, but you can also tell it to pass graphical windows through the network connection, using a system called X-Forwarding. This can be useful for visualising your data without transferring all the files back, but the network connection makes it a bit slower to use than running it on your own computer. You'll need an X server on your own computer to use this: check our page on X-Forwarding for details.","title":"How do I use it?"},{"location":"Clusters/Grace/","text":"Grace \u00a7 Grace is a compute cluster designed for extensively parallel, multi-node batch-processing jobs, having high-bandwidth connections between each individual node. Accounts \u00a7 Grace accounts can be applied for via the Research Computing sign up process . As Grace is intended for multi-node jobs, users who specify that they will need to use multi-node jobs (e.g. with MPI ) will be given access to Grace. Logging in \u00a7 Please use your UCL username and password to connect to Grace with an SSH client. ssh uccaxxx@grace.rc.ucl.ac.uk If using PuTTY, put grace.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall . Logging in to a specific node \u00a7 You can access a specific Grace login node by using their dedicated addresses instead of the main grace.rc.ucl.ac.uk address, for example: ssh uccaxxx@login01.ext.grace.ucl.ac.uk ssh uccaxxx@login02.ext.grace.ucl.ac.uk The main address will unpredictably direct you to either one of these (to balance load), so if you need multiple sessions on one, this lets you do that. Copying data onto Grace \u00a7 You will need to use an SCP or SFTP client to copy data onto Grace. Please refer to the page on How do I transfer data onto the system? Quotas \u00a7 Danger This section has not been filled in. Job sizes and durations \u00a7 For interactive jobs: Cores Max. Duration 32 2h For batch jobs: Cores Max. Duration 1-16 12h 16-256 48h 256-512 24h >512 12h If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Additional Resource Requests for more details. Node types \u00a7 Grace's compute capability comprises roughly 650 compute nodes each with two 8-core Intel Xeon E5-2630v3 2.4GHz processors, 64 gigabytes of 2133MHz DDR4 RAM, 120GB hard drives, and an Intel TrueScale network. Two nodes identical to these serve as the login nodes.","title":"Grace"},{"location":"Clusters/Grace/#grace","text":"Grace is a compute cluster designed for extensively parallel, multi-node batch-processing jobs, having high-bandwidth connections between each individual node.","title":"Grace"},{"location":"Clusters/Grace/#accounts","text":"Grace accounts can be applied for via the Research Computing sign up process . As Grace is intended for multi-node jobs, users who specify that they will need to use multi-node jobs (e.g. with MPI ) will be given access to Grace.","title":"Accounts"},{"location":"Clusters/Grace/#logging-in","text":"Please use your UCL username and password to connect to Grace with an SSH client. ssh uccaxxx@grace.rc.ucl.ac.uk If using PuTTY, put grace.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall .","title":"Logging in"},{"location":"Clusters/Grace/#logging-in-to-a-specific-node","text":"You can access a specific Grace login node by using their dedicated addresses instead of the main grace.rc.ucl.ac.uk address, for example: ssh uccaxxx@login01.ext.grace.ucl.ac.uk ssh uccaxxx@login02.ext.grace.ucl.ac.uk The main address will unpredictably direct you to either one of these (to balance load), so if you need multiple sessions on one, this lets you do that.","title":"Logging in to a specific node"},{"location":"Clusters/Grace/#copying-data-onto-grace","text":"You will need to use an SCP or SFTP client to copy data onto Grace. Please refer to the page on How do I transfer data onto the system?","title":"Copying data onto Grace"},{"location":"Clusters/Grace/#quotas","text":"Danger This section has not been filled in.","title":"Quotas"},{"location":"Clusters/Grace/#job-sizes-and-durations","text":"For interactive jobs: Cores Max. Duration 32 2h For batch jobs: Cores Max. Duration 1-16 12h 16-256 48h 256-512 24h >512 12h If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Additional Resource Requests for more details.","title":"Job sizes and durations"},{"location":"Clusters/Grace/#node-types","text":"Grace's compute capability comprises roughly 650 compute nodes each with two 8-core Intel Xeon E5-2630v3 2.4GHz processors, 64 gigabytes of 2133MHz DDR4 RAM, 120GB hard drives, and an Intel TrueScale network. Two nodes identical to these serve as the login nodes.","title":"Node types"},{"location":"Clusters/Kathleen/","text":"Kathleen \u00a7 Kathleen is a compute cluster designed for extensively parallel, multi-node batch-processing jobs, having high-bandwidth connections between each individual node. It is named after Professor Dame Kathleen Lonsdale , a pioneering chemist and activist, and was installed in December 2019. It went into service at the end of January 2020. Accounts \u00a7 Kathleen accounts can be applied for via the Research Computing sign up process . As Kathleen is intended for multi-node jobs, users who specify that they will need to use multi-node jobs (e.g. with MPI ) will be given access to Kathleen. Logging in \u00a7 Please use your UCL username and password to connect to Kathleen with an SSH client. ssh uccaxxx@kathleen.rc.ucl.ac.uk If using PuTTY, put kathleen.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall . Logging in to a specific node \u00a7 You can access a specific Kathleen login node by using their dedicated addresses instead of the main kathleen.rc.ucl.ac.uk address, for example: ssh uccaxxx@login01.kathleen.rc.ucl.ac.uk The main address will unpredictably direct you to either one of these (to balance load), so if you need multiple sessions on one, this lets you do that. Copying data onto Kathleen \u00a7 You will need to use an SCP or SFTP client to copy data onto Kathleen. Please refer to the page on How do I transfer data onto the system? Note that firewalls mean you cannot connect directly from Grace to Kathleen so you need to use tunnelling with the scp command . Quotas \u00a7 On Kathleen you have a single 250GB quota by default which covers your home and Scratch. This is a hard quota: once you reach it, you will no longer be able to write more data. Keep an eye on it, as this will cause jobs to fail if they cannot create their .o or .e files at the start, or their output files partway through. You can check your quota on Kathleen by running: lquota which will give you output similar to this: Storage Used Quota % Used Path lustre 146.19 MiB 250.00 GiB 0% /home/uccaxxx You can apply for quota increases using the form at Additional Resource Requests . Here are some tips for managing your quota and finding where space is being used. Job sizes and durations \u00a7 Please consider that Kathleen nodes have 40 physical cores - 2 nodes is 80 cores. Jobs do not share nodes, so although asking for 41 cores is possible, it means you are wasting the other 39 cores on your second node! For interactive jobs: Cores Max. Duration 41-80 (2 nodes) 2h For batch jobs: Cores Max. Duration 41-256 48h 257-512 24h 513-5760 12h These are numbers of physical cores. If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Additional Resource Requests for more details. Node types \u00a7 Kathleen's compute capability comprises 192 diskless compute nodes each with two 20-core Intel Xeon Gold 6248 2.5GHz processors, 192 gigabytes of 2933MHz DDR4 RAM, and an Intel OmniPath network. Two nodes identical to these, but with two 1 terabyte hard-disk drives added, serve as the login nodes. Hyperthreading \u00a7 Kathleen has hyperthreading enabled and you can choose on a per-job basis whether you want to use it. Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. If you do not ask for hyperthreading, your job only uses one thread per core as normal. # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process export OMP_NUM_THREADS=2 This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads). # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process # (a whole node's worth) export OMP_NUM_THREADS=80 This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads). Diskless nodes \u00a7 Kathleen nodes are diskless (have no local hard drives) - there is no $TMPDIR available on Kathleen, so you should not request -l tmpfs=10G in your jobscripts or your job will be rejected at submit time. If you need temporary space, you should use somewhere in your Scratch. Loading and unloading modules \u00a7 Kathleen has a newer version of modulecmd which tries to manage module dependencies automatically by loading or unloading prerequisites for you whenever possible. If you get an error like this: [uccaxxx@login01.kathleen ~]$ module unload compilers mpi Unloading compilers/intel/2018/update3 ERROR: compilers/intel/2018/update3 cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. Unloading mpi/intel/2018/update3/intel ERROR: mpi/intel/2018/update3/intel cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. You can use the -f option to force the module change. It will carry it out and warn you about modules it thinks are dependent. [uccaxxx@login01.kathleen ~]$ module unload -f compilers mpi Unloading compilers/intel/2018/update3 WARNING: Dependent default-modules/2018 is loaded Unloading mpi/intel/2018/update3/intel WARNING: Dependent default-modules/2018 is loaded Otherwise you will need to unload default-modules/2018 to swap compiler and MPI module, but that will leave you without gerun in your path. You can then do either of these things: # load everything that was in default-modules except the compiler and mpi module unload default-modules/2018 module load rcps-core/1.0.0 module load whatever or # just reload gerun module unload default-modules/2018 module load gerun module load whatever","title":"Kathleen"},{"location":"Clusters/Kathleen/#kathleen","text":"Kathleen is a compute cluster designed for extensively parallel, multi-node batch-processing jobs, having high-bandwidth connections between each individual node. It is named after Professor Dame Kathleen Lonsdale , a pioneering chemist and activist, and was installed in December 2019. It went into service at the end of January 2020.","title":"Kathleen"},{"location":"Clusters/Kathleen/#accounts","text":"Kathleen accounts can be applied for via the Research Computing sign up process . As Kathleen is intended for multi-node jobs, users who specify that they will need to use multi-node jobs (e.g. with MPI ) will be given access to Kathleen.","title":"Accounts"},{"location":"Clusters/Kathleen/#logging-in","text":"Please use your UCL username and password to connect to Kathleen with an SSH client. ssh uccaxxx@kathleen.rc.ucl.ac.uk If using PuTTY, put kathleen.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall .","title":"Logging in"},{"location":"Clusters/Kathleen/#logging-in-to-a-specific-node","text":"You can access a specific Kathleen login node by using their dedicated addresses instead of the main kathleen.rc.ucl.ac.uk address, for example: ssh uccaxxx@login01.kathleen.rc.ucl.ac.uk The main address will unpredictably direct you to either one of these (to balance load), so if you need multiple sessions on one, this lets you do that.","title":"Logging in to a specific node"},{"location":"Clusters/Kathleen/#copying-data-onto-kathleen","text":"You will need to use an SCP or SFTP client to copy data onto Kathleen. Please refer to the page on How do I transfer data onto the system? Note that firewalls mean you cannot connect directly from Grace to Kathleen so you need to use tunnelling with the scp command .","title":"Copying data onto Kathleen"},{"location":"Clusters/Kathleen/#quotas","text":"On Kathleen you have a single 250GB quota by default which covers your home and Scratch. This is a hard quota: once you reach it, you will no longer be able to write more data. Keep an eye on it, as this will cause jobs to fail if they cannot create their .o or .e files at the start, or their output files partway through. You can check your quota on Kathleen by running: lquota which will give you output similar to this: Storage Used Quota % Used Path lustre 146.19 MiB 250.00 GiB 0% /home/uccaxxx You can apply for quota increases using the form at Additional Resource Requests . Here are some tips for managing your quota and finding where space is being used.","title":"Quotas"},{"location":"Clusters/Kathleen/#job-sizes-and-durations","text":"Please consider that Kathleen nodes have 40 physical cores - 2 nodes is 80 cores. Jobs do not share nodes, so although asking for 41 cores is possible, it means you are wasting the other 39 cores on your second node! For interactive jobs: Cores Max. Duration 41-80 (2 nodes) 2h For batch jobs: Cores Max. Duration 41-256 48h 257-512 24h 513-5760 12h These are numbers of physical cores. If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Additional Resource Requests for more details.","title":"Job sizes and durations"},{"location":"Clusters/Kathleen/#node-types","text":"Kathleen's compute capability comprises 192 diskless compute nodes each with two 20-core Intel Xeon Gold 6248 2.5GHz processors, 192 gigabytes of 2933MHz DDR4 RAM, and an Intel OmniPath network. Two nodes identical to these, but with two 1 terabyte hard-disk drives added, serve as the login nodes.","title":"Node types"},{"location":"Clusters/Kathleen/#hyperthreading","text":"Kathleen has hyperthreading enabled and you can choose on a per-job basis whether you want to use it. Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. If you do not ask for hyperthreading, your job only uses one thread per core as normal. # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process export OMP_NUM_THREADS=2 This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads). # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process # (a whole node's worth) export OMP_NUM_THREADS=80 This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads).","title":"Hyperthreading"},{"location":"Clusters/Kathleen/#diskless-nodes","text":"Kathleen nodes are diskless (have no local hard drives) - there is no $TMPDIR available on Kathleen, so you should not request -l tmpfs=10G in your jobscripts or your job will be rejected at submit time. If you need temporary space, you should use somewhere in your Scratch.","title":"Diskless nodes"},{"location":"Clusters/Kathleen/#loading-and-unloading-modules","text":"Kathleen has a newer version of modulecmd which tries to manage module dependencies automatically by loading or unloading prerequisites for you whenever possible. If you get an error like this: [uccaxxx@login01.kathleen ~]$ module unload compilers mpi Unloading compilers/intel/2018/update3 ERROR: compilers/intel/2018/update3 cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. Unloading mpi/intel/2018/update3/intel ERROR: mpi/intel/2018/update3/intel cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. You can use the -f option to force the module change. It will carry it out and warn you about modules it thinks are dependent. [uccaxxx@login01.kathleen ~]$ module unload -f compilers mpi Unloading compilers/intel/2018/update3 WARNING: Dependent default-modules/2018 is loaded Unloading mpi/intel/2018/update3/intel WARNING: Dependent default-modules/2018 is loaded Otherwise you will need to unload default-modules/2018 to swap compiler and MPI module, but that will leave you without gerun in your path. You can then do either of these things: # load everything that was in default-modules except the compiler and mpi module unload default-modules/2018 module load rcps-core/1.0.0 module load whatever or # just reload gerun module unload default-modules/2018 module load gerun module load whatever","title":"Loading and unloading modules"},{"location":"Clusters/Legion/","text":"Legion \u00a7 Legion is our older high-throughput cluster, that supports a small collection of research-group-owned groups of nodes, and nodes that were formerly research-group-owned but are now generally available for research workloads. Accounts \u00a7 Logging in \u00a7 Please use your UCL username and password to connect to Legion with an SSH client. ssh uccaxxx@legion.rc.ucl.ac.uk If using PuTTY, put legion.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall . Logging in to a specific node \u00a7 Note We are currently awaiting assignment of hostnames for logging in to specific nodes, from the ISD Networks group. Copying data onto Legion \u00a7 You will need to use an SCP or SFTP client to copy data onto Legion. Please refer to the page on How do I transfer data onto the system? Due to a lack of available networking hardware to connect Legion to the rest of the UCL network, the connections to the login nodes can only get a theoretical maximum of 1 gigabit/second (128 megabytes/second). There is a single node which has a faster connection (10x), available by connecting to transfer.legion.rc.ucl.ac.uk . When transferring data from a source with a fast connection, like another cluster or a website, this address should be used in place of either the main Legion address or a specific login node address, to get the best transfer speed. Quotas \u00a7 Danger This section has not been filled in. Job sizes \u00a7 For interactive jobs: Cores Max. Duration 1-16 2h For batch jobs: Cores Max. Duration 1 72h 2-256 48h 16-256 48h 256-512[^1] 24h >512[^1] 12h [^1] While the scheduler will allow you to submit a job of this size, it is unlikely to ever be run. Note that nodes owned by research groups have a job duration limit of 7 days for users in those research groups . If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Applying for Additional Resources for more details. Node types \u00a7 As Legion has been added to in small quantities at many times, it has many types of node. They all have two Sandy Bridge or Ivy Bridge processors. Most have 16 cores and 64GB of RAM. The exceptions are the single GPU node, which has only 12 cores and 8GB of RAM, the single Q-class node, which has 32 cores and 500GB of RAM, and the 6 T-class nodes, which have 32 cores and 1.5TB of RAM.","title":"Legion"},{"location":"Clusters/Legion/#legion","text":"Legion is our older high-throughput cluster, that supports a small collection of research-group-owned groups of nodes, and nodes that were formerly research-group-owned but are now generally available for research workloads.","title":"Legion"},{"location":"Clusters/Legion/#accounts","text":"","title":"Accounts"},{"location":"Clusters/Legion/#logging-in","text":"Please use your UCL username and password to connect to Legion with an SSH client. ssh uccaxxx@legion.rc.ucl.ac.uk If using PuTTY, put legion.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall .","title":"Logging in"},{"location":"Clusters/Legion/#logging-in-to-a-specific-node","text":"Note We are currently awaiting assignment of hostnames for logging in to specific nodes, from the ISD Networks group.","title":"Logging in to a specific node"},{"location":"Clusters/Legion/#copying-data-onto-legion","text":"You will need to use an SCP or SFTP client to copy data onto Legion. Please refer to the page on How do I transfer data onto the system? Due to a lack of available networking hardware to connect Legion to the rest of the UCL network, the connections to the login nodes can only get a theoretical maximum of 1 gigabit/second (128 megabytes/second). There is a single node which has a faster connection (10x), available by connecting to transfer.legion.rc.ucl.ac.uk . When transferring data from a source with a fast connection, like another cluster or a website, this address should be used in place of either the main Legion address or a specific login node address, to get the best transfer speed.","title":"Copying data onto Legion"},{"location":"Clusters/Legion/#quotas","text":"Danger This section has not been filled in.","title":"Quotas"},{"location":"Clusters/Legion/#job-sizes","text":"For interactive jobs: Cores Max. Duration 1-16 2h For batch jobs: Cores Max. Duration 1 72h 2-256 48h 16-256 48h 256-512[^1] 24h >512[^1] 12h [^1] While the scheduler will allow you to submit a job of this size, it is unlikely to ever be run. Note that nodes owned by research groups have a job duration limit of 7 days for users in those research groups . If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Applying for Additional Resources for more details.","title":"Job sizes"},{"location":"Clusters/Legion/#node-types","text":"As Legion has been added to in small quantities at many times, it has many types of node. They all have two Sandy Bridge or Ivy Bridge processors. Most have 16 cores and 64GB of RAM. The exceptions are the single GPU node, which has only 12 cores and 8GB of RAM, the single Q-class node, which has 32 cores and 500GB of RAM, and the 6 T-class nodes, which have 32 cores and 1.5TB of RAM.","title":"Node types"},{"location":"Clusters/Michael/","text":"Michael \u00a7 Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, an EPSRC-funded Tier 2 system providing large scale computation to UK researchers; and delivers computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology. Applying for an account \u00a7 Michael accounts belong to you as an individual and are applied for via David Scanlon who is the point of contact for Michael. You will need to supply an SSH public key, which is the only method used to log in. Creating an ssh key pair \u00a7 An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine). Creating an ssh key in Linux/Unix/Mac OS X \u00a7 ssh-keygen -t rsa The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key. Do not use DSA as OpenSSH 7.0 has deprecated it and does not use it by default on client or server. We no longer accept DSA keys. You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it. You may need to run ssh-add to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running ssh-add -L will show all the public parts of the keys it is aware of. Creating an ssh key in Windows \u00a7 Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no longer accepted. The key must be at least 2048-bit. If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY. Information for Points of Contact \u00a7 Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact . Logging in \u00a7 You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form mmmxxxx (where xxxx is a number) and UCL users will use their central username. You connect with ssh directly to: michael.rc.ucl.ac.uk SSH timeouts \u00a7 Idle ssh sessions will be disconnected after 7 days. Using the system \u00a7 Michael is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\<15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job. Full user guide \u00a7 Michael has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Michael has should be listed on this page. Submitting a job \u00a7 Create a job script for non-interactive use and submit your jobscript using qsub . Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules. A job on Michael must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below). Memory requests \u00a7 Note: the memory you request is always per core, not the total amount. If you ask for 128G RAM and 24 cores, that will run on 24 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process. Monitoring a job \u00a7 In addition to qstat , nodesforjob $JOB_ID can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on. qexplain $JOB_ID will show you the full error for a job that is in Eqw status. Useful utilities \u00a7 As well as nodesforjob , there are the following utilities which can help you find information about your jobs after they have run. jobhist - shows your job history for the last 24hrs by default, including start and end times and the head node it ran on. You can view a longer history by specifying --hours=100 for example. scriptfor $JOB_ID - show the script that was submitted for the given job. These utilities live in GitHub at https://github.com/UCL-RITS/go-clustertools and https://github.com/UCL-RITS/rcps-cluster-scripts Software \u00a7 Michael mounts the RC Systems software stack . Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. Access to software is managed through the use of modules. module avail shows all modules available. module list shows modules currently loaded. Access to licensed software may vary based on your host institution and project. Requesting software installs \u00a7 To request software installs, email us at the support address below or open an issue on our GitHub . You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested. Installing your own software \u00a7 You may install software in your own space. Please look at Compiling for tips. Maintaining a piece of software for a group \u00a7 It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs. Licensed software \u00a7 Reserved application groups exist for software that requires them. The group name will begin with leg or lg . After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the groups command. Please let us know your username when you ask to be added to a group. CASTEP : You/your group leader need to have signed up for a CASTEP license . Send us an acceptance email, or we can ask them to verify you have a license. You will then be added to the reserved application group lgcastep . If you are a member of UKCP you are already covered by a license and just need to tell us when you request access. CRYSTAL : You/your group leader need to have signed up for an Academic license. Crystal Solutions will send an email saying an account has been upgraded to \"Academic UK\" - forward that to us along with confirmation from the group leader that you should be in their group. You will be added to the legcryst group. DL_POLY : has individual licenses for specific versions. Sign up at DL_POLY's website and send us the acceptance email they give you. We will add you to the appropriate version's reserved application group, eg lgdlp408 . Gaussian : not currently accessible for non-UCL institutions. UCL having a site license and another institute having a site license does not allow users from the other institute to run Gaussian on UCL-owned hardware. VASP : When you request access you need to send us the name and email of the main VASP license holder along with the license number. We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group. You may also install your own copy in your home, and we provide a simple build script on Github (tested with VASP 5.4.4, no patches). You need to download the VASP source code and then you can run the script following the instructions at the top. Molpro : Only UCL users are licensed to use our central copy and can request to be added to the lgmolpro reserved application group. Suggested job sizes on original Michael \u00a7 The target job sizes for original Michael K-type nodes are 48-120 cores (2-5 nodes). Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities. Maximum job resources on original Michael \u00a7 Cores Max wallclock 864 48hrs On Michael, interactive sessions using qrsh have the same wallclock limit as other jobs. The K-type nodes in Michael are 24 cores, 128GB RAM. The default maximum jobsize is 864 cores, to remain within the 36-node 1:1 nonblocking interconnect zones. Jobs on Michael do not share nodes . This means that if you request less than 24 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 24 for full usage of your nodes. There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the 3:1 interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details. Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full. 2020 Michael expansion \u00a7 At the end of March 2020, Michael was expanded to include a new set of nodes. The old Michael nodes are the K-type nodes, while the new ones are the A-type nodes. The node name will look like node-a14a-001 or node-k10a-001 . The Michael expansion consists of 208 compute nodes each with two 20-core Intel Xeon Gold 6248 2.5GHz processors, 192 gigabytes of 2933MHz DDR4 RAM, 1TB disk, and an Intel OmniPath network. Expansion nodes have two Hyperthreads available. These are arranged in two 32-node CUs (a and b) and four 36-node CUs (c to f). Jobs are restricted to running either within a CU (all nodes connected to the same switch) or across CUs using only the bottom third of nodes attached to each switch. This approximates 1:1 blocking on a cluster that does not have it. Maximum job resources on Michael expansion \u00a7 Please consider that Michael's A-type nodes have 40 physical cores - 2 nodes is 80 cores. Jobs do not share nodes, so although asking for 41 cores is possible, it means you are wasting the other 39 cores on your second node! Cores Max. Duration 2800 48h These are numbers of physical cores: multiply by two for virtual cores with hyperthreads. Hyperthreading \u00a7 The A-type nodes have hyperthreading enabled and you can choose on a per-job basis whether you want to use it. Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. If you do not ask for hyperthreading, your job only uses one thread per core as normal. # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process export OMP_NUM_THREADS=2 This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads). # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process # (a whole node's worth) export OMP_NUM_THREADS=80 This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads). Choosing node types \u00a7 Given the difference in core count on the original and expansion Michael nodes, we strongly suggest you always specify which type of node you intend your job to run on, to avoid unintentionally wasting cores if your total number does not cleanly fit on that node size. The old nodes are K-type while the new nodes with hyperthreading are A-type. Jobs never run across a mix of node types - it will be all K nodes or all A nodes. To specify node type in your jobscript, add either: # run on original 24-core nodes #$ -ac allow=K or # run on expansion 40-core hyperthread-enabled nodes #$ -ac allow=A Queue names \u00a7 On Michael, users do not submit directly to queues - the scheduler assigns your job to one based on the resources it requested. The queues have somewhat unorthodox names as they are only used internally, and do not directly map to particular job types. Preventing a job from running cross-CU \u00a7 If your job must run within a single CU, you can request the parallel environment as -pe wss instead of -pe mpi ( wss standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue. back to top Disk quotas \u00a7 You have one per-user quota, with a default amount of 250GB - this is the total across home and Scratch. lquota shows you your quota and total usage (twice). request_quota is how you request a quota increase. If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write. Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the Thomas User Group for approval. back to top Budgets and allocations \u00a7 We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free. To see the name of your project(s) and how much allocation that budget has, run the command budgets . $ budgets Project Machines Balance -------- -------- -------- Faraday_Test ANY 22781.89 Submitting a job under a project \u00a7 To submit a paid job that will take Gold from a particular project budget, add this to your jobscript: #$ -P Gold #$ -A MyProject To submit a free job that will not use up any Gold, use this instead: #$ -P Free #$ -A MyProject You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime: #$ -P Test #$ -A MyProject Troubleshooting: Unable to verify membership in policyjsv project \u00a7 Unable to run job: Rejected by policyjsv Unable to verify membership of `<username>` in the policyjsv project You asked for a Free job but didn't specify #$ -A MyProject in your jobscript. Troubleshooting: Unable to verify membership in project / Uninitialized value \u00a7 Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Unable to verify membership of mmmxxxx in the UCL_Example project This error from qsub can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, <GBALANCE> line 1. Failed sending message: (Unable to connect to socket (Connection refused)). If you also get this error from the budgets command, then the Gold daemon is definitely not running and you should contact rc-support. Gold charging \u00a7 When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run. Gold costs of A-type nodes \u00a7 The A-type nodes have twice the peak theoretical performance of the K-type nodes. A 24-core job lasting an hour costs 24 Gold on the K-type nodes. A 40-physical-core job lasting one hour costs 80 Gold on the A-type nodes. An 80-virtual-core job on the A-type nodes also costs 80 Gold. Troubleshooting: Unable to verify sufficient material worth \u00a7 Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Insufficient balance to reserve job This means you don't have enough Gold to cover the cores*wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time. Job deletion \u00a7 If you qdel a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly. Support \u00a7 Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Michael in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.","title":"MMM Michael"},{"location":"Clusters/Michael/#michael","text":"Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, an EPSRC-funded Tier 2 system providing large scale computation to UK researchers; and delivers computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology.","title":"Michael"},{"location":"Clusters/Michael/#applying-for-an-account","text":"Michael accounts belong to you as an individual and are applied for via David Scanlon who is the point of contact for Michael. You will need to supply an SSH public key, which is the only method used to log in.","title":"Applying for an account"},{"location":"Clusters/Michael/#creating-an-ssh-key-pair","text":"An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine).","title":"Creating an ssh key pair"},{"location":"Clusters/Michael/#creating-an-ssh-key-in-linuxunixmac-os-x","text":"ssh-keygen -t rsa The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key. Do not use DSA as OpenSSH 7.0 has deprecated it and does not use it by default on client or server. We no longer accept DSA keys. You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it. You may need to run ssh-add to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running ssh-add -L will show all the public parts of the keys it is aware of.","title":"Creating an ssh key in Linux/Unix/Mac OS X"},{"location":"Clusters/Michael/#creating-an-ssh-key-in-windows","text":"Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no longer accepted. The key must be at least 2048-bit. If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY.","title":"Creating an ssh key in Windows"},{"location":"Clusters/Michael/#information-for-points-of-contact","text":"Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact .","title":"Information for Points of Contact"},{"location":"Clusters/Michael/#logging-in","text":"You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form mmmxxxx (where xxxx is a number) and UCL users will use their central username. You connect with ssh directly to: michael.rc.ucl.ac.uk","title":"Logging in"},{"location":"Clusters/Michael/#ssh-timeouts","text":"Idle ssh sessions will be disconnected after 7 days.","title":"SSH timeouts"},{"location":"Clusters/Michael/#using-the-system","text":"Michael is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\<15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.","title":"Using the system"},{"location":"Clusters/Michael/#full-user-guide","text":"Michael has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Michael has should be listed on this page.","title":"Full user guide"},{"location":"Clusters/Michael/#submitting-a-job","text":"Create a job script for non-interactive use and submit your jobscript using qsub . Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules. A job on Michael must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below).","title":"Submitting a job"},{"location":"Clusters/Michael/#memory-requests","text":"Note: the memory you request is always per core, not the total amount. If you ask for 128G RAM and 24 cores, that will run on 24 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process.","title":"Memory requests"},{"location":"Clusters/Michael/#monitoring-a-job","text":"In addition to qstat , nodesforjob $JOB_ID can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on. qexplain $JOB_ID will show you the full error for a job that is in Eqw status.","title":"Monitoring a job"},{"location":"Clusters/Michael/#useful-utilities","text":"As well as nodesforjob , there are the following utilities which can help you find information about your jobs after they have run. jobhist - shows your job history for the last 24hrs by default, including start and end times and the head node it ran on. You can view a longer history by specifying --hours=100 for example. scriptfor $JOB_ID - show the script that was submitted for the given job. These utilities live in GitHub at https://github.com/UCL-RITS/go-clustertools and https://github.com/UCL-RITS/rcps-cluster-scripts","title":"Useful utilities"},{"location":"Clusters/Michael/#software","text":"Michael mounts the RC Systems software stack . Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. Access to software is managed through the use of modules. module avail shows all modules available. module list shows modules currently loaded. Access to licensed software may vary based on your host institution and project.","title":"Software"},{"location":"Clusters/Michael/#requesting-software-installs","text":"To request software installs, email us at the support address below or open an issue on our GitHub . You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested.","title":"Requesting software installs"},{"location":"Clusters/Michael/#installing-your-own-software","text":"You may install software in your own space. Please look at Compiling for tips.","title":"Installing your own software"},{"location":"Clusters/Michael/#maintaining-a-piece-of-software-for-a-group","text":"It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs.","title":"Maintaining a piece of software for a group"},{"location":"Clusters/Michael/#licensed-software","text":"Reserved application groups exist for software that requires them. The group name will begin with leg or lg . After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the groups command. Please let us know your username when you ask to be added to a group. CASTEP : You/your group leader need to have signed up for a CASTEP license . Send us an acceptance email, or we can ask them to verify you have a license. You will then be added to the reserved application group lgcastep . If you are a member of UKCP you are already covered by a license and just need to tell us when you request access. CRYSTAL : You/your group leader need to have signed up for an Academic license. Crystal Solutions will send an email saying an account has been upgraded to \"Academic UK\" - forward that to us along with confirmation from the group leader that you should be in their group. You will be added to the legcryst group. DL_POLY : has individual licenses for specific versions. Sign up at DL_POLY's website and send us the acceptance email they give you. We will add you to the appropriate version's reserved application group, eg lgdlp408 . Gaussian : not currently accessible for non-UCL institutions. UCL having a site license and another institute having a site license does not allow users from the other institute to run Gaussian on UCL-owned hardware. VASP : When you request access you need to send us the name and email of the main VASP license holder along with the license number. We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group. You may also install your own copy in your home, and we provide a simple build script on Github (tested with VASP 5.4.4, no patches). You need to download the VASP source code and then you can run the script following the instructions at the top. Molpro : Only UCL users are licensed to use our central copy and can request to be added to the lgmolpro reserved application group.","title":"Licensed software"},{"location":"Clusters/Michael/#suggested-job-sizes-on-original-michael","text":"The target job sizes for original Michael K-type nodes are 48-120 cores (2-5 nodes). Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities.","title":"Suggested job sizes on original Michael"},{"location":"Clusters/Michael/#maximum-job-resources-on-original-michael","text":"Cores Max wallclock 864 48hrs On Michael, interactive sessions using qrsh have the same wallclock limit as other jobs. The K-type nodes in Michael are 24 cores, 128GB RAM. The default maximum jobsize is 864 cores, to remain within the 36-node 1:1 nonblocking interconnect zones. Jobs on Michael do not share nodes . This means that if you request less than 24 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 24 for full usage of your nodes. There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the 3:1 interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details. Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full.","title":"Maximum job resources on original Michael"},{"location":"Clusters/Michael/#2020-michael-expansion","text":"At the end of March 2020, Michael was expanded to include a new set of nodes. The old Michael nodes are the K-type nodes, while the new ones are the A-type nodes. The node name will look like node-a14a-001 or node-k10a-001 . The Michael expansion consists of 208 compute nodes each with two 20-core Intel Xeon Gold 6248 2.5GHz processors, 192 gigabytes of 2933MHz DDR4 RAM, 1TB disk, and an Intel OmniPath network. Expansion nodes have two Hyperthreads available. These are arranged in two 32-node CUs (a and b) and four 36-node CUs (c to f). Jobs are restricted to running either within a CU (all nodes connected to the same switch) or across CUs using only the bottom third of nodes attached to each switch. This approximates 1:1 blocking on a cluster that does not have it.","title":"2020 Michael expansion"},{"location":"Clusters/Michael/#maximum-job-resources-on-michael-expansion","text":"Please consider that Michael's A-type nodes have 40 physical cores - 2 nodes is 80 cores. Jobs do not share nodes, so although asking for 41 cores is possible, it means you are wasting the other 39 cores on your second node! Cores Max. Duration 2800 48h These are numbers of physical cores: multiply by two for virtual cores with hyperthreads.","title":"Maximum job resources on Michael expansion"},{"location":"Clusters/Michael/#hyperthreading","text":"The A-type nodes have hyperthreading enabled and you can choose on a per-job basis whether you want to use it. Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. If you do not ask for hyperthreading, your job only uses one thread per core as normal. # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process export OMP_NUM_THREADS=2 This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads). # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process # (a whole node's worth) export OMP_NUM_THREADS=80 This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads).","title":"Hyperthreading"},{"location":"Clusters/Michael/#choosing-node-types","text":"Given the difference in core count on the original and expansion Michael nodes, we strongly suggest you always specify which type of node you intend your job to run on, to avoid unintentionally wasting cores if your total number does not cleanly fit on that node size. The old nodes are K-type while the new nodes with hyperthreading are A-type. Jobs never run across a mix of node types - it will be all K nodes or all A nodes. To specify node type in your jobscript, add either: # run on original 24-core nodes #$ -ac allow=K or # run on expansion 40-core hyperthread-enabled nodes #$ -ac allow=A","title":"Choosing node types"},{"location":"Clusters/Michael/#queue-names","text":"On Michael, users do not submit directly to queues - the scheduler assigns your job to one based on the resources it requested. The queues have somewhat unorthodox names as they are only used internally, and do not directly map to particular job types.","title":"Queue names"},{"location":"Clusters/Michael/#preventing-a-job-from-running-cross-cu","text":"If your job must run within a single CU, you can request the parallel environment as -pe wss instead of -pe mpi ( wss standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue. back to top","title":"Preventing a job from running cross-CU"},{"location":"Clusters/Michael/#disk-quotas","text":"You have one per-user quota, with a default amount of 250GB - this is the total across home and Scratch. lquota shows you your quota and total usage (twice). request_quota is how you request a quota increase. If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write. Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the Thomas User Group for approval. back to top","title":"Disk quotas"},{"location":"Clusters/Michael/#budgets-and-allocations","text":"We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free. To see the name of your project(s) and how much allocation that budget has, run the command budgets . $ budgets Project Machines Balance -------- -------- -------- Faraday_Test ANY 22781.89","title":"Budgets and allocations"},{"location":"Clusters/Michael/#submitting-a-job-under-a-project","text":"To submit a paid job that will take Gold from a particular project budget, add this to your jobscript: #$ -P Gold #$ -A MyProject To submit a free job that will not use up any Gold, use this instead: #$ -P Free #$ -A MyProject You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime: #$ -P Test #$ -A MyProject","title":"Submitting a job under a project"},{"location":"Clusters/Michael/#troubleshooting-unable-to-verify-membership-in-policyjsv-project","text":"Unable to run job: Rejected by policyjsv Unable to verify membership of `<username>` in the policyjsv project You asked for a Free job but didn't specify #$ -A MyProject in your jobscript.","title":"Troubleshooting: Unable to verify membership in policyjsv project"},{"location":"Clusters/Michael/#troubleshooting-unable-to-verify-membership-in-project-uninitialized-value","text":"Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Unable to verify membership of mmmxxxx in the UCL_Example project This error from qsub can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, <GBALANCE> line 1. Failed sending message: (Unable to connect to socket (Connection refused)). If you also get this error from the budgets command, then the Gold daemon is definitely not running and you should contact rc-support.","title":"Troubleshooting: Unable to verify membership in project / Uninitialized value"},{"location":"Clusters/Michael/#gold-charging","text":"When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run.","title":"Gold charging"},{"location":"Clusters/Michael/#gold-costs-of-a-type-nodes","text":"The A-type nodes have twice the peak theoretical performance of the K-type nodes. A 24-core job lasting an hour costs 24 Gold on the K-type nodes. A 40-physical-core job lasting one hour costs 80 Gold on the A-type nodes. An 80-virtual-core job on the A-type nodes also costs 80 Gold.","title":"Gold costs of A-type nodes"},{"location":"Clusters/Michael/#troubleshooting-unable-to-verify-sufficient-material-worth","text":"Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Insufficient balance to reserve job This means you don't have enough Gold to cover the cores*wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time.","title":"Troubleshooting: Unable to verify sufficient material worth"},{"location":"Clusters/Michael/#job-deletion","text":"If you qdel a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly.","title":"Job deletion"},{"location":"Clusters/Michael/#support","text":"Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Michael in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.","title":"Support"},{"location":"Clusters/Myriad/","text":"Myriad \u00a7 Myriad is designed for high I/O, high throughput jobs that will run within a single node rather than multi-node parallel jobs. Accounts \u00a7 Myriad accounts can be applied for via the Research Computing sign up process . As Myriad is our most general-purpose system, everyone who signs up for a Research Computing account is given access to Myriad. Logging in \u00a7 You will use your UCL username and password to ssh in to Myriad. ssh uccaxxx@myriad.rc.ucl.ac.uk If using PuTTY, put myriad.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall . Logging in to a specific node \u00a7 You can access a specific Myriad login node with: ssh uccaxxx@login12.myriad.rc.ucl.ac.uk ssh uccaxxx@login13.myriad.rc.ucl.ac.uk The main address will redirect you on to either one of them. Copying data onto Myriad \u00a7 You will need to use an SCP or SFTP client to copy data onto Myriad. Please refer to the page on How do I transfer data onto the system? Quotas \u00a7 The default quotas on Myriad are 150GB for home and 1TB for Scratch. These are hard quotas: once you reach them, you will no longer be able to write more data. Keep an eye on them, as this will cause jobs to fail if they cannot create their .o or .e files at the start, or their output files partway through. You can check both quotas on Myriad by running: lquota which will give you output similar to this: Storage Used Quota % Used Path home 721.68 MiB 150.00 GiB 0% /home/uccaxxx scratch 52.09 MiB 1.00 TiB 0% /scratch/scratch/uccaxxx You can apply for quota increases using the form at Additional Resource Requests . Here are some tips for managing your quota and finding where space is being used. Job sizes \u00a7 Cores Max wallclock 1 72hrs 2 to 36 48hrs Interactive jobs run with qrsh have a maximum wallclock time of 2 hours. Node types \u00a7 Myriad contains three main node types: standard compute nodes, high memory nodes and GPU nodes. As new nodes as added over time with slightly newer processor variants, new letters are added. Type Cores per node RAM per node Nodes H,D 36 192GB 86 I,B 36 1.5TB 9 J 36 + 2 P100 GPUs 192GB 2 E,F 36 + 2 V100 GPUs 192GB 9 You can tell the type of a node by its name: type H nodes are named node-h00a-001 etc. GPUs \u00a7 Myriad has three types of GPU nodes, J, E and F. There are two J-type nodes each with two nVidia Tesla P100s. There is one F-type and eight E-type nodes, each with two nVidia Tesla V100s. The CPUs are slightly different on these two. You can request one or two GPUs by adding them as a resource request to your jobscript: # For 1 GPU #$ -l gpu=1 # For 2 GPUs #$ -l gpu=2 This will give you either type of GPU. If you need to specify one over the other, add a request for that type of node to your jobscript: # request a V100 node only #$ -ac allow=EF The GPU nodes page has some sample code for running GPU jobs if you need a test example. Tensorflow \u00a7 Tensorflow is installed: type module avail tensorflow to see the available versions. Modules to load for the non-MKL GPU version: module unload compilers mpi module load compilers/gnu/4.9.2 module load python3/recommended module load cuda/8.0.61-patch2/gnu-4.9.2 module load cudnn/6.0/cuda-8.0 module load tensorflow/1.4.1/gpu","title":"Myriad"},{"location":"Clusters/Myriad/#myriad","text":"Myriad is designed for high I/O, high throughput jobs that will run within a single node rather than multi-node parallel jobs.","title":"Myriad"},{"location":"Clusters/Myriad/#accounts","text":"Myriad accounts can be applied for via the Research Computing sign up process . As Myriad is our most general-purpose system, everyone who signs up for a Research Computing account is given access to Myriad.","title":"Accounts"},{"location":"Clusters/Myriad/#logging-in","text":"You will use your UCL username and password to ssh in to Myriad. ssh uccaxxx@myriad.rc.ucl.ac.uk If using PuTTY, put myriad.rc.ucl.ac.uk as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. uccaxxx . When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal. If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall .","title":"Logging in"},{"location":"Clusters/Myriad/#logging-in-to-a-specific-node","text":"You can access a specific Myriad login node with: ssh uccaxxx@login12.myriad.rc.ucl.ac.uk ssh uccaxxx@login13.myriad.rc.ucl.ac.uk The main address will redirect you on to either one of them.","title":"Logging in to a specific node"},{"location":"Clusters/Myriad/#copying-data-onto-myriad","text":"You will need to use an SCP or SFTP client to copy data onto Myriad. Please refer to the page on How do I transfer data onto the system?","title":"Copying data onto Myriad"},{"location":"Clusters/Myriad/#quotas","text":"The default quotas on Myriad are 150GB for home and 1TB for Scratch. These are hard quotas: once you reach them, you will no longer be able to write more data. Keep an eye on them, as this will cause jobs to fail if they cannot create their .o or .e files at the start, or their output files partway through. You can check both quotas on Myriad by running: lquota which will give you output similar to this: Storage Used Quota % Used Path home 721.68 MiB 150.00 GiB 0% /home/uccaxxx scratch 52.09 MiB 1.00 TiB 0% /scratch/scratch/uccaxxx You can apply for quota increases using the form at Additional Resource Requests . Here are some tips for managing your quota and finding where space is being used.","title":"Quotas"},{"location":"Clusters/Myriad/#job-sizes","text":"Cores Max wallclock 1 72hrs 2 to 36 48hrs Interactive jobs run with qrsh have a maximum wallclock time of 2 hours.","title":"Job sizes"},{"location":"Clusters/Myriad/#node-types","text":"Myriad contains three main node types: standard compute nodes, high memory nodes and GPU nodes. As new nodes as added over time with slightly newer processor variants, new letters are added. Type Cores per node RAM per node Nodes H,D 36 192GB 86 I,B 36 1.5TB 9 J 36 + 2 P100 GPUs 192GB 2 E,F 36 + 2 V100 GPUs 192GB 9 You can tell the type of a node by its name: type H nodes are named node-h00a-001 etc.","title":"Node types"},{"location":"Clusters/Myriad/#gpus","text":"Myriad has three types of GPU nodes, J, E and F. There are two J-type nodes each with two nVidia Tesla P100s. There is one F-type and eight E-type nodes, each with two nVidia Tesla V100s. The CPUs are slightly different on these two. You can request one or two GPUs by adding them as a resource request to your jobscript: # For 1 GPU #$ -l gpu=1 # For 2 GPUs #$ -l gpu=2 This will give you either type of GPU. If you need to specify one over the other, add a request for that type of node to your jobscript: # request a V100 node only #$ -ac allow=EF The GPU nodes page has some sample code for running GPU jobs if you need a test example.","title":"GPUs"},{"location":"Clusters/Myriad/#tensorflow","text":"Tensorflow is installed: type module avail tensorflow to see the available versions. Modules to load for the non-MKL GPU version: module unload compilers mpi module load compilers/gnu/4.9.2 module load python3/recommended module load cuda/8.0.61-patch2/gnu-4.9.2 module load cudnn/6.0/cuda-8.0 module load tensorflow/1.4.1/gpu","title":"Tensorflow"},{"location":"Clusters/Thomas/","text":"Thomas is the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling. Applying for an account \u00a7 Thomas accounts belong to you as an individual and are applied for through your own institution's Point of Contact . You will need to supply an SSH public key, which is the only method used to log in. Creating an ssh key pair \u00a7 An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine). Creating an ssh key in Linux/Unix/Mac OS X \u00a7 ssh-keygen -t rsa The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key. Do not use DSA as OpenSSH 7.0 has deprecated it and does not use it by default on client or server. We no longer accept DSA keys. You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it. You may need to run ssh-add to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running ssh-add -L will show all the public parts of the keys it is aware of. Creating an ssh key in Windows \u00a7 Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no longer accepted. The key must be at least 2048-bit. If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY. Information for Points of Contact \u00a7 Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact . Logging in \u00a7 You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form mmmxxxx (where xxxx is a number) and UCL users will use their central username. You ssh directly to: thomas.rc.ucl.ac.uk SSH timeouts \u00a7 Idle ssh sessions will be disconnected after 7 days. Using the system \u00a7 Thomas is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\<15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job. Full user guide \u00a7 Thomas has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Thomas has should be listed on this page. Submitting a job \u00a7 Create a jobscript for non-interactive use and submit your jobscript using qsub . Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules. A job on Thomas must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below.) Memory requests \u00a7 Note: the memory you request is always per core, not the total amount. If you ask for 128GB RAM and 24 cores, that will run on 24 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process. Monitoring a job \u00a7 In addition to qstat , nodesforjob $JOB_ID can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on. qexplain $JOB_ID will show you the full error for a job that is in Eqw status. Useful utilities \u00a7 As well as nodesforjob , there are the following utilities which can help you find information about your jobs after they have run. jobhist - shows your job history for the last 24hrs by default, including start and end times and the head node it ran on. You can view a longer history by specifying --hours=100 for example. scriptfor $JOB_ID - show the script that was submitted for the given job. These utilities live in GitHub at https://github.com/UCL-RITS/go-clustertools and https://github.com/UCL-RITS/rcps-cluster-scripts Software \u00a7 Thomas mounts the RC Systems software stack . Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. Access to software is managed through the use of modules. module avail shows all modules available. module list shows modules currently loaded. Access to licensed software may vary based on your host institution and project. Requesting software installs \u00a7 To request software installs, email us at the support address below or open an issue on our GitHub . You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested. Installing your own software \u00a7 You may install software in your own space. Please look at Compiling for tips. Maintaining a piece of software for a group \u00a7 It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs. Licensed software \u00a7 Reserved application groups exist for software that requires them. The group name will begin with leg or lg . After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the groups command. Please let us know your username when you ask to be added to a group. CASTEP : You/your group leader need to have signed up for a CASTEP license . Send us an acceptance email, or we can ask them to verify you have a license. You will then be added to the reserved application group lgcastep . If you are a member of UKCP you are already covered by a license and just need to tell us when you request access. CRYSTAL : You/your group leader need to have signed up for an Academic license. Crystal Solutions will send an email saying an account has been upgraded to \"Academic UK\" - forward that to us along with confirmation from the group leader that you should be in their group. You will be added to the legcryst group. DL_POLY : has individual licenses for specific versions. Sign up at DL_POLY's website and send us the acceptance email they give you. We will add you to the appropriate version's reserved application group, eg lgdlp408 . Gaussian : not currently accessible for non-UCL institutions. UCL having a site license and another institute having a site license does not allow users from the other institute to run Gaussian on UCL-owned hardware. VASP : When you request access you need to send us the name and email of the main VASP license holder along with the license number. We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group. You may also install your own copy in your home, and we provide a simple build script on Github (tested with VASP 5.4.4, no patches). You need to download the VASP source code and then you can run the script following the instructions at the top. Molpro : Only UCL users are licensed to use our central copy and can request to be added to the lgmolpro reserved application group. Suggested job sizes \u00a7 The target job sizes for Thomas are 48-120 cores (2-5 nodes). Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities. Maximum job resources \u00a7 Cores Max wallclock 864 48hrs On Thomas, interactive sessions using qrsh have the same wallclock limit as other jobs. Nodes in Thomas are 24 cores, 128G RAM. The default maximum jobsize is 864 cores, to remain within the 36-node 1:1 nonblocking interconnect zones. Jobs on Thomas do not share nodes . This means that if you request less than 24 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 24 for full usage of your nodes. There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the 3:1 interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details. Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full. Queue names \u00a7 On Thomas, users do not submit directly to queues - the scheduler assigns your job to one based on the resources it requested. The queues have somewhat unorthodox names as they are only used internally, and do not directly map to particular job types. Preventing a job from running cross-CU \u00a7 If your job must run within a single CU, you can request the parallel environment as -pe wss instead of -pe mpi ( wss standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue. back to top Disk quotas \u00a7 Quotas were enabled on Thomas on 29 May 2019. You have one per-user quota, with a default amount of 250GB - this is the total across home and Scratch. lquota shows you your quota and total usage (twice). request_quota is how you request a quota increase. If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write. Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the Thomas User Group for approval. back to top Budgets and allocations \u00a7 We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free. To see the name of your project(s) and how much allocation that budget has, run the command budgets . budgets Project Machines Balance -------- -------- -------- UCL_Test ANY 22781.89 Pilot users temporarily had access to a project for their institution, eg. Imperial_pilot. These projects are no longer active and will not show up. Info 1 Gold unit is 1 hour of using 1 processor core. Subprojects \u00a7 You might be in a subproject that does not itself have an allocation, but instead takes allocation from a different project: Project Machines Balance -------- -------- -------- UCL_physM ANY 474999.70 UCL_physM_Bowler ANY 0.00 In this case, you submit jobs using the subproject ( UCL_physM_Bowler here) even though it says it has 0 budget and it takes Gold from the superproject. Submitting a job under a project \u00a7 To submit a paid job that will take Gold from a particular project budget, add this to your jobscript: #$ -P Gold #$ -A MyProject To submit a free job that will not use up any Gold, use this instead: #$ -P Free #$ -A MyProject You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime: #$ -P Test #$ -A MyProject Troubleshooting: Unable to verify membership in policyjsv project \u00a7 Unable to run job: Rejected by policyjsv Unable to verify membership of `<username>` in the policyjsv project You asked for a Free job but didn't specify #$ -A MyProject in your jobscript. Troubleshooting: Unable to verify membership in project / Uninitialized value \u00a7 Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Unable to verify membership of mmmxxxx in the UCL_Example project This error from qsub can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, <GBALANCE> line 1. Failed sending message: (Unable to connect to socket (Connection refused)). If you also get this error from the budgets command, then the Gold daemon is definitely not running and you should contact rc-support. Gold charging \u00a7 When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run. Troubleshooting: Unable to verify sufficient material worth \u00a7 Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Insufficient balance to reserve job This means you don't have enough Gold to cover the cores \u2a09 wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time. Job deletion \u00a7 If you qdel a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly. The Tier 2 SAFE \u00a7 SAFE is a service administration platform used by ARCHER and various of the Tier 2 sites. As a user, you can use it to do some management of your details and view your usage across all systems that send data to SAFE. See your usage data in SAFE \u00a7 If you wish, you can claim your Thomas account as belonging to you in the Tier 2 SAFE. This lets you view some individual usage reports, and if you have other Tier 2 accounts that also use SAFE, you use the same login information. You need to login at https://www.archer.ac.uk/tier2/ or create a new Tier2 SAFE account if you do not have one. If creating a new account, you will be asked to create a password - this is for the SAFE login only and has no link to your Thomas account. On Thomas, run hashclaim . This will give you a link to claim your Thomas username's usage data. Visit this link when logged in to SAFE and it will tell you this account has been added to your SAFE account, and username@Thomas will now be visible in the 'Login accounts' menu at the top. Choosing that account will take you to a page where you can view your individual usage reports. If you do not claim your account, then SAFE only contains username/job usage information with no information about who owns that username. Points of Contact can use it to allocate Gold to your budgets. Update your SSH key via SAFE \u00a7 SAFE also gives you a second mechanism for changing your SSH key - changing the key you have associated with your SAFE account will automatically create a request for us to add that key to your Thomas account. (We still need to act on it, so it won't update immediately). Support \u00a7 Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Thomas in the subject along with some descriptive text about the type of problem, and you should mention your username in the body. Notional costs \u00a7 EPSRC contributed to the hardware, so there are two numbers to use for notional costs: Cost per core hour excluding hardware (to be charged on EPSRC grants): 0.6 pence / core hour Cost per core hour including hardware cost (to be charged on non-EPSRC grants): 1.5 pence / core hour Non-grant-funded projects should use the second figure. Acknowledging the use of Thomas in publications \u00a7 All work arising from this facility should be properly acknowledged in presentations and papers with the following text: \"We are grateful to the UK Materials and Molecular Modelling Hub for computational resources, which is partially funded by EPSRC (EP/P020194/1)\" MCC \u00a7 When publishing work that benefited from resources allocated by the MCC: please include the following acknowledgment: \"Via our membership of the UK's HEC Materials Chemistry Consortium, which is funded by EPSRC (EP/L000202), this work used the UK Materials and Molecular Modelling Hub for computational resources, MMM Hub, which is partially funded by EPSRC (EP/P020194)\" UKCP \u00a7 When publishing work that benefited from resources allocated by UKCP , please include: \"We are grateful for computational support from the UK Materials and Molecular Modelling Hub, which is partially funded by EPSRC (EP/P020194), for which access was obtained via the UKCP consortium and funded by EPSRC grant ref EP/P022561/1\"","title":"MMM Thomas"},{"location":"Clusters/Thomas/#applying-for-an-account","text":"Thomas accounts belong to you as an individual and are applied for through your own institution's Point of Contact . You will need to supply an SSH public key, which is the only method used to log in.","title":"Applying for an account"},{"location":"Clusters/Thomas/#creating-an-ssh-key-pair","text":"An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine).","title":"Creating an ssh key pair"},{"location":"Clusters/Thomas/#creating-an-ssh-key-in-linuxunixmac-os-x","text":"ssh-keygen -t rsa The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key. Do not use DSA as OpenSSH 7.0 has deprecated it and does not use it by default on client or server. We no longer accept DSA keys. You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it. You may need to run ssh-add to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running ssh-add -L will show all the public parts of the keys it is aware of.","title":"Creating an ssh key in Linux/Unix/Mac OS X"},{"location":"Clusters/Thomas/#creating-an-ssh-key-in-windows","text":"Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no longer accepted. The key must be at least 2048-bit. If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY.","title":"Creating an ssh key in Windows"},{"location":"Clusters/Thomas/#information-for-points-of-contact","text":"Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact .","title":"Information for Points of Contact"},{"location":"Clusters/Thomas/#logging-in","text":"You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form mmmxxxx (where xxxx is a number) and UCL users will use their central username. You ssh directly to: thomas.rc.ucl.ac.uk","title":"Logging in"},{"location":"Clusters/Thomas/#ssh-timeouts","text":"Idle ssh sessions will be disconnected after 7 days.","title":"SSH timeouts"},{"location":"Clusters/Thomas/#using-the-system","text":"Thomas is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\<15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.","title":"Using the system"},{"location":"Clusters/Thomas/#full-user-guide","text":"Thomas has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Thomas has should be listed on this page.","title":"Full user guide"},{"location":"Clusters/Thomas/#submitting-a-job","text":"Create a jobscript for non-interactive use and submit your jobscript using qsub . Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules. A job on Thomas must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below.)","title":"Submitting a job"},{"location":"Clusters/Thomas/#memory-requests","text":"Note: the memory you request is always per core, not the total amount. If you ask for 128GB RAM and 24 cores, that will run on 24 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process.","title":"Memory requests"},{"location":"Clusters/Thomas/#monitoring-a-job","text":"In addition to qstat , nodesforjob $JOB_ID can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on. qexplain $JOB_ID will show you the full error for a job that is in Eqw status.","title":"Monitoring a job"},{"location":"Clusters/Thomas/#useful-utilities","text":"As well as nodesforjob , there are the following utilities which can help you find information about your jobs after they have run. jobhist - shows your job history for the last 24hrs by default, including start and end times and the head node it ran on. You can view a longer history by specifying --hours=100 for example. scriptfor $JOB_ID - show the script that was submitted for the given job. These utilities live in GitHub at https://github.com/UCL-RITS/go-clustertools and https://github.com/UCL-RITS/rcps-cluster-scripts","title":"Useful utilities"},{"location":"Clusters/Thomas/#software","text":"Thomas mounts the RC Systems software stack . Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. Access to software is managed through the use of modules. module avail shows all modules available. module list shows modules currently loaded. Access to licensed software may vary based on your host institution and project.","title":"Software"},{"location":"Clusters/Thomas/#requesting-software-installs","text":"To request software installs, email us at the support address below or open an issue on our GitHub . You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested.","title":"Requesting software installs"},{"location":"Clusters/Thomas/#installing-your-own-software","text":"You may install software in your own space. Please look at Compiling for tips.","title":"Installing your own software"},{"location":"Clusters/Thomas/#maintaining-a-piece-of-software-for-a-group","text":"It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs.","title":"Maintaining a piece of software for a group"},{"location":"Clusters/Thomas/#licensed-software","text":"Reserved application groups exist for software that requires them. The group name will begin with leg or lg . After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the groups command. Please let us know your username when you ask to be added to a group. CASTEP : You/your group leader need to have signed up for a CASTEP license . Send us an acceptance email, or we can ask them to verify you have a license. You will then be added to the reserved application group lgcastep . If you are a member of UKCP you are already covered by a license and just need to tell us when you request access. CRYSTAL : You/your group leader need to have signed up for an Academic license. Crystal Solutions will send an email saying an account has been upgraded to \"Academic UK\" - forward that to us along with confirmation from the group leader that you should be in their group. You will be added to the legcryst group. DL_POLY : has individual licenses for specific versions. Sign up at DL_POLY's website and send us the acceptance email they give you. We will add you to the appropriate version's reserved application group, eg lgdlp408 . Gaussian : not currently accessible for non-UCL institutions. UCL having a site license and another institute having a site license does not allow users from the other institute to run Gaussian on UCL-owned hardware. VASP : When you request access you need to send us the name and email of the main VASP license holder along with the license number. We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group. You may also install your own copy in your home, and we provide a simple build script on Github (tested with VASP 5.4.4, no patches). You need to download the VASP source code and then you can run the script following the instructions at the top. Molpro : Only UCL users are licensed to use our central copy and can request to be added to the lgmolpro reserved application group.","title":"Licensed software"},{"location":"Clusters/Thomas/#suggested-job-sizes","text":"The target job sizes for Thomas are 48-120 cores (2-5 nodes). Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities.","title":"Suggested job sizes"},{"location":"Clusters/Thomas/#maximum-job-resources","text":"Cores Max wallclock 864 48hrs On Thomas, interactive sessions using qrsh have the same wallclock limit as other jobs. Nodes in Thomas are 24 cores, 128G RAM. The default maximum jobsize is 864 cores, to remain within the 36-node 1:1 nonblocking interconnect zones. Jobs on Thomas do not share nodes . This means that if you request less than 24 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 24 for full usage of your nodes. There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the 3:1 interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details. Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full.","title":"Maximum job resources"},{"location":"Clusters/Thomas/#queue-names","text":"On Thomas, users do not submit directly to queues - the scheduler assigns your job to one based on the resources it requested. The queues have somewhat unorthodox names as they are only used internally, and do not directly map to particular job types.","title":"Queue names"},{"location":"Clusters/Thomas/#preventing-a-job-from-running-cross-cu","text":"If your job must run within a single CU, you can request the parallel environment as -pe wss instead of -pe mpi ( wss standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue. back to top","title":"Preventing a job from running cross-CU"},{"location":"Clusters/Thomas/#disk-quotas","text":"Quotas were enabled on Thomas on 29 May 2019. You have one per-user quota, with a default amount of 250GB - this is the total across home and Scratch. lquota shows you your quota and total usage (twice). request_quota is how you request a quota increase. If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write. Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the Thomas User Group for approval. back to top","title":"Disk quotas"},{"location":"Clusters/Thomas/#budgets-and-allocations","text":"We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free. To see the name of your project(s) and how much allocation that budget has, run the command budgets . budgets Project Machines Balance -------- -------- -------- UCL_Test ANY 22781.89 Pilot users temporarily had access to a project for their institution, eg. Imperial_pilot. These projects are no longer active and will not show up. Info 1 Gold unit is 1 hour of using 1 processor core.","title":"Budgets and allocations"},{"location":"Clusters/Thomas/#subprojects","text":"You might be in a subproject that does not itself have an allocation, but instead takes allocation from a different project: Project Machines Balance -------- -------- -------- UCL_physM ANY 474999.70 UCL_physM_Bowler ANY 0.00 In this case, you submit jobs using the subproject ( UCL_physM_Bowler here) even though it says it has 0 budget and it takes Gold from the superproject.","title":"Subprojects"},{"location":"Clusters/Thomas/#submitting-a-job-under-a-project","text":"To submit a paid job that will take Gold from a particular project budget, add this to your jobscript: #$ -P Gold #$ -A MyProject To submit a free job that will not use up any Gold, use this instead: #$ -P Free #$ -A MyProject You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime: #$ -P Test #$ -A MyProject","title":"Submitting a job under a project"},{"location":"Clusters/Thomas/#troubleshooting-unable-to-verify-membership-in-policyjsv-project","text":"Unable to run job: Rejected by policyjsv Unable to verify membership of `<username>` in the policyjsv project You asked for a Free job but didn't specify #$ -A MyProject in your jobscript.","title":"Troubleshooting: Unable to verify membership in policyjsv project"},{"location":"Clusters/Thomas/#troubleshooting-unable-to-verify-membership-in-project-uninitialized-value","text":"Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Unable to verify membership of mmmxxxx in the UCL_Example project This error from qsub can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, <GBALANCE> line 1. Failed sending message: (Unable to connect to socket (Connection refused)). If you also get this error from the budgets command, then the Gold daemon is definitely not running and you should contact rc-support.","title":"Troubleshooting: Unable to verify membership in project / Uninitialized value"},{"location":"Clusters/Thomas/#gold-charging","text":"When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run.","title":"Gold charging"},{"location":"Clusters/Thomas/#troubleshooting-unable-to-verify-sufficient-material-worth","text":"Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Insufficient balance to reserve job This means you don't have enough Gold to cover the cores \u2a09 wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time.","title":"Troubleshooting: Unable to verify sufficient material worth"},{"location":"Clusters/Thomas/#job-deletion","text":"If you qdel a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly.","title":"Job deletion"},{"location":"Clusters/Thomas/#the-tier-2-safe","text":"SAFE is a service administration platform used by ARCHER and various of the Tier 2 sites. As a user, you can use it to do some management of your details and view your usage across all systems that send data to SAFE.","title":"The Tier 2 SAFE"},{"location":"Clusters/Thomas/#see-your-usage-data-in-safe","text":"If you wish, you can claim your Thomas account as belonging to you in the Tier 2 SAFE. This lets you view some individual usage reports, and if you have other Tier 2 accounts that also use SAFE, you use the same login information. You need to login at https://www.archer.ac.uk/tier2/ or create a new Tier2 SAFE account if you do not have one. If creating a new account, you will be asked to create a password - this is for the SAFE login only and has no link to your Thomas account. On Thomas, run hashclaim . This will give you a link to claim your Thomas username's usage data. Visit this link when logged in to SAFE and it will tell you this account has been added to your SAFE account, and username@Thomas will now be visible in the 'Login accounts' menu at the top. Choosing that account will take you to a page where you can view your individual usage reports. If you do not claim your account, then SAFE only contains username/job usage information with no information about who owns that username. Points of Contact can use it to allocate Gold to your budgets.","title":"See your usage data in SAFE"},{"location":"Clusters/Thomas/#update-your-ssh-key-via-safe","text":"SAFE also gives you a second mechanism for changing your SSH key - changing the key you have associated with your SAFE account will automatically create a request for us to add that key to your Thomas account. (We still need to act on it, so it won't update immediately).","title":"Update your SSH key via SAFE"},{"location":"Clusters/Thomas/#support","text":"Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Thomas in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.","title":"Support"},{"location":"Clusters/Thomas/#notional-costs","text":"EPSRC contributed to the hardware, so there are two numbers to use for notional costs: Cost per core hour excluding hardware (to be charged on EPSRC grants): 0.6 pence / core hour Cost per core hour including hardware cost (to be charged on non-EPSRC grants): 1.5 pence / core hour Non-grant-funded projects should use the second figure.","title":"Notional costs"},{"location":"Clusters/Thomas/#acknowledging-the-use-of-thomas-in-publications","text":"All work arising from this facility should be properly acknowledged in presentations and papers with the following text: \"We are grateful to the UK Materials and Molecular Modelling Hub for computational resources, which is partially funded by EPSRC (EP/P020194/1)\"","title":"Acknowledging the use of Thomas in publications"},{"location":"Clusters/Thomas/#mcc","text":"When publishing work that benefited from resources allocated by the MCC: please include the following acknowledgment: \"Via our membership of the UK's HEC Materials Chemistry Consortium, which is funded by EPSRC (EP/L000202), this work used the UK Materials and Molecular Modelling Hub for computational resources, MMM Hub, which is partially funded by EPSRC (EP/P020194)\"","title":"MCC"},{"location":"Clusters/Thomas/#ukcp","text":"When publishing work that benefited from resources allocated by UKCP , please include: \"We are grateful for computational support from the UK Materials and Molecular Modelling Hub, which is partially funded by EPSRC (EP/P020194), for which access was obtained via the UKCP consortium and funded by EPSRC grant ref EP/P022561/1\"","title":"UKCP"},{"location":"Clusters/Young/","text":"Young is the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling, and replacement for Thomas. Young went into pilot on 3 Aug 2020. Applying for an account \u00a7 Young accounts belong to you as an individual and are applied for through your own institution's Point of Contact . You will need to supply an SSH public key, which is the only method used to log in. Creating an ssh key pair \u00a7 An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine). Creating an ssh key in Linux/Unix/Mac OS X \u00a7 ssh-keygen -t rsa The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key. Do not use DSA as OpenSSH 7.0 has deprecated it and does not use it by default on client or server. We no longer accept DSA keys. You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it. You may need to run ssh-add to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running ssh-add -L will show all the public parts of the keys it is aware of. Creating an ssh key in Windows \u00a7 Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no longer accepted. The key must be at least 2048-bit. If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY. Information for Points of Contact \u00a7 Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact . Logging in \u00a7 You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form mmmxxxx (where xxxx is a number) and UCL users will use their central username. You ssh directly to: young.rc.ucl.ac.uk SSH timeouts \u00a7 Idle ssh sessions will be disconnected after 7 days. Using the system \u00a7 Young is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\<15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job. Full user guide \u00a7 Young has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Young has should be listed on this page. Submitting a job \u00a7 Create a jobscript for non-interactive use and submit your jobscript using qsub . Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules. A job on Young must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below.) Memory requests \u00a7 Note: the memory you request is always per core, not the total amount. If you ask for 192GB RAM and 40 cores, that may run on 40 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process. Young also has high memory nodes , where a job like this may run. Monitoring a job \u00a7 In addition to qstat , nodesforjob $JOB_ID can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on. qexplain $JOB_ID will show you the full error for a job that is in Eqw status. Useful utilities \u00a7 As well as nodesforjob , there are the following utilities which can help you find information about your jobs after they have run. jobhist - shows your job history for the last 24hrs by default, including start and end times and the head node it ran on. You can view a longer history by specifying --hours=100 for example. scriptfor $JOB_ID - show the script that was submitted for the given job. These utilities live in GitHub at https://github.com/UCL-RITS/go-clustertools and https://github.com/UCL-RITS/rcps-cluster-scripts Software \u00a7 Young mounts the RC Systems software stack . Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. Access to software is managed through the use of modules. module avail shows all modules available. module list shows modules currently loaded. Access to licensed software may vary based on your host institution and project. Loading and unloading modules \u00a7 Young has a newer version of modulecmd which tries to manage module dependencies automatically by loading or unloading prerequisites for you whenever possible. If you get an error like this: [uccaxxx@login01 ~]$ module unload compilers mpi Unloading compilers/intel/2018/update3 ERROR: compilers/intel/2018/update3 cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. Unloading mpi/intel/2018/update3/intel ERROR: mpi/intel/2018/update3/intel cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. You can use the -f option to force the module change. It will carry it out and warn you about modules it thinks are dependent. [uccaxxx@login01 ~]$ module unload -f compilers mpi Unloading compilers/intel/2018/update3 WARNING: Dependent default-modules/2018 is loaded Unloading mpi/intel/2018/update3/intel WARNING: Dependent default-modules/2018 is loaded Requesting software installs \u00a7 To request software installs, email us at the support address below or open an issue on our GitHub . You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested. Installing your own software \u00a7 You may install software in your own space. Please look at Compiling for tips. Maintaining a piece of software for a group \u00a7 It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs. Licensed software \u00a7 Reserved application groups exist for software that requires them. The group name will begin with leg or lg . After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the groups command. Please let us know your username when you ask to be added to a group. CASTEP : You/your group leader need to have signed up for a CASTEP license . Send us an acceptance email, or we can ask them to verify you have a license. You will then be added to the reserved application group lgcastep . If you are a member of UKCP you are already covered by a license and just need to tell us when you request access. CRYSTAL : You/your group leader need to have signed up for an Academic license. Crystal Solutions will send an email saying an account has been upgraded to \"Academic UK\" - forward that to us along with confirmation from the group leader that you should be in their group. You will be added to the legcryst group. DL_POLY : has individual licenses for specific versions. Sign up at DL_POLY's website and send us the acceptance email they give you. We will add you to the appropriate version's reserved application group, eg lgdlp408 . Gaussian : not currently accessible for non-UCL institutions. UCL having a site license and another institute having a site license does not allow users from the other institute to run Gaussian on UCL-owned hardware. VASP : When you request access you need to send us the name and email of the main VASP license holder along with the license number. We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group. You may also install your own copy in your home, and we provide a simple build script on Github (tested with VASP 5.4.4, no patches). You need to download the VASP source code and then you can run the script following the instructions at the top. Molpro : Only UCL users are licensed to use our central copy and can request to be added to the lgmolpro reserved application group. Suggested job sizes \u00a7 The target job sizes for Young are 2-5 nodes. Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities. Maximum job resources \u00a7 Cores Max wallclock 5120 48hrs These are numbers of physical cores: multiply by two for virtual cores with hyperthreads . On Young, interactive sessions using qrsh have the same wallclock limit as other jobs. Jobs on Young do not share nodes . This means that if you request less than 40 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 40 for full usage of your nodes. There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details. Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full. Preventing a job from running cross-CU \u00a7 If your job must run within a single CU, you can request the parallel environment as -pe wss instead of -pe mpi ( wss standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue. back to top Node types \u00a7 Young has three types of node: standard nodes, big memory nodes, and really big memory nodes. Note those last have only 36 cores per node, not 40. Type Cores per node RAM per node tmpfs Nodes C 40 192G None 576 Y 40 1.5T None 3 Z 36 3.0T None 3 These are numbers of physical cores: multiply by two for virtual cores with hyperthreading. Restricting to one node type \u00a7 The scheduler will schedule your job on the relevant nodetype based on the resources you request, but if you really need to specify the nodetype yourself, use: # Only run on Z-type nodes #$ -ac allow=Z Hyperthreading \u00a7 Young has hyperthreading enabled and you can choose on a per-job basis whether you want to use it. Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. If you do not ask for hyperthreading, your job only uses one thread per core as normal. # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process export OMP_NUM_THREADS=2 This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads). # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process # (a whole node's worth) export OMP_NUM_THREADS=80 This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads). Diskless nodes \u00a7 Young nodes are diskless (have no local hard drives) - there is no $TMPDIR available, so you should not request -l tmpfs=10G in your jobscripts or your job will be rejected at submit time. If you need temporary space, you should use somewhere in your Scratch. Disk quotas \u00a7 You have one per-user quota, with a default amount of 250GB - this is the total across home and Scratch. lquota shows you your quota and total usage (twice). request_quota is how you request a quota increase. If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write. Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the MMM Hub User Group for approval. back to top Budgets and allocations \u00a7 We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free. To see the name of your project(s) and how much allocation that budget has, run the command budgets . budgets Project Machines Balance -------- -------- -------- UCL_Test ANY 22781.89 Pilot users temporarily have access to a project for their institution, eg. Imperial_pilot. These will be deactivated after the pilot and no longer show up. Info 1 Gold unit is 1 hour of using 1 virtual processor core (= 0.5 physical core). Since Young has hyperthreading , a job asking for 40 physical cores and one asking for 80 virtual cores with hyperthreading on both cost the same amount: 80 Gold. Subprojects \u00a7 You might be in a subproject that does not itself have an allocation, but instead takes allocation from a different project: Project Machines Balance -------- -------- -------- UCL_physM ANY 474999.70 UCL_physM_Bowler ANY 0.00 In this case, you submit jobs using the subproject ( UCL_physM_Bowler here) even though it says it has 0 budget and it takes Gold from the superproject. Submitting a job under a project \u00a7 To submit a paid job that will take Gold from a particular project budget, add this to your jobscript: #$ -P Gold #$ -A MyProject To submit a free job that will not use up any Gold, use this instead: #$ -P Free #$ -A MyProject You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime: #$ -P Test #$ -A MyProject Troubleshooting: Unable to verify membership in policyjsv project \u00a7 Unable to run job: Rejected by policyjsv Unable to verify membership of `<username>` in the policyjsv project You asked for a Free job but didn't specify #$ -A MyProject in your jobscript. Troubleshooting: Unable to verify membership in project / Uninitialized value \u00a7 Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Unable to verify membership of mmmxxxx in the UCL_Example project This error from qsub can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, <GBALANCE> line 1. Failed sending message: (Unable to connect to socket (Connection refused)). If you also get this error from the budgets command, then the Gold daemon is definitely not running and you should contact rc-support. Gold charging \u00a7 When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run. Troubleshooting: Unable to verify sufficient material worth \u00a7 Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Insufficient balance to reserve job This means you don't have enough Gold to cover the cores \u2a09 wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time. Job deletion \u00a7 If you qdel a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly. Support \u00a7 Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Young in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.","title":"MMM Hub Young"},{"location":"Clusters/Young/#applying-for-an-account","text":"Young accounts belong to you as an individual and are applied for through your own institution's Point of Contact . You will need to supply an SSH public key, which is the only method used to log in.","title":"Applying for an account"},{"location":"Clusters/Young/#creating-an-ssh-key-pair","text":"An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine).","title":"Creating an ssh key pair"},{"location":"Clusters/Young/#creating-an-ssh-key-in-linuxunixmac-os-x","text":"ssh-keygen -t rsa The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key. Do not use DSA as OpenSSH 7.0 has deprecated it and does not use it by default on client or server. We no longer accept DSA keys. You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it. You may need to run ssh-add to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running ssh-add -L will show all the public parts of the keys it is aware of.","title":"Creating an ssh key in Linux/Unix/Mac OS X"},{"location":"Clusters/Young/#creating-an-ssh-key-in-windows","text":"Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no longer accepted. The key must be at least 2048-bit. If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY.","title":"Creating an ssh key in Windows"},{"location":"Clusters/Young/#information-for-points-of-contact","text":"Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact .","title":"Information for Points of Contact"},{"location":"Clusters/Young/#logging-in","text":"You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form mmmxxxx (where xxxx is a number) and UCL users will use their central username. You ssh directly to: young.rc.ucl.ac.uk","title":"Logging in"},{"location":"Clusters/Young/#ssh-timeouts","text":"Idle ssh sessions will be disconnected after 7 days.","title":"SSH timeouts"},{"location":"Clusters/Young/#using-the-system","text":"Young is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\<15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.","title":"Using the system"},{"location":"Clusters/Young/#full-user-guide","text":"Young has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Young has should be listed on this page.","title":"Full user guide"},{"location":"Clusters/Young/#submitting-a-job","text":"Create a jobscript for non-interactive use and submit your jobscript using qsub . Jobscripts must begin #!/bin/bash -l in order to run as a login shell and get your login environment and modules. A job on Young must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below.)","title":"Submitting a job"},{"location":"Clusters/Young/#memory-requests","text":"Note: the memory you request is always per core, not the total amount. If you ask for 192GB RAM and 40 cores, that may run on 40 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process. Young also has high memory nodes , where a job like this may run.","title":"Memory requests"},{"location":"Clusters/Young/#monitoring-a-job","text":"In addition to qstat , nodesforjob $JOB_ID can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on. qexplain $JOB_ID will show you the full error for a job that is in Eqw status.","title":"Monitoring a job"},{"location":"Clusters/Young/#useful-utilities","text":"As well as nodesforjob , there are the following utilities which can help you find information about your jobs after they have run. jobhist - shows your job history for the last 24hrs by default, including start and end times and the head node it ran on. You can view a longer history by specifying --hours=100 for example. scriptfor $JOB_ID - show the script that was submitted for the given job. These utilities live in GitHub at https://github.com/UCL-RITS/go-clustertools and https://github.com/UCL-RITS/rcps-cluster-scripts","title":"Useful utilities"},{"location":"Clusters/Young/#software","text":"Young mounts the RC Systems software stack . Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. Access to software is managed through the use of modules. module avail shows all modules available. module list shows modules currently loaded. Access to licensed software may vary based on your host institution and project.","title":"Software"},{"location":"Clusters/Young/#loading-and-unloading-modules","text":"Young has a newer version of modulecmd which tries to manage module dependencies automatically by loading or unloading prerequisites for you whenever possible. If you get an error like this: [uccaxxx@login01 ~]$ module unload compilers mpi Unloading compilers/intel/2018/update3 ERROR: compilers/intel/2018/update3 cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. Unloading mpi/intel/2018/update3/intel ERROR: mpi/intel/2018/update3/intel cannot be unloaded due to a prereq. HINT: Might try \"module unload default-modules/2018\" first. You can use the -f option to force the module change. It will carry it out and warn you about modules it thinks are dependent. [uccaxxx@login01 ~]$ module unload -f compilers mpi Unloading compilers/intel/2018/update3 WARNING: Dependent default-modules/2018 is loaded Unloading mpi/intel/2018/update3/intel WARNING: Dependent default-modules/2018 is loaded","title":"Loading and unloading modules"},{"location":"Clusters/Young/#requesting-software-installs","text":"To request software installs, email us at the support address below or open an issue on our GitHub . You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested.","title":"Requesting software installs"},{"location":"Clusters/Young/#installing-your-own-software","text":"You may install software in your own space. Please look at Compiling for tips.","title":"Installing your own software"},{"location":"Clusters/Young/#maintaining-a-piece-of-software-for-a-group","text":"It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs.","title":"Maintaining a piece of software for a group"},{"location":"Clusters/Young/#licensed-software","text":"Reserved application groups exist for software that requires them. The group name will begin with leg or lg . After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the groups command. Please let us know your username when you ask to be added to a group. CASTEP : You/your group leader need to have signed up for a CASTEP license . Send us an acceptance email, or we can ask them to verify you have a license. You will then be added to the reserved application group lgcastep . If you are a member of UKCP you are already covered by a license and just need to tell us when you request access. CRYSTAL : You/your group leader need to have signed up for an Academic license. Crystal Solutions will send an email saying an account has been upgraded to \"Academic UK\" - forward that to us along with confirmation from the group leader that you should be in their group. You will be added to the legcryst group. DL_POLY : has individual licenses for specific versions. Sign up at DL_POLY's website and send us the acceptance email they give you. We will add you to the appropriate version's reserved application group, eg lgdlp408 . Gaussian : not currently accessible for non-UCL institutions. UCL having a site license and another institute having a site license does not allow users from the other institute to run Gaussian on UCL-owned hardware. VASP : When you request access you need to send us the name and email of the main VASP license holder along with the license number. We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group. You may also install your own copy in your home, and we provide a simple build script on Github (tested with VASP 5.4.4, no patches). You need to download the VASP source code and then you can run the script following the instructions at the top. Molpro : Only UCL users are licensed to use our central copy and can request to be added to the lgmolpro reserved application group.","title":"Licensed software"},{"location":"Clusters/Young/#suggested-job-sizes","text":"The target job sizes for Young are 2-5 nodes. Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities.","title":"Suggested job sizes"},{"location":"Clusters/Young/#maximum-job-resources","text":"Cores Max wallclock 5120 48hrs These are numbers of physical cores: multiply by two for virtual cores with hyperthreads . On Young, interactive sessions using qrsh have the same wallclock limit as other jobs. Jobs on Young do not share nodes . This means that if you request less than 40 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 40 for full usage of your nodes. There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details. Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full.","title":"Maximum job resources"},{"location":"Clusters/Young/#preventing-a-job-from-running-cross-cu","text":"If your job must run within a single CU, you can request the parallel environment as -pe wss instead of -pe mpi ( wss standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue. back to top","title":"Preventing a job from running cross-CU"},{"location":"Clusters/Young/#node-types","text":"Young has three types of node: standard nodes, big memory nodes, and really big memory nodes. Note those last have only 36 cores per node, not 40. Type Cores per node RAM per node tmpfs Nodes C 40 192G None 576 Y 40 1.5T None 3 Z 36 3.0T None 3 These are numbers of physical cores: multiply by two for virtual cores with hyperthreading.","title":"Node types"},{"location":"Clusters/Young/#restricting-to-one-node-type","text":"The scheduler will schedule your job on the relevant nodetype based on the resources you request, but if you really need to specify the nodetype yourself, use: # Only run on Z-type nodes #$ -ac allow=Z","title":"Restricting to one node type"},{"location":"Clusters/Young/#hyperthreading","text":"Young has hyperthreading enabled and you can choose on a per-job basis whether you want to use it. Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. If you do not ask for hyperthreading, your job only uses one thread per core as normal. # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process export OMP_NUM_THREADS=2 This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads). # request hyperthreading in this job #$ -l threads=1 # request the number of virtual cores #$ -pe mpi 160 # set number of OpenMP threads being used per MPI process # (a whole node's worth) export OMP_NUM_THREADS=80 This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads).","title":"Hyperthreading"},{"location":"Clusters/Young/#diskless-nodes","text":"Young nodes are diskless (have no local hard drives) - there is no $TMPDIR available, so you should not request -l tmpfs=10G in your jobscripts or your job will be rejected at submit time. If you need temporary space, you should use somewhere in your Scratch.","title":"Diskless nodes"},{"location":"Clusters/Young/#disk-quotas","text":"You have one per-user quota, with a default amount of 250GB - this is the total across home and Scratch. lquota shows you your quota and total usage (twice). request_quota is how you request a quota increase. If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write. Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the MMM Hub User Group for approval. back to top","title":"Disk quotas"},{"location":"Clusters/Young/#budgets-and-allocations","text":"We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free. To see the name of your project(s) and how much allocation that budget has, run the command budgets . budgets Project Machines Balance -------- -------- -------- UCL_Test ANY 22781.89 Pilot users temporarily have access to a project for their institution, eg. Imperial_pilot. These will be deactivated after the pilot and no longer show up. Info 1 Gold unit is 1 hour of using 1 virtual processor core (= 0.5 physical core). Since Young has hyperthreading , a job asking for 40 physical cores and one asking for 80 virtual cores with hyperthreading on both cost the same amount: 80 Gold.","title":"Budgets and allocations"},{"location":"Clusters/Young/#subprojects","text":"You might be in a subproject that does not itself have an allocation, but instead takes allocation from a different project: Project Machines Balance -------- -------- -------- UCL_physM ANY 474999.70 UCL_physM_Bowler ANY 0.00 In this case, you submit jobs using the subproject ( UCL_physM_Bowler here) even though it says it has 0 budget and it takes Gold from the superproject.","title":"Subprojects"},{"location":"Clusters/Young/#submitting-a-job-under-a-project","text":"To submit a paid job that will take Gold from a particular project budget, add this to your jobscript: #$ -P Gold #$ -A MyProject To submit a free job that will not use up any Gold, use this instead: #$ -P Free #$ -A MyProject You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime: #$ -P Test #$ -A MyProject","title":"Submitting a job under a project"},{"location":"Clusters/Young/#troubleshooting-unable-to-verify-membership-in-policyjsv-project","text":"Unable to run job: Rejected by policyjsv Unable to verify membership of `<username>` in the policyjsv project You asked for a Free job but didn't specify #$ -A MyProject in your jobscript.","title":"Troubleshooting: Unable to verify membership in policyjsv project"},{"location":"Clusters/Young/#troubleshooting-unable-to-verify-membership-in-project-uninitialized-value","text":"Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Unable to verify membership of mmmxxxx in the UCL_Example project This error from qsub can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, <GBALANCE> line 1. Failed sending message: (Unable to connect to socket (Connection refused)). If you also get this error from the budgets command, then the Gold daemon is definitely not running and you should contact rc-support.","title":"Troubleshooting: Unable to verify membership in project / Uninitialized value"},{"location":"Clusters/Young/#gold-charging","text":"When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run.","title":"Gold charging"},{"location":"Clusters/Young/#troubleshooting-unable-to-verify-sufficient-material-worth","text":"Unable to run job: Rejected by policyjsv Reason:Unable to verify sufficient material worth to submit this job: Insufficient balance to reserve job This means you don't have enough Gold to cover the cores \u2a09 wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time.","title":"Troubleshooting: Unable to verify sufficient material worth"},{"location":"Clusters/Young/#job-deletion","text":"If you qdel a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly.","title":"Job deletion"},{"location":"Clusters/Young/#support","text":"Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Young in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.","title":"Support"},{"location":"Installed_Software_Lists/module-packages/","text":"General Software Lists \u00a7 Our clusters have a wide range of software installed, available by using the modules system. The module files are organised by name, version, variant (where applicable) and, if relevant, the compiler version used to build the software. If no compiler version is given, either no compiler was required, or only the base system compiler ( /usr/bin/gcc ) and libraries were used. When we install applications, we try to install them on all of our clusters, but sometimes licence restrictions prevent it. If something seems to be missing, it may be because we are not able to provide it. Please contact us for more information if this is hindering your work. The lists below were last updated at 15:31:16 (+0100) on 06 Jun 2020, and are generated from the software installed on the Myriad cluster. Bundles \u00a7 Some applications or tools depend on a lot of other modules, or have some awkward requirements. For these, we sometimes make a \"bundle\" module in this section, that loads all the dependencies. For Python and R in particular, we also have recommended bundles that load the module for a recent version of Python or R, along with a collection of packages for it that have been requested by users, and the modules those packages require. The lists of Python and R packages installed for those bundles are on separate pages: Python packages R packages We'll sometimes include /new and /old versions of these bundles, if we've recently made a version switch or are intending to make one soon. We send out emails to the user lists about version changes, so if you use these bundles, you should look out for those. Module Description beta-modules This module adds the beta module space to your environment. bioperl/recommended Loads all the modules needed to use BioPerl. blic-modules Adds Cancer Biology supported modules to your environment. cancerit/20190218 adds UCL set of cancerit packages to your environment variables cancerit/recommended adds UCL recommended set of cancerit packages to your environment variables chemistry-modules Adds Chemistry Department supported modules to your environment. climate-tools/recommended Adds set of default applications to the environment for climate science users. deep_earth Sets up VASP, Gnuplot etc for Earth Sciences default-modules-aristotle Adds default Aristotle modules to your environment. default-modules/2015 Adds default Legion modules to your environment. default-modules/2017 Adds default Legion modules to your environment. default-modules/2018 Adds default Legion modules to your environment. economics-modules Adds Economics Department modules to your environment. farr-modules Adds FARR supported modules to your environment. farr/recommended Adds set of default applications to the environment for FARR users. gmt/new Adds set of default modules to the environment for GMT users. gmt/old Adds set of default modules to the environment for gmt users. gmt/recommended Adds set of default modules to the environment for gmt users. naglib/mark27-intel-2019 adds the NAG Library Mark 27 and required modules to your environment. octave/recommended Octave is an open source competitor to Matlab. personal-modules Adds personal modules to your environment. physics-modules Adds Pysics Department supported modules to your environment. pypy3/3.5-compat Adds UCL recommended set of Pypy3 python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/pypy-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/pypy-3.list python2/recommended Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-2.list python3/3.4 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.5 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.6 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.7 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.8 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/recommended Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list r/new adds UCL recommended set of R packages to your environment variables r/old adds UCL recommended set of R packages to your environment variables r/r-3.6.0_bc-3.9 adds UCL recommended set of R packages to your environment variables r/recommended adds UCL recommended set of R packages to your environment variables rsd-modules Adds Research Software Development supported modules to your environment. thermo-modules Adds modules for Molecular Thermodynamics to your environment. torch-deps Loads the dependencies for Torch and makes a quick-install alias. workaround-modules This module adds the workarounds module space to your environment. Applications \u00a7 Module Description abaqus/2017 Adds Abaqus 2017 to your environment. adf/2014.10 Adds ADF 2014.10 to your environment. afni/20151030 Adds AFNI to your environment. afni/20181011 Adds AFNI to your environment. amber/14/mpi/intel-2015-update2 Adds AMBER 14 to your environment amber/14/openmp/intel-2015-update2 Adds AMBER 14 to your environment amber/14/serial/intel-2015-update2 Adds AMBER 14 to your environment amber/16/mpi/gnu-4.9.2 Adds AMBER 16 to your environment amber/16/mpi/intel-2015-update2 Adds AMBER 16 to your environment amber/16/openmp/gnu-4.9.2 Adds AMBER 16 to your environment amber/16/openmp/intel-2015-update2 Adds AMBER 16 to your environment amber/16/serial/gnu-4.9.2 Adds AMBER 16 to your environment amber/16/serial/intel-2015-update2 Adds AMBER 16 to your environment ansys/17.2 Adds Ansys CFX/Fluent etc to your environment ansys/18.0 Adds Ansys CFX/Fluent etc to your environment ansys/19.1 Adds Ansys CFX/Fluent, EM etc to your environment ansys/2019.r3 Adds Ansys CFX/Fluent, EM etc to your environment ants/2.1.0 Adds ANTs 2.1.0 (Advanced Normalization Tools) to your environment. ANTs is popularly considered a state-of-the-art medical image registration and segmentation toolkit. approxwf/gnu-4.9.2 Adds ApproxWF to your environment. arrayfire/3.5.0/gnu-4.9.2 Adds ArrayFire 3.5.0 to your environment. asp/2.6.2 Adds NASA Ames Stereo Pipeline (ASP) 6.2.2 to your environment. autodock/4.2.6 Adds AutoDock and AutoGrid 4.2.6 to your environment. AutoDock is a suite of automated docking tools. It is designed to predict how small molecules, such as substrates or drug candidates, bind to a receptor of known 3D structure. bamtools/2.4.0/gnu-4.9.2 Adds BamTools 2.4.0 to your environment. BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. bcftools/1.2/gnu-4.9.2 Adds BCFtools 1.2 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants bcftools/1.3.1/gnu-4.9.2 Adds BCFtools 1.3.1 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants bcftools/2.1/gnu-4.9.2 Adds BCFtools 1.2 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants bcl2fastq/1.8.4 Adds bcl2fastq 1.8.4 to your environment. bcl2fastq2/2.19.1 Adds bcl2fastq2 2.19.1 to your environment. beast/2.3.0 Adds BEAST 2.3.0 with addons to your PATH. bedtools/2.25.0 Adds bedtools 2.25.0 to your environment. The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. bgen/1.1.4 Adds BGen 1.1.4 to your environment. blast+/2.2.30/intel-2015-update2 This module adds the BLAST+ 2.2.30 package to your environment. blast/2.2.26 Adds Blast 2.2.26 to your environment. blender/2.79 Adds Blender Version 2.79 to your environment. boltztrap/1.2.5/intel-2018 Adds boltztrap 1.2.5 to your environment. bowtie/1.1.2 Adds Bowtie 1.1.2 to your environment. bowtie2/2.2.5 Adds Bowtie2 2.2.5 to your environment. bwa/0.6.2/gnu-4.9.2 Adds BWA 0.7.12 to your environment. BWA is a software package for mapping DNA sequences against a large reference genome, such as the human genome. bwa/0.7.12/gnu-4.9.2 Adds BWA 0.7.12 to your environment. BWA is a software package for mapping DNA sequences against a large reference genome, such as the human genome. caffe/1.0/cpu Adds Caffe 1.0 for CPU to your environment. caffe/1.0/cudnn Adds Caffe 1.0 for CUDA+CudNN to your environment. caffe/1.0/gpu Adds Caffe 1.0 for CUDA to your environment. cancerit/20190218-python-2.7.12/gnu-4.9.2 Adds CancerIT program versions as of 20190218 to your environment. The CancerIT Suite is a collection of linked bioinformatics tools. cancerit/gnu-4.9.2 Adds the cancer it suite to your environment. castep/17.2/intel-2017 Adds castep 17.2 to your environment. CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. castep/17.21/intel-2017 Adds castep 17.21 to your environment. CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. castep/19.1.1/intel-2019 CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. cctools/5.4.1/gnu-4.9.2 Adds cctools 5.4.1 to your environment. cctools/7.0.11/gnu-4.9.2 Adds cctools 7.0.11 to your environment. cesm/1.0.6/intel-2015-update2 Adds CESM 1.0.6 to your environment. cesm/1.2.2/intel-2015-update2 Adds CESM 1.2.2 to your environment. cfd-ace/2014.1 Adds CFD-ACE+ to your execution path. Only on Grace and Myriad. cfd-ace/2018.0 Adds CFD-ACE+ to your execution path. Only on Grace and Myriad. chemshell/3.7.1/mpi/gulp4.5 This is a modulefile for ChemShell 3.7.1, MPI+GULP version. Can be used to run other packages if you load a module for those. chemshell/3.7.1/standalone This is a modulefile for ChemShell 3.7.1, standalone serial version. Can be used to run GULP and other packages if you load a module for those. clustal-omega/1.2.1 Adds Clustal Omega 1.2.1 to your environment. clustal-w/2.1 Adds Clustal W 2.1 to your environment. cmg/2017.101 Adds CMG Reservoir Simulation Software Version 2017.101 to your environment. cmg/2018.101 Adds CMG Reservoir Simulation Software Version 2018.101 to your environment. cmg/2019.101 Adds CMG Reservoir Simulation Software Version 2019.101 to your environment. collectl/4.0.2 [collectl/4.0.2] collectl is a tool for tracking and monitoring various node usage statistics. compucell3d/3.7.4 Adds CompuCell3D to your environment comsol/52 Adds the COMSOL 52 binaries to your environment. COMSOL Multiphysics\u00ae is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. Module must be loaded once from a login node prior to running jobs. comsol/52a Adds the COMSOL 52a binaries to your environment. COMSOL Multiphysics\u00ae is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. Module must be loaded once from a login node prior to running jobs. comsol/53a Adds COMSOL Multiphysics Version 53a to your environment. cosi-corr/oct14 Adds COSI-Corr Version Oct14 for use with ENVI 5.5.2/5.5.3 to your environment. covid-19-spatial-sim/0.8.0/intel-2020 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.9.0/gnu-4.9.2 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.13.0/gnu-4.9.2 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.14.0/gnu-4.9.2 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.14.0/intel-2020 SpatialSim COVID-19 pandemic modelling tool from Imperial College. cp2k/4.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cp2k/5.1/ompi-plumed/gnu-4.9.2 Adds CP2K to your environment. cp2k/5.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cp2k/6.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cp2k/7.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cpmd/4.1/intel-2017 Adds CPMD 4.1 to your environment. crystal14/v1.0.3 Adds Crystal14 v1.0.3 to your environment. crystal14/v1.0.4 Adds Crystal14 v1.0.4 to your environment. crystal14/v1.0.4_2017 Adds Crystal14 v1.0.4 to your environment. crystal17/v1.0.1 Adds Crystal17 v1.0.1 to your environment. crystal17/v1.0.2/intel-2017 The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. cuba/4.2/gnu-4.9.2 adds Cuba Numerical Integration Package Version 4.2 to your environment. cufflinks/2.2.1 Adds Cufflinks 2.2.1 to your environment. curl/7.47.1/gnu-4.9.2 Adds curl 7.47.1 to your environment. datamash/1.4 This is a module with no description string. deeptools/3.0.2 Adds deeptools to your environment. delly/0.7.8-bindist Delly is an integrated structural variant (SV) prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. dftbplus/17.1/intel-2017 DFTB+ is a quantum mechanical simulation software package, based on the Density Functional Tight Binding (DFTB) method. dftbplus/18.2/intel-2018 DFTB+ is a software package for carrying out fast quantum mechanical atomistic calculations based on the Density Functional Tight Binding method. dftbplus/19.1/intel-2018 DFTB+ is a software package for carrying out fast quantum mechanical atomistic calculations based on the Density Functional Tight Binding method. dftbplus/dev/d07f92e/intel-2017 DFTB+ is a quantum mechanical simulation software package, based on the Density Functional Tight Binding (DFTB) method. dl_poly/4.07/intel-2015-update2 Adds DL_POLY 4.07 to your environment dl_poly/4.08-plumed-2.3.1/intel-2017 Adds dl_poly 4.08 to your environment. DL_POLY is a general purpose classical molecular dynamics (MD) simulation software developed at Daresbury Laboratory. This version has been linked against the PLUMED metadynamics library. dl_poly/4.08/intel-2015-update2 Adds DL_POLY 4.08 to your environment. dl_poly/4.09/intel-2018 Adds DL_POLY 4.09 to your environment. dl_poly/classic/1.9/intel-2015-update2 Adds DL_POLY Classic 1.9 to your environment dock/6.9-impi/intel-2018 The DOCK suite of programs is designed to find favorable orientations of a ligand in a receptor. This is the Intel MPI build, intended for high-performance parallel runs. dock/6.9-reference/gnu-4.9.2 The DOCK suite of programs is designed to find favorable orientations of a ligand in a receptor. This is a reference build intended to be close to the version of the software the developers test with: a serial build using the GNU compilers. dos2unix/7.3 Adds dos2unix 7.3 to your environment. Text format converters dos2unix, unix2dos, mac2unix, unix2mac. dssp/3.0.0/gnu-4.9.2 Adds dssp 3.0.0 to your environment. DSSP calculates DSSP entries from Protein Databank (PDB) entries. dymola/2020.1-1 Dymola is a commercial modeling and simulation environment based on the open Modelica modeling language. ea-utils/822 Adds ea-utils to your environment. easylausanne/55c7bf0 Adds Easy Lausanne to your environment. eigensoft/6.1.1/gnu-4.9.2 Adds EIGENSOFT 6.1.1 to your environment. Population genetics methods and EIGENSTRAT stratification correction method. elk/4.0.15/intel-2018 Adds Elk 4.0.15 to your environment. Binary is elk. elk/4.0.15/intel-2018+wa Adds Elk 4.0.15 to your environment. Binary is elk. elk/4.3.6/intel-2017 Adds Elk 4.3.6 to your environment. Binary is elk. elk/4.3.6/intel-2017+wa Adds Elk 4.3.6 to your environment. Binary is elk. elk/5.2.14/intel-2018 Elk is an all-electron full-potential linearised augmented-planewave (FP-LAPW) code. energyplus/8.9.0-bindist EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. energyplus/9.1.0-bindist EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. envi/5.5.2 Adds ENVI 5.5.2 with IDL 8.7.2 to your environment. envi/5.5.3 Adds ENVI 5.5.3 with IDL 8.7.3 to your environment. epacts/3.3.0/gnu-4.9.2 Adds EPACTS 3.3.0 to your environment. examl/8dcf2cc/gnu-4.9.2 Adds ExaML to your environment. exonerate/2.2.0 Adds Exonerate to your environment. fasta/36.3.8d/gnu-4.9.2 Adds the cancer it suite to your environment. fastqc/0.11.5 Adds FastQC 0.11.5 to your environment. A quality control application for high throughput sequence data. fastqc/0.11.8 Adds FastQC 0.11.8 to your environment. A quality control application for high throughput sequence data. ffmpeg/4.1/gnu-4.9.2 FFmpeg is a framework for encoding, decoding, muxing, demuxing, encoding, transcoding, streaming, filtering, and playing many types of audio and video media. fgbio/0.5.1 Adds fgbio to your environment. fgbio is a command line toolkit for working with genomic and particularly next generation sequencing data. fgbio/0.6.1 Adds fgbio to your environment. fgbio is a command line toolkit for working with genomic and particularly next generation sequencing data. figtree/1.4.2 Adds Figtree 1.4.2. foldx/4 Adds FoldX Suite 4 to your environment. freesurfer/5.3.0 Adds FreeSurfer 5.3.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. freesurfer/6.0.0 Adds FreeSurfer 6.0.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. fsl/5.0.9 Adds FSL 5.0.9 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. fsl/5.0.10 Adds FSL 5.0.10 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. fsl/6.0.0 Adds FSL 6.0.0 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. fsl/6.0.0_cuda Adds FSL 6.0.0 CUDA (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. gamess/5Dec2014_R1/intel-2015-update2 Adds GAMESS 5Dec2014_R1 to your environment, built for Intel MPI. Uses ~/Scratch/gamess for USERSCR. You can override by exporting GAMESS_USERSCR as another path. gatk/3.4.46 Adds GATK 3.4.46 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gatk/3.8.0 Adds GATK 3.8.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gatk/4.0.3.0 Adds GATK 4.0.3.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gatk/4.0.8.0 Adds GATK 4.0.8.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gaussian/g09-c01_linda/pgi-2013.9 Adds Gaussian 09 Revision C01 and GaussView 5 to your environment. gaussian/g09-d01/pgi-2015.4 Adds Gaussian G09-D01 to your environment and also includes Linda and Gaussview 5. gaussian/g09-d01/pgi-2015.7 Adds Gaussian G09-D01 to your environment and also includes Linda and Gaussview 5 gaussian/g16-a03/pgi-2016.5 Adds Gaussian G16-A03 to your environment and also includes Linda and Gaussview 6 gdal/2.0.0 Adds GDAL 2.0.0 to your environment variables. Works with Python 2. gdal/2.1.1 adds GDAL 2.1.1 with PROJ.4 4.9.1 to your environment variables. Works with Python 2. gdal/2.1.4 adds GDAL 2.1.4 with PROJ.4 6.1.0 to your environment variables. Works with Python 2. gdal/3.0.4/gnu-4.9.2 adds GDAL 3.0.4 with PROJ.4 6.1.0 to your environment variables. Works with Python 2. gephi/0.9.1 Adds Gephi Version 0.9.1 to your environment. ghostscript/9.16/gnu-4.9.2 Adds Ghostscript 9.16 to your environment. ghostscript/9.19/gnu-4.9.2 Adds Ghostscript 9.19 to your environment. gmsh/2.12.0-bindist Adds gmsh 2.12.0 to your environment. Gmsh is a free 3D finite element grid generator with a build-in CAD engine and post-processor. gmt/5.1.2 adds GMT 5.1.2 to your environment variables gmt/5.3.1 adds GMT 5.3.1 to your environment variables gmt/5.4.5 adds GMT 5.4.5 to your environment variables gnuplot/5.0.1 Adds gnuplot 5.0.1 to your environment. Gnuplot is a portable command-line driven graphing utility. grace/5.1.25 Adds Grace 5.1.25 to your environment. Grace is a 2D plotting tool. graphicsmagick/1.3.21 adds GraphicsMagick 1.3.21 to your environment variables graphviz/2.38.0/gnu-4.9.2 This module adds the Graphviz 2.38.0 package to your environment. Graphviz is open source graph visualization software. graphviz/2.40.1/gnu-4.9.2 This module adds the Graphviz 2.40.1 package to your environment. Graphviz is open source graph visualization software. groff/1.22.3/gnu-4.9.2 Adds GNU groff Version 1.22.3 to your environment. gromacs/5.0.4/intel-2015-update2 Adds GROMACS 5.0.4 to your environment, built using MKL gromacs/5.0.4/plumed/intel-2015-update2 Adds GROMACS 5.0.4 with Plumed 2.1.2 to your environment. Note: Plumed will always run in double precision even if GROMACS is single-precision, so only use that combination if you need it and are aware of the effects. gromacs/5.1.1/intel-2015-update2 Adds GROMACS 5.1.1 to your environment, built using MKL gromacs/5.1.1/plumed/intel-2015-update2 Adds GROMACS 5.1.1 with Plumed 2.2 to your environment. Note: Plumed will always run in double precision even if GROMACS is single-precision, so only use that combination if you need it and are aware of the effects. gromacs/5.1.3/plumed/intel-2015-update2 GROMACS 5.1.3 molecular dynamics package, built with Intel 2015u2 compilers, PLUMED 2.2.3 patches (including libmatheval), and OpenBLAS 0.2.14. gromacs/2016.3/intel-2017-update1 Adds GROMACS 2016.3 to your environment, built using MKL gromacs/2016.3/plumed/intel-2017-update1 GROMACS 2016.3 molecular dynamics package, built with Intel 2017u1 compilers, PLUMED 2.3.1 patches (including libmatheval), and OpenBLAS 0.2.14. gromacs/2016.4/plumed/intel-2017 GROMACS 2016.4 molecular dynamics package, built with Intel 2017u4 compilers, PLUMED 2.4.1 patches (including libmatheval) with hrex, and OpenBLAS 0.2.14. gromacs/2018.2/intel-2018 Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2018.3/intel-2018 Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2018.3/plumed/intel-2018 GROMACS 2018.3 molecular dynamics package, built with Intel 2018u3 compilers, PLUMED 2.4.3 patches (including libmatheval). gromacs/2018/intel-2017 Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2019.3/cuda-10 Adds gromacs 2019 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2019.3/intel-2018 Adds gromacs 2019 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2019.3/plumed/intel-2018 GROMACS 2019.3 molecular dynamics package, built with Intel 2018u3 compilers, PLUMED 2.5.2 patches (including libmatheval). gulp/4.5/intel-2018 Adds GULP 4.5 to your environment. Built with FoX and without plumed. GULP is a materials simulation code. gulp/5.1.1/intel-2018 Adds GULP 5.1.1 to your environment. Built with FoX and without plumed. GULP is a materials simulation code. gurobi/7.5.1 Adds Gurobi 7.5.1 to your environment. gurobi/8.1.1 Adds Gurobi 8.1.1 to your environment. h5utils/1.12.1 Adds h5utils 1.12.1 to your environment. h5utils is a set of utilities for visualization and conversion of scientific data in HDF5 format. hammock/1.0.5 Loads the dependencies for Hammock 1.0.5 to your environment and makes a quick-install alias, do-hammock-install. Run as java -Xmx2g -jar $HAMMOCKPATH/Hammock.jar mode param1 param2 -d outputpath. Will use Scratch for temporary files. hhsuite/3.0-beta.1/gnu-4.9.2 Adds hhsuite 3.0-beta.1 to your environment. hmmer/3.1b2 Adds HMMER 3.1b2 to your environment. hoomd-blue/2.4.2 Adds HOOMD-blue to your environment. hopspack/2.0.2/gnu-4.9.2 Adds HOPSPACK 2.0.2 to your environment hopspack/2.0.2/intel-2017 Adds HOPSPACK 2.0.2 to your environment icommands/4.1.7 [icommands/4.1.7] The iRODS iCommands are the command-line clients to an iRODS system. idl/8.4.1 Adds IDL 8.4.1 to your environment. idl/8.7.3 Adds IDL 8.7.3 to your environment. illustrate/20190807 adds Illustrate to your environment variables impute2/2.3.2 adds Impute2 V2.3.2 to your environment. inetutils/1.9.4 GNU inetutils is a package of utilities for performing a range of network tasks including FTP and telnet clients. intltool/0.51.0 Adds intltool 0.51.0 to your environment. intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. iva/0.11.6 Adds IVA 0.11.6 to your environment. iva/1.0.0 Adds IVA 1.0.0 to your environment. jags/3.4.0/gnu.4.9.2-atlas Adds JAGS 3.4.0 to your environment. jags/3.4.0/gnu.4.9.2-openblas Adds JAGS 3.4.0 to your environment. jags/4.2.0/gnu.4.9.2-openblas Adds JAGS 4.2.0 to your environment. jq/1.5/gnu-4.9.2 adds jq for GCC 4.9.2 to your environment. kallisto/v0.42.5 Adds Kallisto v0.42.5 to your environment. keras/2.2.4 Adds Keras to your environment. kmc/2.1.1/gnu-4.9.2 Adds KMC 2.1.1 to your environment. KMC is a disk-based program for counting k-mers from FASTQ/FASTA files. knitro/12.0.0/gnu-4.9.2 Adds Knitro solver 12.0.0 to your environment. lammps/3Mar20/plumed-colvars/intel-2018 Adds LAMMPS 3Mar20 to your environment. LAMMPS is a GPL molecular dynamics code which shows exceptional scaling on a wide variety of machines. Binary is lmp_mpi or lmp_default. This version was built with packages kspace, manybody, molecule, rigid, lib-linalg, user-colvars and user-plumed. lammps/7Aug19/basic/intel-2018 Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. lammps/7Aug19/gpu/intel-2018 Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. lammps/7Aug19/userintel/intel-2018 Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. lammps/8Dec15/intel-2015-update2 Adds LAMMPS 8Dec15 to your environment. Binary is lmp_default. lammps/10Feb15/intel-2015-update2 Adds LAMMPS 10Feb15 to your environment. Binary is lmp_default. lammps/13Apr17/intel-2017 Adds LAMMPS 13Apr17 to your environment. Binary is lmp_default. lammps/16Mar18/basic/intel-2018 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lammps/16Mar18/gpu/intel-2018 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lammps/16Mar18/intel-2017 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lammps/16Mar18/userintel/intel-2018 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lynx/2.8.9 Adds Lynx Version 2.8.9 to your environment. mathematica/10.1.0 Adds Mathematica 10.1.0 to your environment. mathematica/10.2.0 Adds Mathematica 10.2.0 to your environment. mathematica/10.4.0 Adds Mathematica 10.4.0 to your environment. mathematica/11.0.1 Adds Mathematica 11.0.1 to your environment. mathematica/11.2.0 Adds Mathematica 11.2 to your environment. mathematica/11.3.0 Adds Mathematica 11.3 to your environment. matlab/full/r2015a/8.5 Adds Matlab R2015a for SPM to your environment. matlab/full/r2015b/8.6 Adds Matlab R2015b to your environment. matlab/full/r2016b/9.1 Adds Matlab R2016b to your environment. matlab/full/r2017a/9.2 Adds Matlab R2017a to your environment. matlab/full/r2018a/9.4 Adds Matlab R2018a to your environment. matlab/full/r2018b/9.5 Adds Matlab R2018b to your environment. matlab/full/r2019b/9.7 Adds Matlab R2019b to your environment. mcl/14-137 Adds MCL 14-137 your environment. meep/1.3-ompi/gnu-4.9.2 Adds meep 1.3-ompi to your environment. meep/1.3/gnu-4.9.2 Adds meep 1.3 to your environment. meep/1.11.0-ompi/gnu-4.9.2 Adds meep 1.11.0-ompi to your environment. MEEP is a package for electromagnetics simulation via the finite-diffe rence time-domain (FDTD) method. meme/4.10.1_4 Adds MEME Suite 4.10.1_4 to your environment. The MEME Suite: Motif-based sequence analysis tools. This install is for the command-line tools and connects to their website for further analysis. Web: http://meme-suite.org mgltools/1.5.6 Adds MGLTools 1.5.6 to your environment. Applications for visualization and analysis of molecular structures. Contains AutoDockTools (ADT), Python Molecular Viewer (PMV) and Vision. mirdeep/2.0.0.7 Adds mirdeep 2.0.0.7 to your environment. molden/5.2.2 Adds Molden 5.2.2 to your environment. molpro/2012.1.25/gnu-4.9.2 Adds Molpro to your environment molpro/2015.1.3 Adds Molpro 2015.1.3 binary (no Infiniband support) to your environment. molpro/2015.1.5/intel-2015-update2 Adds Molpro 2015.1.5 built from source with MPI to your environment. mosek/9.1.12 Adds Mosek 9.1.12 to your environment. mothur/1.41.3-bindist Mothur is an expandable, multi-purpose bioinformatics tool aimed at microbial ecology. mpb/1.5-ompi/gnu-4.9.2 Adds mpb 1.5 to your environment. mpb/1.5/gnu-4.9.2 Adds mpb 1.5 to your environment. mpb/1.9.0-hdf5-ompi/gnu-4.9.2 Adds serial mpb 1.9.0 to your environment. Built with HDF5-ompi for use by parallel MEEP. mrbayes/3.2.5/mpi/intel-2015-update2 Adds MrBayes 3.2.5 to your environment mrbayes/3.2.5/serial/intel-2015-update2 Adds MrBayes 3.2.5 to your environment mrtrix/0.3.12/nogui Adds MRtrix3 0.3.12 to your environment. MRtrix3 provides a set of tools to perform analysis of diffusion MRI data, based around the concept of spherical deconvolution and probabilistic tractography. Note: mrview and shview cannot be run over a remote X11 connection so are not usable. mrtrix/0.3.16/gnu-4.9.2/nogui MRtrix provides a set of tools to perform various advanced diffusion MRI analyses, including constrained spherical deconvolution (CSD), probabilistic tractography, track-density imaging, and apparent fibre density. mrtrix/3.0rc3/gnu-4.9.2/nogui Adds MRtrix 3.0RC3 to your environment. mstor/2013/gnu-4.9.2 MSTor is a program for calculating partition functions, free energies, enthalpies, entropies, and heat capacities of complex molecules including torsional anharmonicity. mumax/3.9.3 Adds Mumax 3.9.3 to your environment. mummer/3.23/gnu-4.9.2 Adds MUMmer 3.23 to your environment. MUMmer is a system for rapidly aligning entire genomes, whether in complete or draft form. muscle/3.8.31 Adds MUSCLE 3.8.31 to your environment. mutect/1.1.7 Adds MuTect 1.1.7 to your environment. MuTect is a GATK-based variant caller specialized for somatic/cancer variants. namd/2.10/intel-2015-update2 Adds NAMD 2.10 to your environment namd/2.11/intel-2015-update2 Adds NAMD 2.11 to your environment namd/2.12/intel-2015-update2 Adds NAMD 2.12 to your environment namd/2.12/intel-2017-update1 Adds NAMD 2.12 to your environment namd/2.12/intel-2018-update3 Adds NAMD 2.12 to your environment namd/2.13/intel-2018-update3 Adds NAMD 2.13 to your environment namd/2.13/plumed/intel-2018-update3 Adds NAMD 2.13 to your environment nco/4.5.0 Adds nco to your environment. nektar++/4.3.5-impi/intel-2017-update1 Adds Nektar++ Version 4.3.5 to your environment nektar++/4.3.5-ompi/gnu-4.9.2 Adds Nektar++ Version 4.3.5 to your environment ngsutils/0.5.9 Adds a set of python scripts for handling various NGS tasks to your environment. nighres/1.1.0b Adds Nighres to your environment. nonmem/7.3.0/gnu-4.9.2 Adds NONMEM 7.3.0 using GCC Fortran 4.9.2 to your environment. nonmem/7.3.0/intel-2015-update2 Adds NONMEM 7.3.0 using Intel Fortran 2015 to your environment. novocraft/3.04.06 Adds novocraft 3.04.06 to your environment. Novocraft is a set of tools for bioinformatics, including Novoalign for short-read mapping. nwchem/6.5-r26243/atlas/intel-2015-update2 Adds NWChem 6.5 revision 26243 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and ATLAS. Global .nwchemrc: /shared/ucl/apps/nwchem/6.5-r26243-atlas/intel-2015-update2.nwchemrc nwchem/6.5-r26243/intel-2015-update2 Adds NWChem 6.5 revision 26243 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.5-r26243/intel-2015-update2/.nwchemrc nwchem/6.6-r27746/intel-2015-update2 Adds NWChem 6.6 revision 27746 patched 2016-01-20 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.6-r27746/intel-2015-update2/.nwchemrc nwchem/6.6-r27746/intel-2017 Adds NWChem 6.6 revision 27746 patched 2016-01-20 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.6-r27746/intel-2017/.nwchemrc nwchem/6.8-47-gdf6c956/intel-2017 Adds NWChem 6.8 47-gdf6c956 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.8-47-gdf6c956/intel-2017/.nwchemrc oasislmf/1.2.4 Oasis LMF 1.2.4 oasislmf/ktools/3.0.3/gnu-4.9.2 OasisLMF ktools package built with the GNU compilers oasislmf/ktools/f92a41f/gnu-4.9.2 OasisLMF ktools package built with the GNU compilers octave/4.4.1 Octave is an open source competitor to Matlab which is mostly compatible with Matlab. octopus/4.1.2-impi/intel-2015-update2 Adds octopus 4.1.2 to your environment. octopus/4.1.2/intel-2015-update2 Adds octopus 4.1.2 to your environment. octopus/5.0.1-ompi/gnu-4.9.2 Adds octopus 5.0.1 to your environment. octopus/5.0.1/gnu-4.9.2 Adds octopus 5.0.1 to your environment. octopus/6.0-ompi/gnu-4.9.2 Adds octopus 6.0 to your environment. octopus/6.0/gnu-4.9.2 Adds octopus 6.0 to your environment. openbabel/2.4.1/gnu-4.9.2 OpenBabel is a library and command-line tool for manipulating and converting between various chemistry file formats. opencv/2.4.13/gnu-4.9.2 Adds OpenCV 2.4.13 to your environment. Open Source Computer Vision Library. opencv/3.4.1/gnu-4.9.2 Adds OpenCV 3.4.1 to your environment. Open Source Computer Vision Library. openfoam/2.3.1/intel-2015-update2 Adds OpenFOAM 2.3.1 to your environment openfoam/2.4.0/intel-2017-update1 Adds OpenFOAM 2.4.0 to your environment openfoamplus/v1706/gnu-4.9.2 Adds OpenFOAMplus v1706 to your environment openfoamplus/v1906/gnu-7.3.0 Adds OpenFOAMplus v1906 to your environment openmm/7.3.1/cuda-10 Adds OpenMM to your environment. openmm/7.3.1/gnu-4.9.2 Adds OpenMM to your environment. openmx/3.8.3 Adds OpenMX 3.8.3 to your environment. optimet/1.0.1/gnu-4.9.2 Adds Optimet to your environment. p7zip/15.09/gnu-4.9.2 Adds p7zip 15.09 to your environment. To expand 7z files: 7za x archive.7z pandoc/1.19.2.1 Adds pandoc Version 1.19.2.1 to your environment. parallel/20181122 GNU parallel is a shell tool for executing jobs in parallel using one or more computers. paraview/5.3.0 This module adds the ParaView 5.3.0 binaries to your environment. ParaView is an open-source, multi-platform data analysis and visualization application. parmed/3.2.0 Adds ParmEd to your environment. petsc/3.12.1/gnu-4.9.2 Adds Petsc 3.12.1 to your environment phon/1.39/gnu-4.9.2 Adds Phon 1.3.9 with addons to your PATH. phon/1.43/gnu-4.9.2 Adds Phon 1.43 with addons to your PATH. picard-tools/1.136 Adds Picard Tools 1.136 to your environment. If using the java -jar command, you should pass TMP_DIR=$TMPDIR to Picard. picard-tools/2.18.9 Adds Picard Tools to your environment. If using the java -jar command, you should pass TMP_DIR=$TMPDIR to Picard. platypus/3e72641 Adds Platypus to your environment. plink/1.07 Adds Plink 1.07 with addons to your PATH. plink/1.90b3.40 Adds PLINK 1.90b3.40 to your environment. A comprehensive update to the PLINK association analysis toolset. plink/2.0alpha-git Adds PLINK 2.0 alpha to your environment. A comprehensive update to the PLINK association analysis toolset. plumed/2.1.2/intel-2015-update2 Adds PLUMED 2.1.2 to your environment, built using OpenBLAS plumed/2.2.3/intel-2015-update2 Adds PLUMED 2.2.3 to your environment, built using OpenBLAS and libmatheval plumed/2.2/intel-2015-update2 Adds PLUMED 2.2 to your environment, built using OpenBLAS plumed/2.3.1/intel-2017-update1 Adds PLUMED 2.3.1 to your environment, built using OpenBLAS and libmatheval plumed/2.4.1/gnu-4.9.2 Adds PLUMED 2.4.1 to your environment, built using OpenBLAS and libmatheval plumed/2.4.1/intel-2017-update4 Adds PLUMED 2.4.1 to your environment, built using OpenBLAS and libmatheval plumed/2.4.3/intel-2018 Adds PLUMED 2.4.3 to your environment, built using MKL and libmatheval plumed/2.5.2/intel-2018 Adds PLUMED 2.5.2 to your environment, built using MKL and libmatheval plumed/2.6.0/intel-2018 Adds PLUMED 2.6.0 to your environment, built using MKL and libmatheval postgres+postgis/9.5.3+2.2.2/gnu-4.9.2 Adds postgres+postgis 9.5.3+2.2.2 to your environment. PostgreSQL is a relational database, and PostGIS is a geographical information enhancement for PostgreSQL. postgresql/9.5.3/gnu-4.9.2 Adds postgresql 9.5.3 to your environment. PostgreSQL is a relational database. primer3/2.3.6 This module adds the primer3 package to your environment. probabel/0.4.5/gnu-4.9.2 Adds ProbABEL to your environment. proj.4/4.9.1 Adds the PROJ.4 Cartographic Projections library to your environment. proj.4/5.2.0 Adds the PROJ.4 Cartographic Projections library to your environment. proj.4/6.0.0 Adds the PROJ.4 Cartographic Projections library to your environment. proj.4/6.1.0 Adds the PROJ.4 Cartographic Projections library to your environment. proovread/2.13.11-8Jan2016-f6a856a Adds proovread 2.13.11-8Jan2016-f6a856a to your environment. f6a856a is the commit for this version. pymol/1.7.7.2 Adds PyMol to your environment. pymol/1.8.2.1 Adds PyMol to your environment. pyrosetta/release-73 Adds PyRosetta to your environment. pytorch/1.2.0/cpu Adds PyTorch 1.2.0 to your environment. pytorch/1.2.0/gpu Adds PyTorch 1.2.0 to your environment. qctool/2/beta/ba5eaa44a62f This module adds qctool v2 beta to your environment. quantum-espresso/5.2.0-impi/intel-2015-update2 Adds quantum-espresso 5.2.0 to your environment. quantum-espresso/6.1-impi/intel2017 Adds quantum-espresso 6.1 to your environment. quantum-espresso/6.3-impi/thermo_pw-1.0.9/intel-2018 Adds quantum-espresso 6.3 + thermo_pw 1.0.9 to your environment. quantum-espresso/6.4.1-impi/intel-2018 Adds quantum-espresso 6.4.1 to your environment. quantum-espresso/6.5-impi/intel-2018 Adds quantum-espresso 6.5 to your environment. quantum-espresso/6.5-impi/thermo_pw-1.2.1/intel-2018 Adds quantum-espresso 6.5 + thermo_pw 1.2.1 to your environment. r/3.2.0-atlas/gnu-4.9.2 Adds R 3.2.0 and Bioconductor 3.2 to your environment. r/3.2.2-openblas/gnu-4.9.2 Adds R 3.2.2 and Bioconductor 3.2 to your environment. r/3.3.0-openblas/gnu-4.9.2 Adds R 3.3.0 and Bioconductor 3.3 to your environment. r/3.3.2-openblas/gnu-4.9.2 Adds R 3.3.2 and Bioconductor 3.4 to your environment. r/3.4.0-openblas/gnu-4.9.2 Adds R 3.4.0 and Bioconductor 3.5 to your environment. r/3.4.2-openblas/gnu-4.9.2 Adds R 3.4.2 and Bioconductor 3.6 to your environment. r/3.5.0-openblas/gnu-4.9.2 Adds R 3.5.0 and Bioconductor 3.7 to your environment. r/3.5.1-openblas/gnu-4.9.2 Adds R 3.5.1 and Bioconductor 3.7 to your environment. r/3.5.3-openblas/gnu-4.9.2 Adds R 3.5.3 and Bioconductor 3.8 to your environment. r/3.6.0-openblas/gnu-4.9.2 Adds R 3.6.0 and Bioconductor 3.9 to your environment. randfold/2.0/gnu-4.9.2 Adds randfold 2.0 to your environment. rclone/1.51.0 RClone is a command-line program intended to download and upload files from and to various storage services and providers. repast-hpc/2.1/gnu-4.9.2 Adds Repast HPC 2.1 compiled with GCC 4.9.2 and OpenMPI to your environment. root/5.34.30/gnu-4.9.2 Adds ROOT 5.34.30 to your environment. root/5.34.30/gnu-4.9.2-fftw-3.3.6 Adds ROOT 5.34.30 to your environment. root/5.34.36/gnu-4.9.2-fftw-3.3.6 Adds ROOT 5.34.36 to your environment. root/5.34.36/gnu-4.9.2-fftw-3.3.6-gsl-2.4 Adds ROOT 5.34.36 to your environment. root/6.04.00/gnu-4.9.2 Adds ROOT 6.04.00 to your environment. rosetta/2015.31.58019 Adds Rosetta 2015.31.58019 to your environment. rosetta/2015.31.58019-mpi Adds Rosetta 2015.31.58019 with MPI to your environment. rosetta/2018.48.60516 Adds Rosetta 2018.48.60516 serial version to your environment. rosetta/2018.48.60516-mpi Adds Rosetta 2018.48.60516 MPI version to your environment. rsem/1.2.31 Adds RSEM 1.2.31 to your environment. sac/101.6a Adds SAC 101.6a to your environment. sambamba/0.6.7-bindist A tool for extracting information from SAM/BAM files. samblaster/0.1.24/gnu-4.9.2 samblaster is a program for marking duplicates in read-id grouped paired-end SAM files. samsrf/5.84/matlab.r2019b Adds the SamSrf Matlab toolbox to your environment samtools/0.1.19 This module adds the Samtools 0.1.19 package to your environment. samtools/1.2/gnu-4.9.2 Adds SAMtools 1.2 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. samtools/1.3.1/gnu-4.9.2 Adds SAMtools 1.3.1 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. samtools/1.9/gnu-4.9.2 Adds SAMtools 1.9 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. sas/9.4-M6/64 Adds SAS 9.4 (9.04.01M6) 64 bit to your environment sas/9.4/64 Adds SAS 9.4 64 bit to your environment sc/7.16 Adds sc 7.16 to your environment. siesta/4.0.1/intel-2017 Adds SIESTA 4.0.1 to your environment. skewer/0.2.2 Adds skewer 0.2.2 to your environment. smalt/0.7.6/gnu-4.9.2 Adds SMALT 0.7.6 to your environment. SMALT aligns DNA sequencing reads with a reference genome. Compiled with bambamc support for SAM/BAM input and BAM output. snptest/2.5.4-beta3 Adds SNPtest 2.5.4-beta3 to your environment. sod/3.2.7 Adds SOD 3.2.7 to your environment. SOD is a program that automates tedious data selection, downloading, and routine processing tasks in seismology. spm/8/r6313/matlab.r2015a Adds SPM8 to your environment spm/12/jan2020/matlab.r2019b Adds SPM12 to your environment spm/12/r6470/matlab.r2015a Adds SPM12 to your environment spss/24 Adds SPSS 24 to your environment spss/25 Adds SPSS 25 to your environment spss/26 Adds SPSS 26 to your environment sqlite/3.31.1/gnu-9.2.0 Adds SQLite Version 3.31.1 to your environment. star-ccm+/9.06.011 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/11.04.010-R8 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/12.04.010 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/13.02.011 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/13.06.012 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/14.06.013 Adds STAR-CCM+ and STAR-View to your environment. star-cd/4.22.058 Adds STAR-CD 4.22.058 to your environment. star-cd/4.26.011 Adds STAR-CD 4.26.011 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. star-cd/4.26.022 Adds STAR-CD 4.26.022 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. star-cd/4.28.050 Adds STAR-CD 4.28.050 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. star/2.5.2a Adds STAR 2.5.2a to your environment. star/2.7.3a Adds STAR 2.7.3a to your environment. stata/14 Adds Stata/MP 14 to your environment. stata/15 Adds Stata/MP 15 to your environment. supermagic/1.2/intel-2017 Adds supermagic 1.2 to your environment. Supermagic is a simple MPI sanity test. taup/2.1.2 adds TauP 2.1.2 to your environment variables tensorflow/1.4.1/cpu Adds Tensorflow 1.4.1 to your environment. tensorflow/1.4.1/gpu Adds Tensorflow 1.4.1 to your environment. tensorflow/1.4.1/mkl Adds Tensorflow 1.4.1 to your environment. tensorflow/1.8.0/cpu Adds Tensorflow 1.8.0 to your environment. tensorflow/1.8.0/gpu Adds Tensorflow 1.8.0 to your environment. tensorflow/1.8.0/mkl Adds Tensorflow 1.8.0 to your environment. tensorflow/1.12.0/cpu Adds Tensorflow 1.12.0 to your environment. tensorflow/1.12.0/gpu Adds Tensorflow 1.12.0 to your environment. tensorflow/1.12.0/mkl Adds Tensorflow 1.12.0 to your environment. tensorflow/1.13.1/cpu Adds Tensorflow 1.13.1 to your environment. tensorflow/1.13.1/gpu Adds Tensorflow 1.13.1 to your environment. tensorflow/1.13.1/mkl Adds Tensorflow 1.13.1 to your environment. tensorflow/1.14.0/cpu Adds Tensorflow 1.14.0 to your environment. tensorflow/1.14.0/gpu Adds Tensorflow 1.14.0 to your environment. tensorflow/1.14.0/mkl Adds Tensorflow 1.14.0 to your environment. tensorflow/2.0.0/gpu-py37 Adds Tensorflow 2.0.0 to your environment. tensorflow/2.0.0/gpu-py37-cudnn75 Adds Tensorflow 2.0.0 to your environment. tensorflow/2.0.0/mkl-py37 Adds Tensorflow 2.0.0 to your environment. tephra2/2.0/gnu-4.9.2 Adds Tephra2 version 2.0 to your environment. tephra2/normal/r149 Adds Tephra2 version r149 to your environment. tesseract/3.05.01 Adds Tesseract 3.05.01 to your environment. texinfo/5.2/gnu-4.9.2 Adds GNU texinfo 5.2 to your environment. texinfo/6.6/gnu-4.9.2 Adds GNU texinfo 6.6 to your environment. texlive/2014 Adds TeX Live 2014 to your environment. texlive/2015 Adds TeX Live 2015 to your environment. texlive/2019 Adds TeX Live 2019 to your environment. textract/1.5.0 Adds textract 1.5.0 to your environment. textract extracts text from a wide range of document types. tmux/2.2 This module adds the tmux 2.2 package to your environment. tophat/2.1.0 Adds Tophat 2.1.0 to your environment. tracer/1.6 Adds Tracer 1.6. tractor/3.2.5 Adds TractoR 3.2.5 to your environment. tree/1.7.0 Adds tree 1.7.0 to your environment. This shows your directory structure as a tree. trim_galore/0.4.1 Adds Trim Galore 0.4.1 to your environment. A wrapper tool around Cutadapt and FastQC to consistently apply quality and adapter trimming to FastQ files. trimmomatic/0.33 Adds Trimmomatic 0.33 to your environment. A flexible read trimming tool for Illumina NGS data. turbomole/6.4/mpi Adds turbomole 6.4 (using MPI) to your environment. turbomole/6.4/serial Adds turbomole 6.4 (serial) to your environment. turbomole/6.4/smp Adds turbomole 6.4 (using SMP) to your environment. turbomole/6.5/mpi Adds turbomole 6.5 (using MPI) to your environment. turbomole/6.5/serial Adds turbomole 6.5 (serial) to your environment. turbomole/6.5/smp Adds turbomole 6.5 (using SMP) to your environment. turbomole/6.6/mpi Adds turbomole 6.6 (using MPI) to your environment. turbomole/6.6/serial Adds turbomole 6.6 (serial) to your environment. turbomole/6.6/smp Adds turbomole 6.6 (using SMP) to your environment. ubpred/1-bin32dist UbPred is a random forest-based predictor of potential ubiquitination sites in proteins. udunits/2.2.19 Adds udunits to your environment. udunits/2.2.20/gnu-4.9.2 adds the UDUNITS-2 package to your environment. udunits/2.2.26/gnu-4.9.2 adds the UDUNITS-2 package to your environment. varscan/2.3.9 Adds VarScan v2.3.9 to your environment. VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. vasp/5.4.1-05feb16-p2/intel-2015-update2 The VASP Quantum Chemistry package, version 5.4.1-05feb16 with patches 1 and 2. vasp/5.4.1-24jun15-p2-vtst-r160/intel-2015-update2 Adds VASP 5.4.1 built with VTST r160 to your environment. vasp/5.4.1-24jun15-p08072015/intel-2015-update2 The VASP Quantum Chemistry package, version 5.4.1-24jun15 with patch 08072015. vasp/5.4.4-18apr2017-libbeef/intel-2017-update1 Adds VASP 5.4.4 with BEEF-vdW functionals to your environment. vasp/5.4.4-18apr2017-vtst-r178/intel-2017-update1 Adds VASP 5.4.4 built with VTST r178 to your environment. vasp/5.4.4-18apr2017/intel-2017-update1 Adds VASP 5.4.4 to your environment. vcftools/0.1.15/gnu-4.9.2 Adds VCFtools version 0.1.15 to your environment. Tools for working with VCF files. velvet/1.2.10 Adds Velvet 1.2.10 to your environment. vep/95.0 Adds VEP 95.0 to your environment. vesta/3.4.6-bindist VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. vg/1.11.0 This is a module with no description string. viennarna/2.1.9/gnu-4.9.2 Adds viennarna 2.1.9 to your environment. vinalc/1.1.2/gnu-4.9.2 Adds VinaLC to your environment. visit/2.9.2 This package adds VisIt 2.9.2 to your environment. VisIt is a distributed, parallel visualization and graphical analysis tool for data defined on two- and three-dimensional (2D and 3D) meshes. Visit will create a ~/.visit directory and a ~/Scratch/visit directory. Jobfiles created by the GUI will go in the latter. Legion hostfile: /shared/ucl/apps/visit/2.9.2/gnu-4.9.2/host_legion.xml Web: https://wci.llnl.gov/simulation/computer-codes/visit/ vmd/1.9.3/GL+CUDA VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. vmd/1.9.3/text-only The binary, text only version of VMD 1.9.3 vt/2018-08-01/gnu-4.9.2 [ref:f6d2b5dab73c] A tool set for short variant discovery in genetic sequence data. xmds/2.2.2 Adds XMDS 2.2.2 (GNU/ATLAS/Intel MPI/FFTW toolchain) to your environment. xtalopt/r12.1/gnu-4.9.2 Adds XtalOpt r12.1 to your environment. xulrunner/3.6.28/gnu-4.9.2 Adds XULRunner 3.6.28 to your environment. XULRunner is a Mozilla runtime package that can be used to bootstrap XUL+XPCOM applications. This version was built including javaxpcom. xulrunner/10.0.2 Adds the XULRunner 3.6.28 64-bit runtime binaries to your environment. XULRunner is a Mozilla runtime package that can be used to bootstrap XUL+XPCOM applications. yambo/4.1.4/intel-2017 This is a module with no description string. Libraries \u00a7 Modules in this section set up your environment to use specific C, C++, or Fortran libraries. This can include being able to use them with other languages, like Python. Module Description apr-util/1.5.4 adds APR-util 1.5.4 to your environment variables apr/1.5.2 adds APR 1.5.2 to your environment variables argtable/2.13 Adds argtable 2.13 to your environment. armadillo/7.400.3/intel-2015-update2 Adds armadillo 7.400.3 to your environment. Armadillo is a linear alebra library for C++, aiming to balance speed and ease of use. arpack-ng/3.4.0/intel-2015-update2 Adds arpack-ng 3.4.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. arpack-ng/3.5.0/gnu-4.9.2-serial Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. arpack-ng/3.5.0/gnu-4.9.2-threaded Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. arpack-ng/3.5.0/intel-2017 Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. atlas/3.10.2/gnu-4.9.2 adds ATLAS 3.10.2 for GCC 4.9.2 compilers to your environment variables atlas/3.10.2/intel-2015-update2 adds ATLAS 3.10.2 for Intel 15 compilers to your environment variables bambamc/0.0.50/gnu-4.9.2 Adds bambamc 0.0.50 to your environment. bambamc is a lightweight C implementation of the read name collation code from the larger libmaus/biobambam C++ project. boost/1_54_0/gnu-4.9.2 Adds Boost 1.54.0 with Python libraries to your environment. boost/1_54_0/mpi/gnu-4.9.2 Adds Boost 1.54.0 with Python and MPI libraries to your environment. boost/1_54_0/mpi/gnu-4.9.2-ompi-1.10.1 Adds Boost 1.54.0 with Python and MPI libraries to your environment. boost/1_54_0/mpi/intel-2015-update2 Adds Boost 1.54.0 with Python and MPI libraries to your environment. boost/1_63_0/gnu-4.9.2 Adds Boost 1.63.0 with Python libraries to your environment. boost/1_63_0/mpi/gnu-4.9.2 Adds Boost 1.63.0 with Python and MPI libraries to your environment. boost/1_63_0/mpi/intel-2017-update1 Adds Boost 1.63.0 with Python and Intel MPI libraries to your environment. cernlib/2006-35 Adds the CERN Program library to your environment. 2006-35 EL6 RPM binaries. cernlib/2006/gnu-4.9.2 Adds the CERN Program library to your environment cfitsio/3370/gnu-4.9.2 Adds cfitsio 3370 to your environment. cfitsio/3370/intel-2015-update2 Adds cfitsio 3370 to your environment. cgal/4.9/gnu-4.9.2 Adds CGAL 4.9 with Qt5 to your environment. The Computational Geometry Algorithms Library. clusteringsuite/2.6.6/bindist Adds clusteringsuite 2.6.6 to your environment. Clustering Suite is a set of tools to automatically expose the main performance trends in applications' computation structure. cunit/2.1-3/gnu-4.9.2 Adds cunit 2.1-3 to your environment. CUnit is a package for writing and running unit tests in C. cvmfs/2.2.1/gnu-4.9.2 Adds libcvmfs 2.2.1 to your environment. dyninst/9.3.2/gnu-4.9.2 Adds dyninst 9.3.2 to your environment. DynInst is a library for performing dynamic instrumentation of executables. eigen/3.2.5/gnu-4.9.2 adds Eigen for GCC 4.9.2 compilers to your environment variables elfutils/0.170/gnu-4.9.2 Adds elfutils 0.170 to your environment. Elfutils provides utilities for manipulating binary ELF files, and is one possible provider of libelf. fftw/2.1.5/gnu-4.9.2 adds FFTW 2.1.5 for GCC 4.9.2 compilers to your environment variables fftw/2.1.5/intel-2015-update2 adds FFTW 2.1.5 for Intel compilers to your environment variables fftw/3.3.4-impi/gnu-4.9.2 Adds fftw 3.3.4 (built with Intel MPI) to your environment. fftw/3.3.4-impi/intel-2017-update1 Adds fftw 3.3.4 (built with Intel MPI) to your environment. fftw/3.3.4-ompi-1.10.1/gnu-4.9.2 Adds fftw 3.3.4 (built with OpenMPI) to your environment. fftw/3.3.4-ompi/gnu-4.9.2 Adds fftw 3.3.4 (built with OpenMPI) to your environment. fftw/3.3.4-threads/gnu-4.9.2 adds FFTW 3.3.4 for GCC 4.9.2 compilers to your environment variables fftw/3.3.4/gnu-4.9.2 adds FFTW 3.3.4 for GCC 4.9.2 compilers to your environment variables fftw/3.3.4/intel-2015-update2 adds FFTW 3.3.4 for Intel compilers to your environment variables fftw/3.3.6-pl2/gnu-4.9.2 Adds FFTW 3.3.6 pl2 for GCC 4.9.2 compilers to your environment variables. Includes single and double precision only on Legion, plus long-double and quad on Grace/Thomas. Includes OpenMP and POSIX threads libraries. fftw/3.3.6-pl2/intel-2017 Adds FFTW 3.3.6 pl2 for Intel compilers to your environment variables. Includes single and double precision versions on Legion, plus long-double on Grace/Thomas. Includes OpenMP and POSIX threads libraries. fftw/3.3.8-impi/intel-2018 Adds fftw fftw (built with Intel MPI) to your environment. fftw/3.3.8-ompi/gnu-4.9.2 Adds fftw 3.3.8 (built with OpenMPI) to your environment. forge/1.0.0/gnu-4.9.2 Adds forge 1.0.0 to your environment. freeimage/3.17.0/gnu-4.9.2 Adds FreeImage 3.17.0 to your environment. freetype/2.8.1/gnu-4.9.2 Adds freetype 2.8.1 to your environment. FreeType is a freely available software library to render fonts. ga/5.7-8BInts/intel-2018 Global Arrays (GA) is a library that provides a Partitioned Global Address Space (PGAS) programming model. This version has been compiled with 8-byte integers in the Fortran code. ga/5.7/intel-2018 Global Arrays (GA) is a library that provides a Partitioned Global Address Space (PGAS) programming model. gcc-libs/4.9.2 adds GCC 4.9.2 runtime to your evironment. geos/3.5.0/gnu-4.9.2 Adds geos 3.5.0 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. gflags/2.2.1 Adds Google gflags 2.2.1 to your environment. giflib/5.1.1 Adds giflib 5.1.1 to your environment. A library and utilities for processing gifs. glbinding/2.1.2/gnu-4.9.2 Adds glbinding 2.1.2 to your environment. glew/1.13.0/gnu-4.9.2 Adds GLEW The OpenGL Extension Wrangler Library 1.11.0 to your environment. glfw/3.2.1/gnu-4.9.2 Adds GLFW 3.2.1 to your environment. glog/0.3.5 Adds Google glog 0.3.5 to your environment. glpk/4.60/gnu-4.9.2 Adds the GNU Linear Programming Kit Version 4.60 for GCC 4.9.2 to your environment. gsl/1.16/gnu-4.9.2 adds GSL 1.16 for GCC 4.9.2 to your environment. gsl/1.16/intel-2015-update2 Adds gsl 1.16 to your environment. gsl/2.4/gnu-4.9.2 adds GSL 2.4 for GCC 4.9.2 to your environment. gsl/2.4/intel-2017 adds GSL 2.4 for Intel 2017 to your environment. gstreamer/1.12.0 GStreamer is a library for constructing graphs of media-handling components, including codecs for various audio and video formats. gulp/4.5/libgulp/intel-2018 Adds GULP 4.5 library version to your environment. Built libgulp only, without FoX, for programs such as ChemShell to link. GULP is a materials simulation code. h5py/2.10.0-ompi/gnu-4.9.2 Adds h5py 2.10.0-ompi for Python 3.7 to your environment. harminv/1.4.1/gnu-4.9.2 Adds harminv 1.4.1 to your environment. harminv/1.4/gnu-4.9.2 Adds harminv 1.4 to your environment. hdf/5-1.8.15-p1-impi/intel-2015-update2 Adds hdf5 1.8.5-p1 (built with Fortran and IntelMPI options) to your environment. hdf/5-1.8.15-p1-ompi/gnu-4.9.2 Adds hdf5 1.8.5-p1 (built with Fortran and OpenMPI options) to your environment. hdf/5-1.8.15/gnu-4.9.2 adds HDF5 1.8.15 (Serial) for GCC 4.9.2 to your environment. hdf/5-1.8.15/intel-2015-update2 adds HDF5 1.8.15 (Serial) for Intel 2015 to your environment. hdf/5-1.10.2-impi/intel-2018 adds HDF5 1.10.2 (Parallel) for Intel 2018 to your environment. hdf/5-1.10.2/intel-2018 adds HDF5 1.10.2 (Serial) for Intel 2018 to your environment. hdf/5-1.10.5-ompi/gnu-4.9.2 Adds hdf 5-1.10.5-ompi to your environment. Built with OpenMPI and GNU. hdf/5-1.10.5/gnu-4.9.2 Adds hdf 5-1.10.5 to your environment. Serial version built with GNU. htslib/1.2.1 This module adds the HTSlib 1.2.1 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. htslib/1.3.1 This module adds the HTSlib 1.3.1 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. htslib/1.7 This module adds the HTSlib 1.7 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. hwloc/1.11.12 The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. This installation includes the optional libnuma dependency. hypre/2.11.2/openmpi-3.0.0/intel-2017 Adds HYPRE 2.11.2 to your environment. hypre/2.11.2/openmpi-3.1.1/intel-2018 Adds HYPRE 2.11.2 to your environment. jansson/2.11 This is a module with no description string. json-c/0.12/gnu-4.9.2 Adds json-c 0.12 to your environment. JSON-C is a library for converting between JSON-formatted strings and C representations of the equivalent objects. lapack/3.8.0/gnu-4.9.2 LAPACK is a reference library of routines for Linear Algebra. It is not recommended for use, as its ABI is replicated in the much higher-performance libraries OpenBLAS, MKL, or ATLAS instead. leptonica/1.74.4 Adds Leptonica 1.74.4 to your environment. leveldb/1.20 Adds Google leveldb 1.20 to your environment. libbdwgc/7.4.2/gnu-4.9.2 Adds libbdwgc (a garbage-collector library) to your environment. libbeef/0.1.3/intel-2018 Library for Bayesian error estimation functionals for use in density functional theory codes: libbeef 0.1.3 commit 2822afe libctl/3.2.2/gnu.4.9.2 Adds libctl (built using Intel compilers) to your environment. libctl/4.3.0/gnu-4.9.2 Adds libctl 4.3.0 to your environment. libdwarf/20170709/gnu-4.9.2 Adds libdwarf 20170709 to your environment. libdwarf is a library for interacting with debugging info in the DWARF 2, 3, and 5 formats. libelf/0.8.13/gnu-4.9.2 Adds libelf 0.8.13 to your environment. libetsfio/1.0.4/gnu-4.9.2 Adds libetsfio 1.0.4 to your environment. libetsfio/1.0.4/intel-2015-update2 Adds libetsfio 1.0.4 to your environment. libflac/1.3.1/gnu-4.9.2 Adds libflac 1.3.1 to your environment. libFLAC is the Xiph library for handling their lossless audio codec. libgd/2.1.1/gnu-4.9.2 Adds libgd 2.1.1 to your environment. libgd/2.1.1/intel-2015-update2 Adds libgd 2.1.1 to your environment. libgdsii/0.21/gnu-4.9.2 Adds libgdsii 0.21 to your environment. C++ library and command-line utility for reading GDSII geometry files. libint/1.1.4/gnu-4.9.2 Adds libint 1.1.4 to your environment. Libint is required for CP2K. libmatheval/1.1.11 Adds libmatheval 1.1.11 to your environment. GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libsodium/1.0.6/gnu-4.9.2 Adds libsodium 1.0.6 to your environment. libsodium is a crypto library primarily used by ZeroMQ. libsox/14.4.2/gnu-4.9.2 Adds libsox 14.4.2 to your environment. SoX is a library for reading, writing, and converting a variety of sound file formats. If you require support for a file format that is not installed, contact rc-support and the library can be rebuilt. libuuid/1.0.3/gnu-4.9.2 Adds a static libuuid 1.0.3 to your environment. libxc/2.1.2/intel-2015-update2 Adds libxc 2.1.2 to your environment. libxc/2.2.2/gnu-4.9.2 Adds libxc 2.2.2 to your environment. libxc/2.2.2/intel-2015-update2 Adds libxc 2.2.2 to your environment. libxc/3.0.0/gnu-4.9.2 Adds libxc 3.0.0 to your environment. libxc/3.0.0/intel-2015-update2 Adds libxc 3.0.0 to your environment. libxc/4.2.3/intel-2018 libxc is a library of routines implementing a range of exchange-correlation functionals for density-functional theory calculations. libxml2/2.9.4/gnu-4.9.2 Adds libxml2 2.9.4 to your environment. Libxml2 is an XML C parser and toolkit. Includes Python (2.7.9) bindings. llvm/3.3 This module adds the LLVM 3.3 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. llvm/3.9.1 This module adds the LLVM 3.9.1 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. llvm/6.0.1 This module adds the LLVM 6.0.1 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. llvm/8.0.0/gnu-4.9.2 The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. This installation includes clang, a C compiler based on LLVM. lmdb/0.9.22 Adds LMDB 0.9.22 to your environment. lz4/1.8.3 This is a module with no description string. magma/2.4.0 This is a module with no description string. med/4.0.0/gnu-4.9.2 Adds med 4.0.0 to your environment. Allows reading and writing of MED format files. mesa/6.5/gnu-4.9.2 Adds mesa 6.5 to your environment. Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. This is an old version installed to satisfy a particular dependency: please do not use for new builds. mesa/10.6.3 Adds Mesa 10.6.3 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built for offscreen rendering: OSMesa, Xlib GLX, no Gallium, no EGL, no llvm, no DRI). mesa/10.6.9/gnu-4.9.2 Adds Mesa 10.6.9 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built for offscreen rendering: OSMesa, Xlib GLX, no Gallium, no EGL, no llvm, no DRI). mesa/13.0.6/gnu-4.9.2 Adds Mesa 13.0.6 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built options: Gallium, LLVM, no EGL, no DRI, no GLX). The default driver is llvmpipe. You can use \"export GALLIUM_DRIVER\" to explicitly choose llvmpipe, softpipe, or swr metis/5.1.0/gnu-4.9.2 Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. metis/5.1.0/intel-2015-update2 Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. mpi/intel/2013/update1/intel adds Intel MPI 4.1.3.048 to your environment variables mpi/intel/2015/update3/gnu-4.9.2 adds Intel MPI to your environment variables mpi/intel/2015/update3/intel adds Intel MPI to your environment variables mpi/intel/2017/update1/gnu-4.9.2 adds Intel MPI to your environment variables mpi/intel/2017/update1/intel adds Intel MPI to your environment variables mpi/intel/2017/update2/gnu-4.9.2 adds Intel MPI to your environment variables configured to use GCC 4.9.2 mpi/intel/2017/update2/intel adds Intel MPI to your environment variables mpi/intel/2017/update3/gnu-4.9.2 [Intel MPI/2017.3.196] This is Intel's MPI implementation, version 2017.3.196, which is bundled with compiler package version 2017.Update4. This module sets up the compiler wrappers to use GCC 4.9.2 underneath. mpi/intel/2017/update3/intel [Intel MPI/2017.3.196] This is Intel's MPI implementation, version 2017.3.196, which is bundled with compiler package version 2017.Update4. mpi/intel/2018/update3/intel [Intel MPI/2018.3.222] This is Intel's MPI implementation, version 2018.3.222, which is bundled with compiler package version 2018.Update3. mpi/intel/2019/update4/intel [Intel MPI/2019.4.243] This is Intel's MPI implementation, version 2019.4.243, which is bundled with compiler package version 2019.Update4. mpi/intel/2019/update5/intel [Intel MPI/2019.5.281] This is Intel's MPI implementation, version 2019.5.281, which is bundled with compiler package version 2019.Update5. mpi/intel/2019/update6/intel [Intel MPI/2019.6.166] This is Intel's MPI implementation, version 2019.6.166, which is bundled with compiler package version 2020. mpi/openmpi/1.8.4/gnu-4.9.2 adds OpenMPI 1.8.4 for GCC 4.9.2 compilers to your environment variables mpi/openmpi/1.8.4/intel-2015-update2 adds OpenMPI 1.8.4 for Intel 2015 update 2 compilers to your environment variables mpi/openmpi/1.10.1/gnu-4.9.2 adds OpenMPI 1.10.1 for GCC 4.9.2 compilers to your environment variables mpi/openmpi/1.10.1/intel-2015-update2 adds OpenMPI 1.10.1 for Intel 2015 update 2 compilers to your environment variables mpi/openmpi/2.0.2/gnu-4.9.2 Adds openmpi 2.0.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/2.0.2/intel-2017 Adds openmpi 2.0.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/2.1.2/gnu-4.9.2 Adds openmpi 2.1.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/2.1.2/intel-2017 Adds openmpi 2.1.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.0.0/gnu-4.9.2 Adds openmpi 3.0.0 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.0.0/intel-2017 Adds openmpi 3.0.0 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.1/gnu-4.9.2 Adds openmpi 3.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.1/intel-2018 Adds openmpi 3.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.4/gnu-4.9.2 Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.4/intel-2018 Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.6/gnu-4.9.2 Adds openmpi 3.1.6 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/4.0.3/gnu-4.9.2 Adds openmpi 4.0.3 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi4py/2.0.0/python2 Adds Python2 mpi4py 2.0.0 to your environment. MPI for Python. mpi4py/2.0.0/python3 Adds Python3 mpi4py 2.0.0 to your environment. MPI for Python. mpi4py/3.0.0/python3 Adds Python3 mpi4py 3.0.0 to your environment. MPI for Python. mpi4py/3.0.2/gnu-4.9.2 Adds mpi4py 3.0.2 for Python 3.7 to your environment. mumps/5.2.1/intel-2018 Adds mumps 5.2.1 to your environment. Sequential version built with Intel and METIS. mysql-connector-python/2.0.4/python-3.5.2 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 mysql-connector-python/2.0.4/python-3.6.3 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 mysql-connector-python/2.0.4/python-3.7.4 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 mysql-connector-python/2.0.4/python-3.8.0 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 nag/fortran/mark22/gnu-4.9.2 Adds NAG Fortran Library Mark 22 for GCC to your environment. nag/fortran/mark24/gnu-4.9.2 Adds NAG Fortran Library Mark 24 for GCC to your environment. nag/fortran/mark24/nag-6.0.1044 Adds NAG Fortran Library Mark 24 for NAG Fortran to your environment. nag/fortran/mark25/intel-2015-update2 Adds NAG Fortran Library Mark 25 for Intel 2015 to your environment. nag/fortran/mark26/gnu-4.9.2 Adds NAG Fortran Library Mark 26 for GCC to your environment. nag/fortran/mark26/intel-2017 Adds NAG Fortran Library Mark 26 for Intel 2017 to your environment. nag/fortran/mark26/nag-6.1.6106 Adds NAG Fortran Library Mark 26 for NAG Fortran to your environment. nag/fortran/mark26/nag-6.2.6223 Adds NAG Fortran Library Mark 26 for NAG Fortran to your environment. nag/mark27/intel-2019 Adds NAG Library Mark 27 for Intel 2019 to your environment. netcdf-c++/4.2/gnu-4.9.2 adds NetCDF C++ 4.2 for GCC to your environment. netcdf-c++/4.2/intel-2015-update2 adds NetCDF C++ 4.2 for Intel 2015 to your environment. netcdf-c++4/4.2/gnu-4.9.2 adds NetCDF C++ 4.2 for GCC to your environment. netcdf-c++4/4.2/intel-2015-update2 adds NetCDF C++ 4.2 for Intel 2015 to your environment. netcdf-fortran/4.4.1/gnu-4.9.2 adds NetCDF 4.4.1 for GCC to your environment. netcdf-fortran/4.4.1/intel-2015-update2 adds NetCDF 4.4.1 for Intel 2015 to your environment. netcdf/4.3.3.1/gnu-4.9.2 adds NetCDF 4.3.3.1 for GCC 4.9.2 to your environment. netcdf/4.3.3.1/intel-2015-update2 adds NetCDF 4.3.3.1 for Intel 2015 to your environment. numactl/2.0.12 numactl provides NUMA policy support, as well as tools and a library to display NUMA allocation statistics and debugging information. openblas/0.2.14-threads/gnu-4.9.2 adds OpenBLAS 0.2.14 for GCC 4.9.2 compilers to your environment variables openblas/0.2.14/gnu-4.9.2 adds OpenBLAS 0.2.14 for GCC 4.9.2 compilers to your environment variables openblas/0.2.14/intel-2015-update2 adds OpenBLAS 0.2.14 for Intel 2015 update 2compilers to your environment variables openblas/0.3.2-native-threads/gnu-4.9.2 This is a module with no description string. openblas/0.3.2-openmp/gnu-4.9.2 This is a module with no description string. openblas/0.3.2-serial/gnu-4.9.2 This is a module with no description string. openblas/0.3.7-native-threads/gnu-4.9.2 This is a module with no description string. openblas/0.3.7-openmp/gnu-4.9.2 This is a module with no description string. openblas/0.3.7-serial/gnu-4.9.2 This is a module with no description string. papi/5.5.1/gnu-4.9.2 Adds PAPI 5.5.1 to your environment. PAPI is a library for working with performance counters, often used in profiling applications. parmetis/4.0.3/intel-2015-update2 Adds parmetis 4.0.3 to your environment. ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. pcre2/10.21/gnu-4.9.2 Adds pcre2 10.21 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. pgplot/5.2.2/intel-2017 Adds PGPlot 5.2.2 to your environment. pgplot/5.2.2/intel-2018 Adds PGPlot 5.2.2 to your environment. pillow-simd/6.0.0.post0/python-3.7.4 Adds Pillow-SIMD to your environment. protobuf/3.5.1/gnu-4.9.2 adds Google Protocol Buffers for GCC 4.9.2 to your environment. protobuf/12-2017/gnu-4.9.2 adds Google Protocol Buffers for GCC 4.9.2 to your environment. pstreams/1.0.1/gnu-4.9.2 Adds pstreams 1.0.1 to your environment. PStreams is a C++ wrapper for process control and streaming using popen and pclose. pygsl/2.1.1-python3.6/gnu-4.9.2 Adds pygsl 2.1.1 to your environment. PyGSL provides Python bindings for the GNU Scientific Library. pyngl/1.4.0 Adds PyNGL to your environment. pynio/1.4.1 Adds PyNIO to your environment. quip/18c5440-threads/gnu-4.9.2 Adds QUIP to your environment. QUIP is required for CP2K. quip/18c5440/gnu-4.9.2 Adds QUIP to your environment. QUIP is required for CP2K. qutip/4.1.0/python-2.7.12 Adds qutip to your environment. scalapack/2.0.2/gnu-4.9.2/openblas Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. scalapack/2.0.2/gnu-4.9.2/openblas-0.3.2 Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. scalapack/2.0.2/gnu-4.9.2/openblas-0.3.7 Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. snappy/1.1.7 Adds Google snappy 1.1.7 to your environment. sparskit2/2009.11.18/gnu-4.9.2 Adds sparskit2 2009.11.18 to your environment. sparskit2/2009.11.18/intel-2015-update2 Adds sparskit2 2009.11.18 to your environment. spectral/3.4.0/bindist Adds spectral 3.4.0 to your environment. Spectral is a set of tools for performing spectral analysis on traces produced by the BSC profiling toolkit. spglib/1.7.4/gnu-4.9.2 Adds spglib 1.7.4 to your environment. Spglib is a library for finding and handling crystal symmetries written in C. squid/1.9g/gnu-4.9.2 Adds squid 1.9g to your environment. suitesparse/4.5.5/gnu-4.9.2-serial Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. suitesparse/4.5.5/gnu-4.9.2-threaded Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. suitesparse/4.5.5/intel-2017-update1 Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. superlu-dist/5.1.0/intel-2015-update2 Adds superlu-dist 5.1.0 to your environment. SuperLU_DIST is the distributed-memory parallel version of SuperLU, a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. superlu/5.2.1/intel-2015-update2 Adds superlu 5.2.1 to your environment. SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. szip/2.1 Adds szip to your environment. ucx/1.8.0/gnu-4.9.2 Adds ucx 1.8.0 to your environment. Unified Communication X (UCX) provides an optimized communication layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric applications. unixodbc/2.3.7 Unix ODBC driver vtk/5.10.1/gnu-4.9.2 adds VTK 5.10.1 for GCC 4.9.2 to your environment. vtk/6.2.0/gnu-4.9.2 adds VTK 6.2.0 for GCC 4.9.2 to your environment. wavpack/5.1.0/gnu-4.9.2 WavPack is a completely open audio compression format providing lossless, high-quality lossy, and a unique hybrid compression mode. webkitgtk/2.2.4-1 Adds the webkitgtk-1 with webkitgtk-devel library to your environment. 2.2.4-1 EL7 RPM binaries. webkitgtk/2.4.9-1 Adds the webkitgtk with webkitgtk-devel library to your environment. 2.4.9-1 EL7 RPM binaries. zeromq/4.1.4/gnu-4.9.2 Adds zeromq 4.1.4 to your environment. ZeroMQ is a distributed messaging library that supports many message-passing patterns and methods. Development Tools \u00a7 This section is for modules for programs that are used in software development, profiling, or troubleshooting. It also contains language interpreters, like Python, Ruby, and Java. Module Description autoconf/2.69 Adds GNU Autoconf Version 2.69 to your environment. autogen/5.18.12/gnu-4.9.2 AutoGen is a tool designed to simplify the creation and maintenance of programs that contain large amounts of repetitious text. automake/1.16.1 Adds GNU Automake Version 1.16.1 to your environment. bazel/0.7.0 Adds bazek to your environment. bazel/0.14.1/gnu-4.9.2 Adds bazek to your environment. bazel/0.21.0/gnu-4.9.2 Adds bazek to your environment. bazel/0.24.0/gnu-4.9.2 Adds bazek to your environment. bazel/0.24.1/gnu-4.9.2 Adds bazek to your environment. bazel/0.26.1/gnu-4.9.2 Adds bazek to your environment. bazel/0.27.1/gnu-4.9.2 Adds bazek to your environment. binutils/2.29.1/gnu-4.9.2 Adds binutils 2.29.1 to your environment. The GNU binutils are a collection of tools for working with binary files and assembling and disassembling machine instructions. bison/3.0.4/gnu-4.9.2 Adds Bison 3.0.4 to your environment. Bison is a general-purpose parser generator. chicken/4.13.0 adds Chicken 4.13.0 to your environment variables clojure/1.10.0.411 This is a module with no description string. cmake/3.2.1 adds Cmake 3.2.1 compilers to your environment variables cmake/3.7.2 adds Cmake 3.7.2 compilers to your environment variables cmake/3.13.3 adds Cmake 3.13.3 compilers to your environment variables cuda/7.5.18/gnu-4.9.2 Adds cuda 7.5.18 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/8.0.61-patch2/gnu-4.9.2 Adds cuda 8.0.61 patch2 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/9.0.176-patch4/gnu-4.9.2 Adds cuda 9.0.176 patch4 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/10.0.130/gnu-4.9.2 Adds cuda 10.0.130 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/10.1.243/gnu-4.9.2 Adds cuda 10.1.243 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cudnn/5.1/cuda-7.5 Adds cuDNN to your environment. cudnn/5.1/cuda-8.0 Adds cuDNN to your environment. cudnn/6.0/cuda-7.5 Adds cuDNN to your environment. cudnn/6.0/cuda-8.0 Adds cuDNN to your environment. cudnn/7.0.4/cuda-8.0 Adds cuDNN to your environment. cudnn/7.1.4/cuda-9.0 Adds cuDNN to your environment. cudnn/7.4.2.24/cuda-9.0 Adds cuDNN to your environment. cudnn/7.4.2.24/cuda-10.0 Adds cuDNN to your environment. cudnn/7.5.0.56/cuda-10.0 Adds cuDNN to your environment. cudnn/7.5.0.56/cuda-10.1 Adds cuDNN to your environment. cudnn/7.6.5.32/cuda-10.0 Adds cuDNN to your environment. cudnn/7.6.5.32/cuda-10.1 Adds cuDNN to your environment. ddt/6.0.4 This module adds DDT 6.0.4 with MIC support to your environment. depot_tools/788d9e0d adds depot_tools to your environment variables depot_tools/c03a9cf adds depot_tools to your environment variables dimemas/5.3.3/bindist Adds dimemas 5.3.3 to your environment. Dimemas is an abstracted network simulator for message-passing programs. doxygen/1.8.14 This is a module with no description string. emacs/24.5 Adds Emacs 24.5 to your environment. An extensible text editor. emacs/26.3 Adds Emacs 26.3 to your environment. An extensible text editor. extrae/3.5.2/intel-2017 Adds extrae 3.5.2 to your environment. Extrae is an instrumentation framework to generate execution traces of the most used parallel runtimes. f2c/2013-09-26/gnu-4.9.2 Adds f2c 2013-09-26 to your environment. f2c is a source-to-source translator from Fortran 77 to C. It is not standards-compliant and is not recommended for use under any circumstances. flex/2.5.39 adds Flex 2.4.39 to your environment variables git/2.3.5 adds Git 2.3.5 to your environment variables git/2.10.2 adds Git 2.10.2 to your environment variables git/2.19.1 adds Git 2.19.1 to your environment variables gperf/3.0.4/gnu-4.9.2 Adds gperf 3.0.4 to your environment. GNU gperf is a perfect hash function generator. guile/2.0.11/gnu-4.9.2 Adds guile 2.0.11 to your environment. haskellplatform/2014.2.0.0 adds Haskell Platform to your environment variables htop/1.0.3/gnu-4.9.2 Adds htop 1.0.3 to your environment. java/1.8.0_45 adds Oracle JDK 1.8.0_45 compilers to your environment variables java/1.8.0_92 adds Oracle JDK 1.8.0_92 compilers to your environment variables java/openjdk-8/8u212/hotspot adds Oracle JDK 8 compilers to your environment variables java/openjdk-8/8u212/openj9 adds Oracle JDK 8 compilers to your environment variables java/openjdk-11/11.0.1 adds Oracle JDK 11.0.1 compilers to your environment variables java/openjdk-11/11.0.3u7/hotspot adds Oracle JDK 11.0.3 compilers to your environment variables java/openjdk-11/11.0.3u7/openj9 adds Oracle JDK 11.0.3 compilers to your environment variables julia/0.3.10 adds Julia 0.3.10 to your environment variables julia/0.4.0 adds Julia 0.4.0 to your environment variables julia/0.4.7 adds Julia 0.4.7 to your environment variables julia/0.5.0 adds Julia 0.5.0 to your environment variables julia/0.6.0 adds Julia 0.6.0 to your environment variables julia/0.7.0 adds Julia 0.7.0 to your environment variables julia/1.0.0 adds Julia 1.0.0 to your environment variables julia/1.1.0 adds Julia 1.1.0 to your environment variables julia/1.2.0 adds Julia 1.2.0 to your environment variables julia/1.3.1 adds Julia 1.3.1 to your environment variables libtool/2.4.6 Adds libtool 2.4.6 to your environment. GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. ltrace/0.7.3/gnu-4.9.2 Adds ltrace 0.7.3 to your environment. lua/5.3.1 This module adds the Lua 5.3.1 package to your environment. Lua is a powerful, fast, lightweight, embeddable scripting language. mc/4.8.14 This module adds Midnight Commander 4.8.14 to your environment. mono/3.12.1 adds Mono 3.12.1 compilers to your environment variables mono/5.20.1.27/gnu-4.9.2 This is a module with no description string. nano/2.4.2 Adds nano 2.4.2 to your environment. A simple text editor. nano/4.9 The nano text editor. nasm/2.13.01 The Netwide Assembler, NASM, is an 80x86 and x86-64 assembler. ncl/6.0.0 adds NCL 6.0.0 to your environment variables ncl/6.3.0 adds NCL 6.3.0 to your environment variables nedit/5.6-aug15 Adds the NEdit GUI text editor to your environment. netlogo/6.1.0 adds NetLogo tooklit compilers to your environment variables paraver/4.6.4.rc1/bindist Adds paraver 4.6.4.rc1 to your environment. Paraver is a trace visualizer for post-mortem trace analysis. perl/5.16.0 This module adds adds Perl 5.16.0 to your environment. perl/5.22.0 This module adds adds Perl 5.22.0 to your environment. perlbrew/0.73 This module adds the Perlbrew 0.73 package to your environment. Use Perlbrew to manage your own Perls and Perl modules pigz/2.4 pigz is a fully functional replacement for gzip that exploits multiple processors and multiple cores when compressing data. pycuda/2017.1/python2 Adds Python2 PyCuda to your environment. MPI for Python. pycuda/2017.1/python3 Adds Python3 PyCuda to your environment. MPI for Python. pypy3/6.0.0/gnu-4.9.2 Pypy is a JIT-ing interpreter for the Python language. This is the version intended to be compatible with CPython 3.5. python/2.7.9 adds Python 2.7.9 with pip and virtualenv to your environment variables python/2.7.12 adds Python 2.7.12 with pip and virtualenv to your environment variables python/3.4.3 adds Python 3.4.3 with pip and virtualenv to your environment variables python/3.5.2 adds Python 3.5.2 with pip and virtualenv to your environment variables python/3.6.1/gnu-4.9.2 Adds Python 3.6.1 with pip and virtualenv to your environment variables. python/3.6.3 Adds Python 3.6.3 with pip and virtualenv to your environment variables. python/3.7.0 Adds Python 3.7.0 with pip and virtualenv to your environment variables. python/3.7.2 Adds Python 3.7.2 with pip and virtualenv to your environment variables. python/3.7.4 Adds Python 3.7.4 with pip and virtualenv to your environment variables. python/3.8.0 Adds Python 3.8.0 with pip and virtualenv to your environment variables. python/idp3/2019/3.6.8 Adds Intel Distribution for Python to your environment variables. python/miniconda3/4.5.11 Adds Miniconda 4.5.11 to your environment variables. qt/4.8.6/gnu-4.9.2 Adds Qt 4.8.6 to your environment. Qt is a cross-platform development tool. qt/5.4.2/gnu-4.9.2 Adds Qt 5.4.2 to your environment. Qt is a cross-platform development tool. qt/5.12.1/gnu-4.9.2 Adds Qt 5.12.1 to your environment. Qt is a cross-platform development tool. qwt/6.1.4/gnu-4.9.2 Adds Qwt 6.1.4 to your environment. racket/6.8 Adds Racket 6.8 to your enviroment. rappture/20130903 Adds the Rappture toolkit to your environment. ruby/2.2.2 Ruby 2.2.2 with RubyGems 2.4.8 and libffi 3.2.1 ruse/1.0.1 A command-line utility to periodically measure the memory use of a process and its subprocesses. sbcl/1.3.19 Adds Steelbank Common LISP 1.3.19 to your environment. scons/2.3.4 adds scons 2.3.4 to your environment variables strace/4.12 Adds strace 4.12 to your environment. Trace system calls and signals. subversion/1.8.13 adds Subversion 1.8.13 to your environment variables swig/3.0.5/gnu-4.9.2 This module adds the SWIG 3.0.5 package to your environment. SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. swig/3.0.7/gnu-4.9.2 This module adds the SWIG 3.0.7 package to your environment. SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. tcl/8.6.8 This is a modulefile for Tcl/Tk 8.6.8 v8/3.15 adds v8 to your environment variables v8/5.6 adds v8 to your environment variables valgrind/3.11.0/gnu-4.9.2 Adds valgrind 3.11.0 to your environment. Valgrind is a framework for building dynamic analysis tools. It includes the memgrind and cachegrind tools. xbae/4.60.4 Adds the Xbae Matrix Widget to your environment. xorg-utils/X11R7.7 Adds xorg-utils from X11R7.7 to your environment. Includes util-macros-1.17, makedepend-1.0.5 libXdmcp-1.1.1 and libXScrnSaver-1.2.2 and imake-1.0.7. Core Modules \u00a7 These modules refer to groups of system tools, rather than applications. They're intended to help you use the system, and some are loaded by default. Module Description gerun adds gerun wrapper to your environment variables lm-utils/1.0 adds utilities to check license manager status to your environment. mrxvt/0.5.4 Adds Mrxvt a multi-tabbed xterm replacement to your environment. ops-tools/1.0.0 Tools for Ops work ops-tools/1.1.0 Tools for Ops work ops-tools/2.0.0 Tools for Ops work rcps-core/1.0.0 adds a core set of applications and libraries to your environment. rlwrap/0.43 adds rlwrap 0.43 to your environment variables screen/4.2.1 adds Screen 4.2.1 to your environment variables userscripts/1.0.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.1.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.2.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.3.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.4.0 Adds userscripts dir to your path. Provides jobhist among other utilities. Beta Modules \u00a7 This section is for modules we're still trying out. They may or may not work with applications from other sections. Module Description compilers/gnu/7.3.0 The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). compilers/gnu/8.3.0 The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). compilers/gnu/9.2.0 The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). cuda/10.1.243/gnu-7.3.0 Adds cuda 10.1.243 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. fftw/3.3.8/gnu-9.2.0 Adds FFTW 3.3.8 for GCC 9.2.0 compilers to your environment variables. Includes single and double precision, plus long-double and quad. Includes OpenMP and POSIX threads libraries. gcc-libs/7.3.0 Base module for gcc 7.3.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). gcc-libs/8.3.0 Base module for gcc 8.3.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). gcc-libs/9.2.0 Base module for gcc 9.2.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). gdal/3.0.4/gnu-9.2.0 adds GDAL 3.0.4 with PROJ.4 7.0.0 to your environment variables. geos/3.8.1/gnu-9.2.0 Adds geos 3.8.1 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. gmt/6.0.0/gnu-9.2.0 adds GMT 6.0.0 to your environment variables gromacs/2020.1/cuda-10.1 Adds gromacs 2020.1 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2020.1/intel-2020 Adds gromacs 2020.1 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gsl/2.6/gnu-9.2.0 adds GSL 2.6 for GCC 9.2.0 to your environment. hdf/5-1.10.5/gnu-9.2.0 Adds hdf 5-1.10.5 to your environment. Serial version built with GNU. libpng/1.6.37/gnu-9.2.0 Adds libpng 1.6.37 to your environment. matlab/full/r2018a/9.4-prefdir-fix Adds Matlab R2018a to your environment. med/4.0.0/gnu-9.2.0 Adds med 4.0.0 to your environment. Allows reading and writing of MED format files. mpi/openmpi/3.1.4/gnu-7.3.0 Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.5/gnu-9.2.0 Adds openmpi 3.1.5 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mumps/5.2.1/gnu-9.2.0 Adds mumps 5.2.1 to your environment. Sequential (threaded) version built with GNU, OpenBLAS and METIS. namd/2.13/intel-2018-update3/testing Adds NAMD 2.13 to your environment namd/2.13/plumed/intel-2018-update3/testing Adds NAMD 2.13 to your environment netcdf/4.7.4/gnu-9.2.0 adds NetCDF 4.7.4 for GCC 9.2.0 to your environment. openblas/0.3.7-native-threads/gnu-9.2.0 OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. openblas/0.3.7-openmp/gnu-9.2.0 OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. openblas/0.3.7-serial/gnu-9.2.0 OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. pcre2/10.35/gnu-9.2.0 Adds pcre2 10.35 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. proj.4/7.0.0/gnu-9.2.0 Adds the PROJ.4 Cartographic Projections library to your environment. r/3.6.3-openblas/gnu-9.2.0 Adds R 3.6.3 and Bioconductor 3.10 to your environment. r/4.0.2-openblas/gnu-9.2.0 Adds R 4.0.2 and Bioconductor 3.11 to your environment. r/r-3.6.3_bc-3.10 adds UCL recommended set of R packages to your environment variables stata/16 Adds Stata/MP 16 to your environment. udunits/2.2.26/gnu-9.2.0 adds the UDUNITS-2 package to your environment. Workaround Modules \u00a7 Sometimes we'll find a problem that can't be fixed properly, but can be worked-around by doing something that can be loaded as a module. That kind of module goes in this section. Module Description bazel-compiler-helpers/intel-2018 Adds bazel compiler wrappers to your environment. getcwd-autoretry This module uses LD_PRELOAD to shadow the getcwd function with a version that retries on failure, and is intended to workaround a bug in the Lustre filesystem.","title":"General Software Lists"},{"location":"Installed_Software_Lists/module-packages/#general-software-lists","text":"Our clusters have a wide range of software installed, available by using the modules system. The module files are organised by name, version, variant (where applicable) and, if relevant, the compiler version used to build the software. If no compiler version is given, either no compiler was required, or only the base system compiler ( /usr/bin/gcc ) and libraries were used. When we install applications, we try to install them on all of our clusters, but sometimes licence restrictions prevent it. If something seems to be missing, it may be because we are not able to provide it. Please contact us for more information if this is hindering your work. The lists below were last updated at 15:31:16 (+0100) on 06 Jun 2020, and are generated from the software installed on the Myriad cluster.","title":"General Software Lists"},{"location":"Installed_Software_Lists/module-packages/#bundles","text":"Some applications or tools depend on a lot of other modules, or have some awkward requirements. For these, we sometimes make a \"bundle\" module in this section, that loads all the dependencies. For Python and R in particular, we also have recommended bundles that load the module for a recent version of Python or R, along with a collection of packages for it that have been requested by users, and the modules those packages require. The lists of Python and R packages installed for those bundles are on separate pages: Python packages R packages We'll sometimes include /new and /old versions of these bundles, if we've recently made a version switch or are intending to make one soon. We send out emails to the user lists about version changes, so if you use these bundles, you should look out for those. Module Description beta-modules This module adds the beta module space to your environment. bioperl/recommended Loads all the modules needed to use BioPerl. blic-modules Adds Cancer Biology supported modules to your environment. cancerit/20190218 adds UCL set of cancerit packages to your environment variables cancerit/recommended adds UCL recommended set of cancerit packages to your environment variables chemistry-modules Adds Chemistry Department supported modules to your environment. climate-tools/recommended Adds set of default applications to the environment for climate science users. deep_earth Sets up VASP, Gnuplot etc for Earth Sciences default-modules-aristotle Adds default Aristotle modules to your environment. default-modules/2015 Adds default Legion modules to your environment. default-modules/2017 Adds default Legion modules to your environment. default-modules/2018 Adds default Legion modules to your environment. economics-modules Adds Economics Department modules to your environment. farr-modules Adds FARR supported modules to your environment. farr/recommended Adds set of default applications to the environment for FARR users. gmt/new Adds set of default modules to the environment for GMT users. gmt/old Adds set of default modules to the environment for gmt users. gmt/recommended Adds set of default modules to the environment for gmt users. naglib/mark27-intel-2019 adds the NAG Library Mark 27 and required modules to your environment. octave/recommended Octave is an open source competitor to Matlab. personal-modules Adds personal modules to your environment. physics-modules Adds Pysics Department supported modules to your environment. pypy3/3.5-compat Adds UCL recommended set of Pypy3 python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/pypy-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/pypy-3.list python2/recommended Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-2.list python3/3.4 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.5 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.6 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.7 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/3.8 Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list python3/recommended Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list r/new adds UCL recommended set of R packages to your environment variables r/old adds UCL recommended set of R packages to your environment variables r/r-3.6.0_bc-3.9 adds UCL recommended set of R packages to your environment variables r/recommended adds UCL recommended set of R packages to your environment variables rsd-modules Adds Research Software Development supported modules to your environment. thermo-modules Adds modules for Molecular Thermodynamics to your environment. torch-deps Loads the dependencies for Torch and makes a quick-install alias. workaround-modules This module adds the workarounds module space to your environment.","title":"Bundles"},{"location":"Installed_Software_Lists/module-packages/#applications","text":"Module Description abaqus/2017 Adds Abaqus 2017 to your environment. adf/2014.10 Adds ADF 2014.10 to your environment. afni/20151030 Adds AFNI to your environment. afni/20181011 Adds AFNI to your environment. amber/14/mpi/intel-2015-update2 Adds AMBER 14 to your environment amber/14/openmp/intel-2015-update2 Adds AMBER 14 to your environment amber/14/serial/intel-2015-update2 Adds AMBER 14 to your environment amber/16/mpi/gnu-4.9.2 Adds AMBER 16 to your environment amber/16/mpi/intel-2015-update2 Adds AMBER 16 to your environment amber/16/openmp/gnu-4.9.2 Adds AMBER 16 to your environment amber/16/openmp/intel-2015-update2 Adds AMBER 16 to your environment amber/16/serial/gnu-4.9.2 Adds AMBER 16 to your environment amber/16/serial/intel-2015-update2 Adds AMBER 16 to your environment ansys/17.2 Adds Ansys CFX/Fluent etc to your environment ansys/18.0 Adds Ansys CFX/Fluent etc to your environment ansys/19.1 Adds Ansys CFX/Fluent, EM etc to your environment ansys/2019.r3 Adds Ansys CFX/Fluent, EM etc to your environment ants/2.1.0 Adds ANTs 2.1.0 (Advanced Normalization Tools) to your environment. ANTs is popularly considered a state-of-the-art medical image registration and segmentation toolkit. approxwf/gnu-4.9.2 Adds ApproxWF to your environment. arrayfire/3.5.0/gnu-4.9.2 Adds ArrayFire 3.5.0 to your environment. asp/2.6.2 Adds NASA Ames Stereo Pipeline (ASP) 6.2.2 to your environment. autodock/4.2.6 Adds AutoDock and AutoGrid 4.2.6 to your environment. AutoDock is a suite of automated docking tools. It is designed to predict how small molecules, such as substrates or drug candidates, bind to a receptor of known 3D structure. bamtools/2.4.0/gnu-4.9.2 Adds BamTools 2.4.0 to your environment. BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. bcftools/1.2/gnu-4.9.2 Adds BCFtools 1.2 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants bcftools/1.3.1/gnu-4.9.2 Adds BCFtools 1.3.1 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants bcftools/2.1/gnu-4.9.2 Adds BCFtools 1.2 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants bcl2fastq/1.8.4 Adds bcl2fastq 1.8.4 to your environment. bcl2fastq2/2.19.1 Adds bcl2fastq2 2.19.1 to your environment. beast/2.3.0 Adds BEAST 2.3.0 with addons to your PATH. bedtools/2.25.0 Adds bedtools 2.25.0 to your environment. The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. bgen/1.1.4 Adds BGen 1.1.4 to your environment. blast+/2.2.30/intel-2015-update2 This module adds the BLAST+ 2.2.30 package to your environment. blast/2.2.26 Adds Blast 2.2.26 to your environment. blender/2.79 Adds Blender Version 2.79 to your environment. boltztrap/1.2.5/intel-2018 Adds boltztrap 1.2.5 to your environment. bowtie/1.1.2 Adds Bowtie 1.1.2 to your environment. bowtie2/2.2.5 Adds Bowtie2 2.2.5 to your environment. bwa/0.6.2/gnu-4.9.2 Adds BWA 0.7.12 to your environment. BWA is a software package for mapping DNA sequences against a large reference genome, such as the human genome. bwa/0.7.12/gnu-4.9.2 Adds BWA 0.7.12 to your environment. BWA is a software package for mapping DNA sequences against a large reference genome, such as the human genome. caffe/1.0/cpu Adds Caffe 1.0 for CPU to your environment. caffe/1.0/cudnn Adds Caffe 1.0 for CUDA+CudNN to your environment. caffe/1.0/gpu Adds Caffe 1.0 for CUDA to your environment. cancerit/20190218-python-2.7.12/gnu-4.9.2 Adds CancerIT program versions as of 20190218 to your environment. The CancerIT Suite is a collection of linked bioinformatics tools. cancerit/gnu-4.9.2 Adds the cancer it suite to your environment. castep/17.2/intel-2017 Adds castep 17.2 to your environment. CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. castep/17.21/intel-2017 Adds castep 17.21 to your environment. CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. castep/19.1.1/intel-2019 CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. cctools/5.4.1/gnu-4.9.2 Adds cctools 5.4.1 to your environment. cctools/7.0.11/gnu-4.9.2 Adds cctools 7.0.11 to your environment. cesm/1.0.6/intel-2015-update2 Adds CESM 1.0.6 to your environment. cesm/1.2.2/intel-2015-update2 Adds CESM 1.2.2 to your environment. cfd-ace/2014.1 Adds CFD-ACE+ to your execution path. Only on Grace and Myriad. cfd-ace/2018.0 Adds CFD-ACE+ to your execution path. Only on Grace and Myriad. chemshell/3.7.1/mpi/gulp4.5 This is a modulefile for ChemShell 3.7.1, MPI+GULP version. Can be used to run other packages if you load a module for those. chemshell/3.7.1/standalone This is a modulefile for ChemShell 3.7.1, standalone serial version. Can be used to run GULP and other packages if you load a module for those. clustal-omega/1.2.1 Adds Clustal Omega 1.2.1 to your environment. clustal-w/2.1 Adds Clustal W 2.1 to your environment. cmg/2017.101 Adds CMG Reservoir Simulation Software Version 2017.101 to your environment. cmg/2018.101 Adds CMG Reservoir Simulation Software Version 2018.101 to your environment. cmg/2019.101 Adds CMG Reservoir Simulation Software Version 2019.101 to your environment. collectl/4.0.2 [collectl/4.0.2] collectl is a tool for tracking and monitoring various node usage statistics. compucell3d/3.7.4 Adds CompuCell3D to your environment comsol/52 Adds the COMSOL 52 binaries to your environment. COMSOL Multiphysics\u00ae is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. Module must be loaded once from a login node prior to running jobs. comsol/52a Adds the COMSOL 52a binaries to your environment. COMSOL Multiphysics\u00ae is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. Module must be loaded once from a login node prior to running jobs. comsol/53a Adds COMSOL Multiphysics Version 53a to your environment. cosi-corr/oct14 Adds COSI-Corr Version Oct14 for use with ENVI 5.5.2/5.5.3 to your environment. covid-19-spatial-sim/0.8.0/intel-2020 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.9.0/gnu-4.9.2 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.13.0/gnu-4.9.2 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.14.0/gnu-4.9.2 SpatialSim COVID-19 pandemic modelling tool from Imperial College. covid-19-spatial-sim/0.14.0/intel-2020 SpatialSim COVID-19 pandemic modelling tool from Imperial College. cp2k/4.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cp2k/5.1/ompi-plumed/gnu-4.9.2 Adds CP2K to your environment. cp2k/5.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cp2k/6.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cp2k/7.1/ompi/gnu-4.9.2 Adds CP2K to your environment. cpmd/4.1/intel-2017 Adds CPMD 4.1 to your environment. crystal14/v1.0.3 Adds Crystal14 v1.0.3 to your environment. crystal14/v1.0.4 Adds Crystal14 v1.0.4 to your environment. crystal14/v1.0.4_2017 Adds Crystal14 v1.0.4 to your environment. crystal17/v1.0.1 Adds Crystal17 v1.0.1 to your environment. crystal17/v1.0.2/intel-2017 The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. cuba/4.2/gnu-4.9.2 adds Cuba Numerical Integration Package Version 4.2 to your environment. cufflinks/2.2.1 Adds Cufflinks 2.2.1 to your environment. curl/7.47.1/gnu-4.9.2 Adds curl 7.47.1 to your environment. datamash/1.4 This is a module with no description string. deeptools/3.0.2 Adds deeptools to your environment. delly/0.7.8-bindist Delly is an integrated structural variant (SV) prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. dftbplus/17.1/intel-2017 DFTB+ is a quantum mechanical simulation software package, based on the Density Functional Tight Binding (DFTB) method. dftbplus/18.2/intel-2018 DFTB+ is a software package for carrying out fast quantum mechanical atomistic calculations based on the Density Functional Tight Binding method. dftbplus/19.1/intel-2018 DFTB+ is a software package for carrying out fast quantum mechanical atomistic calculations based on the Density Functional Tight Binding method. dftbplus/dev/d07f92e/intel-2017 DFTB+ is a quantum mechanical simulation software package, based on the Density Functional Tight Binding (DFTB) method. dl_poly/4.07/intel-2015-update2 Adds DL_POLY 4.07 to your environment dl_poly/4.08-plumed-2.3.1/intel-2017 Adds dl_poly 4.08 to your environment. DL_POLY is a general purpose classical molecular dynamics (MD) simulation software developed at Daresbury Laboratory. This version has been linked against the PLUMED metadynamics library. dl_poly/4.08/intel-2015-update2 Adds DL_POLY 4.08 to your environment. dl_poly/4.09/intel-2018 Adds DL_POLY 4.09 to your environment. dl_poly/classic/1.9/intel-2015-update2 Adds DL_POLY Classic 1.9 to your environment dock/6.9-impi/intel-2018 The DOCK suite of programs is designed to find favorable orientations of a ligand in a receptor. This is the Intel MPI build, intended for high-performance parallel runs. dock/6.9-reference/gnu-4.9.2 The DOCK suite of programs is designed to find favorable orientations of a ligand in a receptor. This is a reference build intended to be close to the version of the software the developers test with: a serial build using the GNU compilers. dos2unix/7.3 Adds dos2unix 7.3 to your environment. Text format converters dos2unix, unix2dos, mac2unix, unix2mac. dssp/3.0.0/gnu-4.9.2 Adds dssp 3.0.0 to your environment. DSSP calculates DSSP entries from Protein Databank (PDB) entries. dymola/2020.1-1 Dymola is a commercial modeling and simulation environment based on the open Modelica modeling language. ea-utils/822 Adds ea-utils to your environment. easylausanne/55c7bf0 Adds Easy Lausanne to your environment. eigensoft/6.1.1/gnu-4.9.2 Adds EIGENSOFT 6.1.1 to your environment. Population genetics methods and EIGENSTRAT stratification correction method. elk/4.0.15/intel-2018 Adds Elk 4.0.15 to your environment. Binary is elk. elk/4.0.15/intel-2018+wa Adds Elk 4.0.15 to your environment. Binary is elk. elk/4.3.6/intel-2017 Adds Elk 4.3.6 to your environment. Binary is elk. elk/4.3.6/intel-2017+wa Adds Elk 4.3.6 to your environment. Binary is elk. elk/5.2.14/intel-2018 Elk is an all-electron full-potential linearised augmented-planewave (FP-LAPW) code. energyplus/8.9.0-bindist EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. energyplus/9.1.0-bindist EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. envi/5.5.2 Adds ENVI 5.5.2 with IDL 8.7.2 to your environment. envi/5.5.3 Adds ENVI 5.5.3 with IDL 8.7.3 to your environment. epacts/3.3.0/gnu-4.9.2 Adds EPACTS 3.3.0 to your environment. examl/8dcf2cc/gnu-4.9.2 Adds ExaML to your environment. exonerate/2.2.0 Adds Exonerate to your environment. fasta/36.3.8d/gnu-4.9.2 Adds the cancer it suite to your environment. fastqc/0.11.5 Adds FastQC 0.11.5 to your environment. A quality control application for high throughput sequence data. fastqc/0.11.8 Adds FastQC 0.11.8 to your environment. A quality control application for high throughput sequence data. ffmpeg/4.1/gnu-4.9.2 FFmpeg is a framework for encoding, decoding, muxing, demuxing, encoding, transcoding, streaming, filtering, and playing many types of audio and video media. fgbio/0.5.1 Adds fgbio to your environment. fgbio is a command line toolkit for working with genomic and particularly next generation sequencing data. fgbio/0.6.1 Adds fgbio to your environment. fgbio is a command line toolkit for working with genomic and particularly next generation sequencing data. figtree/1.4.2 Adds Figtree 1.4.2. foldx/4 Adds FoldX Suite 4 to your environment. freesurfer/5.3.0 Adds FreeSurfer 5.3.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. freesurfer/6.0.0 Adds FreeSurfer 6.0.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. fsl/5.0.9 Adds FSL 5.0.9 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. fsl/5.0.10 Adds FSL 5.0.10 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. fsl/6.0.0 Adds FSL 6.0.0 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. fsl/6.0.0_cuda Adds FSL 6.0.0 CUDA (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. gamess/5Dec2014_R1/intel-2015-update2 Adds GAMESS 5Dec2014_R1 to your environment, built for Intel MPI. Uses ~/Scratch/gamess for USERSCR. You can override by exporting GAMESS_USERSCR as another path. gatk/3.4.46 Adds GATK 3.4.46 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gatk/3.8.0 Adds GATK 3.8.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gatk/4.0.3.0 Adds GATK 4.0.3.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gatk/4.0.8.0 Adds GATK 4.0.8.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php gaussian/g09-c01_linda/pgi-2013.9 Adds Gaussian 09 Revision C01 and GaussView 5 to your environment. gaussian/g09-d01/pgi-2015.4 Adds Gaussian G09-D01 to your environment and also includes Linda and Gaussview 5. gaussian/g09-d01/pgi-2015.7 Adds Gaussian G09-D01 to your environment and also includes Linda and Gaussview 5 gaussian/g16-a03/pgi-2016.5 Adds Gaussian G16-A03 to your environment and also includes Linda and Gaussview 6 gdal/2.0.0 Adds GDAL 2.0.0 to your environment variables. Works with Python 2. gdal/2.1.1 adds GDAL 2.1.1 with PROJ.4 4.9.1 to your environment variables. Works with Python 2. gdal/2.1.4 adds GDAL 2.1.4 with PROJ.4 6.1.0 to your environment variables. Works with Python 2. gdal/3.0.4/gnu-4.9.2 adds GDAL 3.0.4 with PROJ.4 6.1.0 to your environment variables. Works with Python 2. gephi/0.9.1 Adds Gephi Version 0.9.1 to your environment. ghostscript/9.16/gnu-4.9.2 Adds Ghostscript 9.16 to your environment. ghostscript/9.19/gnu-4.9.2 Adds Ghostscript 9.19 to your environment. gmsh/2.12.0-bindist Adds gmsh 2.12.0 to your environment. Gmsh is a free 3D finite element grid generator with a build-in CAD engine and post-processor. gmt/5.1.2 adds GMT 5.1.2 to your environment variables gmt/5.3.1 adds GMT 5.3.1 to your environment variables gmt/5.4.5 adds GMT 5.4.5 to your environment variables gnuplot/5.0.1 Adds gnuplot 5.0.1 to your environment. Gnuplot is a portable command-line driven graphing utility. grace/5.1.25 Adds Grace 5.1.25 to your environment. Grace is a 2D plotting tool. graphicsmagick/1.3.21 adds GraphicsMagick 1.3.21 to your environment variables graphviz/2.38.0/gnu-4.9.2 This module adds the Graphviz 2.38.0 package to your environment. Graphviz is open source graph visualization software. graphviz/2.40.1/gnu-4.9.2 This module adds the Graphviz 2.40.1 package to your environment. Graphviz is open source graph visualization software. groff/1.22.3/gnu-4.9.2 Adds GNU groff Version 1.22.3 to your environment. gromacs/5.0.4/intel-2015-update2 Adds GROMACS 5.0.4 to your environment, built using MKL gromacs/5.0.4/plumed/intel-2015-update2 Adds GROMACS 5.0.4 with Plumed 2.1.2 to your environment. Note: Plumed will always run in double precision even if GROMACS is single-precision, so only use that combination if you need it and are aware of the effects. gromacs/5.1.1/intel-2015-update2 Adds GROMACS 5.1.1 to your environment, built using MKL gromacs/5.1.1/plumed/intel-2015-update2 Adds GROMACS 5.1.1 with Plumed 2.2 to your environment. Note: Plumed will always run in double precision even if GROMACS is single-precision, so only use that combination if you need it and are aware of the effects. gromacs/5.1.3/plumed/intel-2015-update2 GROMACS 5.1.3 molecular dynamics package, built with Intel 2015u2 compilers, PLUMED 2.2.3 patches (including libmatheval), and OpenBLAS 0.2.14. gromacs/2016.3/intel-2017-update1 Adds GROMACS 2016.3 to your environment, built using MKL gromacs/2016.3/plumed/intel-2017-update1 GROMACS 2016.3 molecular dynamics package, built with Intel 2017u1 compilers, PLUMED 2.3.1 patches (including libmatheval), and OpenBLAS 0.2.14. gromacs/2016.4/plumed/intel-2017 GROMACS 2016.4 molecular dynamics package, built with Intel 2017u4 compilers, PLUMED 2.4.1 patches (including libmatheval) with hrex, and OpenBLAS 0.2.14. gromacs/2018.2/intel-2018 Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2018.3/intel-2018 Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2018.3/plumed/intel-2018 GROMACS 2018.3 molecular dynamics package, built with Intel 2018u3 compilers, PLUMED 2.4.3 patches (including libmatheval). gromacs/2018/intel-2017 Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2019.3/cuda-10 Adds gromacs 2019 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2019.3/intel-2018 Adds gromacs 2019 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2019.3/plumed/intel-2018 GROMACS 2019.3 molecular dynamics package, built with Intel 2018u3 compilers, PLUMED 2.5.2 patches (including libmatheval). gulp/4.5/intel-2018 Adds GULP 4.5 to your environment. Built with FoX and without plumed. GULP is a materials simulation code. gulp/5.1.1/intel-2018 Adds GULP 5.1.1 to your environment. Built with FoX and without plumed. GULP is a materials simulation code. gurobi/7.5.1 Adds Gurobi 7.5.1 to your environment. gurobi/8.1.1 Adds Gurobi 8.1.1 to your environment. h5utils/1.12.1 Adds h5utils 1.12.1 to your environment. h5utils is a set of utilities for visualization and conversion of scientific data in HDF5 format. hammock/1.0.5 Loads the dependencies for Hammock 1.0.5 to your environment and makes a quick-install alias, do-hammock-install. Run as java -Xmx2g -jar $HAMMOCKPATH/Hammock.jar mode param1 param2 -d outputpath. Will use Scratch for temporary files. hhsuite/3.0-beta.1/gnu-4.9.2 Adds hhsuite 3.0-beta.1 to your environment. hmmer/3.1b2 Adds HMMER 3.1b2 to your environment. hoomd-blue/2.4.2 Adds HOOMD-blue to your environment. hopspack/2.0.2/gnu-4.9.2 Adds HOPSPACK 2.0.2 to your environment hopspack/2.0.2/intel-2017 Adds HOPSPACK 2.0.2 to your environment icommands/4.1.7 [icommands/4.1.7] The iRODS iCommands are the command-line clients to an iRODS system. idl/8.4.1 Adds IDL 8.4.1 to your environment. idl/8.7.3 Adds IDL 8.7.3 to your environment. illustrate/20190807 adds Illustrate to your environment variables impute2/2.3.2 adds Impute2 V2.3.2 to your environment. inetutils/1.9.4 GNU inetutils is a package of utilities for performing a range of network tasks including FTP and telnet clients. intltool/0.51.0 Adds intltool 0.51.0 to your environment. intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. iva/0.11.6 Adds IVA 0.11.6 to your environment. iva/1.0.0 Adds IVA 1.0.0 to your environment. jags/3.4.0/gnu.4.9.2-atlas Adds JAGS 3.4.0 to your environment. jags/3.4.0/gnu.4.9.2-openblas Adds JAGS 3.4.0 to your environment. jags/4.2.0/gnu.4.9.2-openblas Adds JAGS 4.2.0 to your environment. jq/1.5/gnu-4.9.2 adds jq for GCC 4.9.2 to your environment. kallisto/v0.42.5 Adds Kallisto v0.42.5 to your environment. keras/2.2.4 Adds Keras to your environment. kmc/2.1.1/gnu-4.9.2 Adds KMC 2.1.1 to your environment. KMC is a disk-based program for counting k-mers from FASTQ/FASTA files. knitro/12.0.0/gnu-4.9.2 Adds Knitro solver 12.0.0 to your environment. lammps/3Mar20/plumed-colvars/intel-2018 Adds LAMMPS 3Mar20 to your environment. LAMMPS is a GPL molecular dynamics code which shows exceptional scaling on a wide variety of machines. Binary is lmp_mpi or lmp_default. This version was built with packages kspace, manybody, molecule, rigid, lib-linalg, user-colvars and user-plumed. lammps/7Aug19/basic/intel-2018 Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. lammps/7Aug19/gpu/intel-2018 Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. lammps/7Aug19/userintel/intel-2018 Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. lammps/8Dec15/intel-2015-update2 Adds LAMMPS 8Dec15 to your environment. Binary is lmp_default. lammps/10Feb15/intel-2015-update2 Adds LAMMPS 10Feb15 to your environment. Binary is lmp_default. lammps/13Apr17/intel-2017 Adds LAMMPS 13Apr17 to your environment. Binary is lmp_default. lammps/16Mar18/basic/intel-2018 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lammps/16Mar18/gpu/intel-2018 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lammps/16Mar18/intel-2017 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lammps/16Mar18/userintel/intel-2018 Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. lynx/2.8.9 Adds Lynx Version 2.8.9 to your environment. mathematica/10.1.0 Adds Mathematica 10.1.0 to your environment. mathematica/10.2.0 Adds Mathematica 10.2.0 to your environment. mathematica/10.4.0 Adds Mathematica 10.4.0 to your environment. mathematica/11.0.1 Adds Mathematica 11.0.1 to your environment. mathematica/11.2.0 Adds Mathematica 11.2 to your environment. mathematica/11.3.0 Adds Mathematica 11.3 to your environment. matlab/full/r2015a/8.5 Adds Matlab R2015a for SPM to your environment. matlab/full/r2015b/8.6 Adds Matlab R2015b to your environment. matlab/full/r2016b/9.1 Adds Matlab R2016b to your environment. matlab/full/r2017a/9.2 Adds Matlab R2017a to your environment. matlab/full/r2018a/9.4 Adds Matlab R2018a to your environment. matlab/full/r2018b/9.5 Adds Matlab R2018b to your environment. matlab/full/r2019b/9.7 Adds Matlab R2019b to your environment. mcl/14-137 Adds MCL 14-137 your environment. meep/1.3-ompi/gnu-4.9.2 Adds meep 1.3-ompi to your environment. meep/1.3/gnu-4.9.2 Adds meep 1.3 to your environment. meep/1.11.0-ompi/gnu-4.9.2 Adds meep 1.11.0-ompi to your environment. MEEP is a package for electromagnetics simulation via the finite-diffe rence time-domain (FDTD) method. meme/4.10.1_4 Adds MEME Suite 4.10.1_4 to your environment. The MEME Suite: Motif-based sequence analysis tools. This install is for the command-line tools and connects to their website for further analysis. Web: http://meme-suite.org mgltools/1.5.6 Adds MGLTools 1.5.6 to your environment. Applications for visualization and analysis of molecular structures. Contains AutoDockTools (ADT), Python Molecular Viewer (PMV) and Vision. mirdeep/2.0.0.7 Adds mirdeep 2.0.0.7 to your environment. molden/5.2.2 Adds Molden 5.2.2 to your environment. molpro/2012.1.25/gnu-4.9.2 Adds Molpro to your environment molpro/2015.1.3 Adds Molpro 2015.1.3 binary (no Infiniband support) to your environment. molpro/2015.1.5/intel-2015-update2 Adds Molpro 2015.1.5 built from source with MPI to your environment. mosek/9.1.12 Adds Mosek 9.1.12 to your environment. mothur/1.41.3-bindist Mothur is an expandable, multi-purpose bioinformatics tool aimed at microbial ecology. mpb/1.5-ompi/gnu-4.9.2 Adds mpb 1.5 to your environment. mpb/1.5/gnu-4.9.2 Adds mpb 1.5 to your environment. mpb/1.9.0-hdf5-ompi/gnu-4.9.2 Adds serial mpb 1.9.0 to your environment. Built with HDF5-ompi for use by parallel MEEP. mrbayes/3.2.5/mpi/intel-2015-update2 Adds MrBayes 3.2.5 to your environment mrbayes/3.2.5/serial/intel-2015-update2 Adds MrBayes 3.2.5 to your environment mrtrix/0.3.12/nogui Adds MRtrix3 0.3.12 to your environment. MRtrix3 provides a set of tools to perform analysis of diffusion MRI data, based around the concept of spherical deconvolution and probabilistic tractography. Note: mrview and shview cannot be run over a remote X11 connection so are not usable. mrtrix/0.3.16/gnu-4.9.2/nogui MRtrix provides a set of tools to perform various advanced diffusion MRI analyses, including constrained spherical deconvolution (CSD), probabilistic tractography, track-density imaging, and apparent fibre density. mrtrix/3.0rc3/gnu-4.9.2/nogui Adds MRtrix 3.0RC3 to your environment. mstor/2013/gnu-4.9.2 MSTor is a program for calculating partition functions, free energies, enthalpies, entropies, and heat capacities of complex molecules including torsional anharmonicity. mumax/3.9.3 Adds Mumax 3.9.3 to your environment. mummer/3.23/gnu-4.9.2 Adds MUMmer 3.23 to your environment. MUMmer is a system for rapidly aligning entire genomes, whether in complete or draft form. muscle/3.8.31 Adds MUSCLE 3.8.31 to your environment. mutect/1.1.7 Adds MuTect 1.1.7 to your environment. MuTect is a GATK-based variant caller specialized for somatic/cancer variants. namd/2.10/intel-2015-update2 Adds NAMD 2.10 to your environment namd/2.11/intel-2015-update2 Adds NAMD 2.11 to your environment namd/2.12/intel-2015-update2 Adds NAMD 2.12 to your environment namd/2.12/intel-2017-update1 Adds NAMD 2.12 to your environment namd/2.12/intel-2018-update3 Adds NAMD 2.12 to your environment namd/2.13/intel-2018-update3 Adds NAMD 2.13 to your environment namd/2.13/plumed/intel-2018-update3 Adds NAMD 2.13 to your environment nco/4.5.0 Adds nco to your environment. nektar++/4.3.5-impi/intel-2017-update1 Adds Nektar++ Version 4.3.5 to your environment nektar++/4.3.5-ompi/gnu-4.9.2 Adds Nektar++ Version 4.3.5 to your environment ngsutils/0.5.9 Adds a set of python scripts for handling various NGS tasks to your environment. nighres/1.1.0b Adds Nighres to your environment. nonmem/7.3.0/gnu-4.9.2 Adds NONMEM 7.3.0 using GCC Fortran 4.9.2 to your environment. nonmem/7.3.0/intel-2015-update2 Adds NONMEM 7.3.0 using Intel Fortran 2015 to your environment. novocraft/3.04.06 Adds novocraft 3.04.06 to your environment. Novocraft is a set of tools for bioinformatics, including Novoalign for short-read mapping. nwchem/6.5-r26243/atlas/intel-2015-update2 Adds NWChem 6.5 revision 26243 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and ATLAS. Global .nwchemrc: /shared/ucl/apps/nwchem/6.5-r26243-atlas/intel-2015-update2.nwchemrc nwchem/6.5-r26243/intel-2015-update2 Adds NWChem 6.5 revision 26243 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.5-r26243/intel-2015-update2/.nwchemrc nwchem/6.6-r27746/intel-2015-update2 Adds NWChem 6.6 revision 27746 patched 2016-01-20 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.6-r27746/intel-2015-update2/.nwchemrc nwchem/6.6-r27746/intel-2017 Adds NWChem 6.6 revision 27746 patched 2016-01-20 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.6-r27746/intel-2017/.nwchemrc nwchem/6.8-47-gdf6c956/intel-2017 Adds NWChem 6.8 47-gdf6c956 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.8-47-gdf6c956/intel-2017/.nwchemrc oasislmf/1.2.4 Oasis LMF 1.2.4 oasislmf/ktools/3.0.3/gnu-4.9.2 OasisLMF ktools package built with the GNU compilers oasislmf/ktools/f92a41f/gnu-4.9.2 OasisLMF ktools package built with the GNU compilers octave/4.4.1 Octave is an open source competitor to Matlab which is mostly compatible with Matlab. octopus/4.1.2-impi/intel-2015-update2 Adds octopus 4.1.2 to your environment. octopus/4.1.2/intel-2015-update2 Adds octopus 4.1.2 to your environment. octopus/5.0.1-ompi/gnu-4.9.2 Adds octopus 5.0.1 to your environment. octopus/5.0.1/gnu-4.9.2 Adds octopus 5.0.1 to your environment. octopus/6.0-ompi/gnu-4.9.2 Adds octopus 6.0 to your environment. octopus/6.0/gnu-4.9.2 Adds octopus 6.0 to your environment. openbabel/2.4.1/gnu-4.9.2 OpenBabel is a library and command-line tool for manipulating and converting between various chemistry file formats. opencv/2.4.13/gnu-4.9.2 Adds OpenCV 2.4.13 to your environment. Open Source Computer Vision Library. opencv/3.4.1/gnu-4.9.2 Adds OpenCV 3.4.1 to your environment. Open Source Computer Vision Library. openfoam/2.3.1/intel-2015-update2 Adds OpenFOAM 2.3.1 to your environment openfoam/2.4.0/intel-2017-update1 Adds OpenFOAM 2.4.0 to your environment openfoamplus/v1706/gnu-4.9.2 Adds OpenFOAMplus v1706 to your environment openfoamplus/v1906/gnu-7.3.0 Adds OpenFOAMplus v1906 to your environment openmm/7.3.1/cuda-10 Adds OpenMM to your environment. openmm/7.3.1/gnu-4.9.2 Adds OpenMM to your environment. openmx/3.8.3 Adds OpenMX 3.8.3 to your environment. optimet/1.0.1/gnu-4.9.2 Adds Optimet to your environment. p7zip/15.09/gnu-4.9.2 Adds p7zip 15.09 to your environment. To expand 7z files: 7za x archive.7z pandoc/1.19.2.1 Adds pandoc Version 1.19.2.1 to your environment. parallel/20181122 GNU parallel is a shell tool for executing jobs in parallel using one or more computers. paraview/5.3.0 This module adds the ParaView 5.3.0 binaries to your environment. ParaView is an open-source, multi-platform data analysis and visualization application. parmed/3.2.0 Adds ParmEd to your environment. petsc/3.12.1/gnu-4.9.2 Adds Petsc 3.12.1 to your environment phon/1.39/gnu-4.9.2 Adds Phon 1.3.9 with addons to your PATH. phon/1.43/gnu-4.9.2 Adds Phon 1.43 with addons to your PATH. picard-tools/1.136 Adds Picard Tools 1.136 to your environment. If using the java -jar command, you should pass TMP_DIR=$TMPDIR to Picard. picard-tools/2.18.9 Adds Picard Tools to your environment. If using the java -jar command, you should pass TMP_DIR=$TMPDIR to Picard. platypus/3e72641 Adds Platypus to your environment. plink/1.07 Adds Plink 1.07 with addons to your PATH. plink/1.90b3.40 Adds PLINK 1.90b3.40 to your environment. A comprehensive update to the PLINK association analysis toolset. plink/2.0alpha-git Adds PLINK 2.0 alpha to your environment. A comprehensive update to the PLINK association analysis toolset. plumed/2.1.2/intel-2015-update2 Adds PLUMED 2.1.2 to your environment, built using OpenBLAS plumed/2.2.3/intel-2015-update2 Adds PLUMED 2.2.3 to your environment, built using OpenBLAS and libmatheval plumed/2.2/intel-2015-update2 Adds PLUMED 2.2 to your environment, built using OpenBLAS plumed/2.3.1/intel-2017-update1 Adds PLUMED 2.3.1 to your environment, built using OpenBLAS and libmatheval plumed/2.4.1/gnu-4.9.2 Adds PLUMED 2.4.1 to your environment, built using OpenBLAS and libmatheval plumed/2.4.1/intel-2017-update4 Adds PLUMED 2.4.1 to your environment, built using OpenBLAS and libmatheval plumed/2.4.3/intel-2018 Adds PLUMED 2.4.3 to your environment, built using MKL and libmatheval plumed/2.5.2/intel-2018 Adds PLUMED 2.5.2 to your environment, built using MKL and libmatheval plumed/2.6.0/intel-2018 Adds PLUMED 2.6.0 to your environment, built using MKL and libmatheval postgres+postgis/9.5.3+2.2.2/gnu-4.9.2 Adds postgres+postgis 9.5.3+2.2.2 to your environment. PostgreSQL is a relational database, and PostGIS is a geographical information enhancement for PostgreSQL. postgresql/9.5.3/gnu-4.9.2 Adds postgresql 9.5.3 to your environment. PostgreSQL is a relational database. primer3/2.3.6 This module adds the primer3 package to your environment. probabel/0.4.5/gnu-4.9.2 Adds ProbABEL to your environment. proj.4/4.9.1 Adds the PROJ.4 Cartographic Projections library to your environment. proj.4/5.2.0 Adds the PROJ.4 Cartographic Projections library to your environment. proj.4/6.0.0 Adds the PROJ.4 Cartographic Projections library to your environment. proj.4/6.1.0 Adds the PROJ.4 Cartographic Projections library to your environment. proovread/2.13.11-8Jan2016-f6a856a Adds proovread 2.13.11-8Jan2016-f6a856a to your environment. f6a856a is the commit for this version. pymol/1.7.7.2 Adds PyMol to your environment. pymol/1.8.2.1 Adds PyMol to your environment. pyrosetta/release-73 Adds PyRosetta to your environment. pytorch/1.2.0/cpu Adds PyTorch 1.2.0 to your environment. pytorch/1.2.0/gpu Adds PyTorch 1.2.0 to your environment. qctool/2/beta/ba5eaa44a62f This module adds qctool v2 beta to your environment. quantum-espresso/5.2.0-impi/intel-2015-update2 Adds quantum-espresso 5.2.0 to your environment. quantum-espresso/6.1-impi/intel2017 Adds quantum-espresso 6.1 to your environment. quantum-espresso/6.3-impi/thermo_pw-1.0.9/intel-2018 Adds quantum-espresso 6.3 + thermo_pw 1.0.9 to your environment. quantum-espresso/6.4.1-impi/intel-2018 Adds quantum-espresso 6.4.1 to your environment. quantum-espresso/6.5-impi/intel-2018 Adds quantum-espresso 6.5 to your environment. quantum-espresso/6.5-impi/thermo_pw-1.2.1/intel-2018 Adds quantum-espresso 6.5 + thermo_pw 1.2.1 to your environment. r/3.2.0-atlas/gnu-4.9.2 Adds R 3.2.0 and Bioconductor 3.2 to your environment. r/3.2.2-openblas/gnu-4.9.2 Adds R 3.2.2 and Bioconductor 3.2 to your environment. r/3.3.0-openblas/gnu-4.9.2 Adds R 3.3.0 and Bioconductor 3.3 to your environment. r/3.3.2-openblas/gnu-4.9.2 Adds R 3.3.2 and Bioconductor 3.4 to your environment. r/3.4.0-openblas/gnu-4.9.2 Adds R 3.4.0 and Bioconductor 3.5 to your environment. r/3.4.2-openblas/gnu-4.9.2 Adds R 3.4.2 and Bioconductor 3.6 to your environment. r/3.5.0-openblas/gnu-4.9.2 Adds R 3.5.0 and Bioconductor 3.7 to your environment. r/3.5.1-openblas/gnu-4.9.2 Adds R 3.5.1 and Bioconductor 3.7 to your environment. r/3.5.3-openblas/gnu-4.9.2 Adds R 3.5.3 and Bioconductor 3.8 to your environment. r/3.6.0-openblas/gnu-4.9.2 Adds R 3.6.0 and Bioconductor 3.9 to your environment. randfold/2.0/gnu-4.9.2 Adds randfold 2.0 to your environment. rclone/1.51.0 RClone is a command-line program intended to download and upload files from and to various storage services and providers. repast-hpc/2.1/gnu-4.9.2 Adds Repast HPC 2.1 compiled with GCC 4.9.2 and OpenMPI to your environment. root/5.34.30/gnu-4.9.2 Adds ROOT 5.34.30 to your environment. root/5.34.30/gnu-4.9.2-fftw-3.3.6 Adds ROOT 5.34.30 to your environment. root/5.34.36/gnu-4.9.2-fftw-3.3.6 Adds ROOT 5.34.36 to your environment. root/5.34.36/gnu-4.9.2-fftw-3.3.6-gsl-2.4 Adds ROOT 5.34.36 to your environment. root/6.04.00/gnu-4.9.2 Adds ROOT 6.04.00 to your environment. rosetta/2015.31.58019 Adds Rosetta 2015.31.58019 to your environment. rosetta/2015.31.58019-mpi Adds Rosetta 2015.31.58019 with MPI to your environment. rosetta/2018.48.60516 Adds Rosetta 2018.48.60516 serial version to your environment. rosetta/2018.48.60516-mpi Adds Rosetta 2018.48.60516 MPI version to your environment. rsem/1.2.31 Adds RSEM 1.2.31 to your environment. sac/101.6a Adds SAC 101.6a to your environment. sambamba/0.6.7-bindist A tool for extracting information from SAM/BAM files. samblaster/0.1.24/gnu-4.9.2 samblaster is a program for marking duplicates in read-id grouped paired-end SAM files. samsrf/5.84/matlab.r2019b Adds the SamSrf Matlab toolbox to your environment samtools/0.1.19 This module adds the Samtools 0.1.19 package to your environment. samtools/1.2/gnu-4.9.2 Adds SAMtools 1.2 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. samtools/1.3.1/gnu-4.9.2 Adds SAMtools 1.3.1 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. samtools/1.9/gnu-4.9.2 Adds SAMtools 1.9 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. sas/9.4-M6/64 Adds SAS 9.4 (9.04.01M6) 64 bit to your environment sas/9.4/64 Adds SAS 9.4 64 bit to your environment sc/7.16 Adds sc 7.16 to your environment. siesta/4.0.1/intel-2017 Adds SIESTA 4.0.1 to your environment. skewer/0.2.2 Adds skewer 0.2.2 to your environment. smalt/0.7.6/gnu-4.9.2 Adds SMALT 0.7.6 to your environment. SMALT aligns DNA sequencing reads with a reference genome. Compiled with bambamc support for SAM/BAM input and BAM output. snptest/2.5.4-beta3 Adds SNPtest 2.5.4-beta3 to your environment. sod/3.2.7 Adds SOD 3.2.7 to your environment. SOD is a program that automates tedious data selection, downloading, and routine processing tasks in seismology. spm/8/r6313/matlab.r2015a Adds SPM8 to your environment spm/12/jan2020/matlab.r2019b Adds SPM12 to your environment spm/12/r6470/matlab.r2015a Adds SPM12 to your environment spss/24 Adds SPSS 24 to your environment spss/25 Adds SPSS 25 to your environment spss/26 Adds SPSS 26 to your environment sqlite/3.31.1/gnu-9.2.0 Adds SQLite Version 3.31.1 to your environment. star-ccm+/9.06.011 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/11.04.010-R8 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/12.04.010 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/13.02.011 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/13.06.012 Adds STAR-CCM+ and STAR-View to your environment. star-ccm+/14.06.013 Adds STAR-CCM+ and STAR-View to your environment. star-cd/4.22.058 Adds STAR-CD 4.22.058 to your environment. star-cd/4.26.011 Adds STAR-CD 4.26.011 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. star-cd/4.26.022 Adds STAR-CD 4.26.022 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. star-cd/4.28.050 Adds STAR-CD 4.28.050 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. star/2.5.2a Adds STAR 2.5.2a to your environment. star/2.7.3a Adds STAR 2.7.3a to your environment. stata/14 Adds Stata/MP 14 to your environment. stata/15 Adds Stata/MP 15 to your environment. supermagic/1.2/intel-2017 Adds supermagic 1.2 to your environment. Supermagic is a simple MPI sanity test. taup/2.1.2 adds TauP 2.1.2 to your environment variables tensorflow/1.4.1/cpu Adds Tensorflow 1.4.1 to your environment. tensorflow/1.4.1/gpu Adds Tensorflow 1.4.1 to your environment. tensorflow/1.4.1/mkl Adds Tensorflow 1.4.1 to your environment. tensorflow/1.8.0/cpu Adds Tensorflow 1.8.0 to your environment. tensorflow/1.8.0/gpu Adds Tensorflow 1.8.0 to your environment. tensorflow/1.8.0/mkl Adds Tensorflow 1.8.0 to your environment. tensorflow/1.12.0/cpu Adds Tensorflow 1.12.0 to your environment. tensorflow/1.12.0/gpu Adds Tensorflow 1.12.0 to your environment. tensorflow/1.12.0/mkl Adds Tensorflow 1.12.0 to your environment. tensorflow/1.13.1/cpu Adds Tensorflow 1.13.1 to your environment. tensorflow/1.13.1/gpu Adds Tensorflow 1.13.1 to your environment. tensorflow/1.13.1/mkl Adds Tensorflow 1.13.1 to your environment. tensorflow/1.14.0/cpu Adds Tensorflow 1.14.0 to your environment. tensorflow/1.14.0/gpu Adds Tensorflow 1.14.0 to your environment. tensorflow/1.14.0/mkl Adds Tensorflow 1.14.0 to your environment. tensorflow/2.0.0/gpu-py37 Adds Tensorflow 2.0.0 to your environment. tensorflow/2.0.0/gpu-py37-cudnn75 Adds Tensorflow 2.0.0 to your environment. tensorflow/2.0.0/mkl-py37 Adds Tensorflow 2.0.0 to your environment. tephra2/2.0/gnu-4.9.2 Adds Tephra2 version 2.0 to your environment. tephra2/normal/r149 Adds Tephra2 version r149 to your environment. tesseract/3.05.01 Adds Tesseract 3.05.01 to your environment. texinfo/5.2/gnu-4.9.2 Adds GNU texinfo 5.2 to your environment. texinfo/6.6/gnu-4.9.2 Adds GNU texinfo 6.6 to your environment. texlive/2014 Adds TeX Live 2014 to your environment. texlive/2015 Adds TeX Live 2015 to your environment. texlive/2019 Adds TeX Live 2019 to your environment. textract/1.5.0 Adds textract 1.5.0 to your environment. textract extracts text from a wide range of document types. tmux/2.2 This module adds the tmux 2.2 package to your environment. tophat/2.1.0 Adds Tophat 2.1.0 to your environment. tracer/1.6 Adds Tracer 1.6. tractor/3.2.5 Adds TractoR 3.2.5 to your environment. tree/1.7.0 Adds tree 1.7.0 to your environment. This shows your directory structure as a tree. trim_galore/0.4.1 Adds Trim Galore 0.4.1 to your environment. A wrapper tool around Cutadapt and FastQC to consistently apply quality and adapter trimming to FastQ files. trimmomatic/0.33 Adds Trimmomatic 0.33 to your environment. A flexible read trimming tool for Illumina NGS data. turbomole/6.4/mpi Adds turbomole 6.4 (using MPI) to your environment. turbomole/6.4/serial Adds turbomole 6.4 (serial) to your environment. turbomole/6.4/smp Adds turbomole 6.4 (using SMP) to your environment. turbomole/6.5/mpi Adds turbomole 6.5 (using MPI) to your environment. turbomole/6.5/serial Adds turbomole 6.5 (serial) to your environment. turbomole/6.5/smp Adds turbomole 6.5 (using SMP) to your environment. turbomole/6.6/mpi Adds turbomole 6.6 (using MPI) to your environment. turbomole/6.6/serial Adds turbomole 6.6 (serial) to your environment. turbomole/6.6/smp Adds turbomole 6.6 (using SMP) to your environment. ubpred/1-bin32dist UbPred is a random forest-based predictor of potential ubiquitination sites in proteins. udunits/2.2.19 Adds udunits to your environment. udunits/2.2.20/gnu-4.9.2 adds the UDUNITS-2 package to your environment. udunits/2.2.26/gnu-4.9.2 adds the UDUNITS-2 package to your environment. varscan/2.3.9 Adds VarScan v2.3.9 to your environment. VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. vasp/5.4.1-05feb16-p2/intel-2015-update2 The VASP Quantum Chemistry package, version 5.4.1-05feb16 with patches 1 and 2. vasp/5.4.1-24jun15-p2-vtst-r160/intel-2015-update2 Adds VASP 5.4.1 built with VTST r160 to your environment. vasp/5.4.1-24jun15-p08072015/intel-2015-update2 The VASP Quantum Chemistry package, version 5.4.1-24jun15 with patch 08072015. vasp/5.4.4-18apr2017-libbeef/intel-2017-update1 Adds VASP 5.4.4 with BEEF-vdW functionals to your environment. vasp/5.4.4-18apr2017-vtst-r178/intel-2017-update1 Adds VASP 5.4.4 built with VTST r178 to your environment. vasp/5.4.4-18apr2017/intel-2017-update1 Adds VASP 5.4.4 to your environment. vcftools/0.1.15/gnu-4.9.2 Adds VCFtools version 0.1.15 to your environment. Tools for working with VCF files. velvet/1.2.10 Adds Velvet 1.2.10 to your environment. vep/95.0 Adds VEP 95.0 to your environment. vesta/3.4.6-bindist VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. vg/1.11.0 This is a module with no description string. viennarna/2.1.9/gnu-4.9.2 Adds viennarna 2.1.9 to your environment. vinalc/1.1.2/gnu-4.9.2 Adds VinaLC to your environment. visit/2.9.2 This package adds VisIt 2.9.2 to your environment. VisIt is a distributed, parallel visualization and graphical analysis tool for data defined on two- and three-dimensional (2D and 3D) meshes. Visit will create a ~/.visit directory and a ~/Scratch/visit directory. Jobfiles created by the GUI will go in the latter. Legion hostfile: /shared/ucl/apps/visit/2.9.2/gnu-4.9.2/host_legion.xml Web: https://wci.llnl.gov/simulation/computer-codes/visit/ vmd/1.9.3/GL+CUDA VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. vmd/1.9.3/text-only The binary, text only version of VMD 1.9.3 vt/2018-08-01/gnu-4.9.2 [ref:f6d2b5dab73c] A tool set for short variant discovery in genetic sequence data. xmds/2.2.2 Adds XMDS 2.2.2 (GNU/ATLAS/Intel MPI/FFTW toolchain) to your environment. xtalopt/r12.1/gnu-4.9.2 Adds XtalOpt r12.1 to your environment. xulrunner/3.6.28/gnu-4.9.2 Adds XULRunner 3.6.28 to your environment. XULRunner is a Mozilla runtime package that can be used to bootstrap XUL+XPCOM applications. This version was built including javaxpcom. xulrunner/10.0.2 Adds the XULRunner 3.6.28 64-bit runtime binaries to your environment. XULRunner is a Mozilla runtime package that can be used to bootstrap XUL+XPCOM applications. yambo/4.1.4/intel-2017 This is a module with no description string.","title":"Applications"},{"location":"Installed_Software_Lists/module-packages/#libraries","text":"Modules in this section set up your environment to use specific C, C++, or Fortran libraries. This can include being able to use them with other languages, like Python. Module Description apr-util/1.5.4 adds APR-util 1.5.4 to your environment variables apr/1.5.2 adds APR 1.5.2 to your environment variables argtable/2.13 Adds argtable 2.13 to your environment. armadillo/7.400.3/intel-2015-update2 Adds armadillo 7.400.3 to your environment. Armadillo is a linear alebra library for C++, aiming to balance speed and ease of use. arpack-ng/3.4.0/intel-2015-update2 Adds arpack-ng 3.4.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. arpack-ng/3.5.0/gnu-4.9.2-serial Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. arpack-ng/3.5.0/gnu-4.9.2-threaded Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. arpack-ng/3.5.0/intel-2017 Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. atlas/3.10.2/gnu-4.9.2 adds ATLAS 3.10.2 for GCC 4.9.2 compilers to your environment variables atlas/3.10.2/intel-2015-update2 adds ATLAS 3.10.2 for Intel 15 compilers to your environment variables bambamc/0.0.50/gnu-4.9.2 Adds bambamc 0.0.50 to your environment. bambamc is a lightweight C implementation of the read name collation code from the larger libmaus/biobambam C++ project. boost/1_54_0/gnu-4.9.2 Adds Boost 1.54.0 with Python libraries to your environment. boost/1_54_0/mpi/gnu-4.9.2 Adds Boost 1.54.0 with Python and MPI libraries to your environment. boost/1_54_0/mpi/gnu-4.9.2-ompi-1.10.1 Adds Boost 1.54.0 with Python and MPI libraries to your environment. boost/1_54_0/mpi/intel-2015-update2 Adds Boost 1.54.0 with Python and MPI libraries to your environment. boost/1_63_0/gnu-4.9.2 Adds Boost 1.63.0 with Python libraries to your environment. boost/1_63_0/mpi/gnu-4.9.2 Adds Boost 1.63.0 with Python and MPI libraries to your environment. boost/1_63_0/mpi/intel-2017-update1 Adds Boost 1.63.0 with Python and Intel MPI libraries to your environment. cernlib/2006-35 Adds the CERN Program library to your environment. 2006-35 EL6 RPM binaries. cernlib/2006/gnu-4.9.2 Adds the CERN Program library to your environment cfitsio/3370/gnu-4.9.2 Adds cfitsio 3370 to your environment. cfitsio/3370/intel-2015-update2 Adds cfitsio 3370 to your environment. cgal/4.9/gnu-4.9.2 Adds CGAL 4.9 with Qt5 to your environment. The Computational Geometry Algorithms Library. clusteringsuite/2.6.6/bindist Adds clusteringsuite 2.6.6 to your environment. Clustering Suite is a set of tools to automatically expose the main performance trends in applications' computation structure. cunit/2.1-3/gnu-4.9.2 Adds cunit 2.1-3 to your environment. CUnit is a package for writing and running unit tests in C. cvmfs/2.2.1/gnu-4.9.2 Adds libcvmfs 2.2.1 to your environment. dyninst/9.3.2/gnu-4.9.2 Adds dyninst 9.3.2 to your environment. DynInst is a library for performing dynamic instrumentation of executables. eigen/3.2.5/gnu-4.9.2 adds Eigen for GCC 4.9.2 compilers to your environment variables elfutils/0.170/gnu-4.9.2 Adds elfutils 0.170 to your environment. Elfutils provides utilities for manipulating binary ELF files, and is one possible provider of libelf. fftw/2.1.5/gnu-4.9.2 adds FFTW 2.1.5 for GCC 4.9.2 compilers to your environment variables fftw/2.1.5/intel-2015-update2 adds FFTW 2.1.5 for Intel compilers to your environment variables fftw/3.3.4-impi/gnu-4.9.2 Adds fftw 3.3.4 (built with Intel MPI) to your environment. fftw/3.3.4-impi/intel-2017-update1 Adds fftw 3.3.4 (built with Intel MPI) to your environment. fftw/3.3.4-ompi-1.10.1/gnu-4.9.2 Adds fftw 3.3.4 (built with OpenMPI) to your environment. fftw/3.3.4-ompi/gnu-4.9.2 Adds fftw 3.3.4 (built with OpenMPI) to your environment. fftw/3.3.4-threads/gnu-4.9.2 adds FFTW 3.3.4 for GCC 4.9.2 compilers to your environment variables fftw/3.3.4/gnu-4.9.2 adds FFTW 3.3.4 for GCC 4.9.2 compilers to your environment variables fftw/3.3.4/intel-2015-update2 adds FFTW 3.3.4 for Intel compilers to your environment variables fftw/3.3.6-pl2/gnu-4.9.2 Adds FFTW 3.3.6 pl2 for GCC 4.9.2 compilers to your environment variables. Includes single and double precision only on Legion, plus long-double and quad on Grace/Thomas. Includes OpenMP and POSIX threads libraries. fftw/3.3.6-pl2/intel-2017 Adds FFTW 3.3.6 pl2 for Intel compilers to your environment variables. Includes single and double precision versions on Legion, plus long-double on Grace/Thomas. Includes OpenMP and POSIX threads libraries. fftw/3.3.8-impi/intel-2018 Adds fftw fftw (built with Intel MPI) to your environment. fftw/3.3.8-ompi/gnu-4.9.2 Adds fftw 3.3.8 (built with OpenMPI) to your environment. forge/1.0.0/gnu-4.9.2 Adds forge 1.0.0 to your environment. freeimage/3.17.0/gnu-4.9.2 Adds FreeImage 3.17.0 to your environment. freetype/2.8.1/gnu-4.9.2 Adds freetype 2.8.1 to your environment. FreeType is a freely available software library to render fonts. ga/5.7-8BInts/intel-2018 Global Arrays (GA) is a library that provides a Partitioned Global Address Space (PGAS) programming model. This version has been compiled with 8-byte integers in the Fortran code. ga/5.7/intel-2018 Global Arrays (GA) is a library that provides a Partitioned Global Address Space (PGAS) programming model. gcc-libs/4.9.2 adds GCC 4.9.2 runtime to your evironment. geos/3.5.0/gnu-4.9.2 Adds geos 3.5.0 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. gflags/2.2.1 Adds Google gflags 2.2.1 to your environment. giflib/5.1.1 Adds giflib 5.1.1 to your environment. A library and utilities for processing gifs. glbinding/2.1.2/gnu-4.9.2 Adds glbinding 2.1.2 to your environment. glew/1.13.0/gnu-4.9.2 Adds GLEW The OpenGL Extension Wrangler Library 1.11.0 to your environment. glfw/3.2.1/gnu-4.9.2 Adds GLFW 3.2.1 to your environment. glog/0.3.5 Adds Google glog 0.3.5 to your environment. glpk/4.60/gnu-4.9.2 Adds the GNU Linear Programming Kit Version 4.60 for GCC 4.9.2 to your environment. gsl/1.16/gnu-4.9.2 adds GSL 1.16 for GCC 4.9.2 to your environment. gsl/1.16/intel-2015-update2 Adds gsl 1.16 to your environment. gsl/2.4/gnu-4.9.2 adds GSL 2.4 for GCC 4.9.2 to your environment. gsl/2.4/intel-2017 adds GSL 2.4 for Intel 2017 to your environment. gstreamer/1.12.0 GStreamer is a library for constructing graphs of media-handling components, including codecs for various audio and video formats. gulp/4.5/libgulp/intel-2018 Adds GULP 4.5 library version to your environment. Built libgulp only, without FoX, for programs such as ChemShell to link. GULP is a materials simulation code. h5py/2.10.0-ompi/gnu-4.9.2 Adds h5py 2.10.0-ompi for Python 3.7 to your environment. harminv/1.4.1/gnu-4.9.2 Adds harminv 1.4.1 to your environment. harminv/1.4/gnu-4.9.2 Adds harminv 1.4 to your environment. hdf/5-1.8.15-p1-impi/intel-2015-update2 Adds hdf5 1.8.5-p1 (built with Fortran and IntelMPI options) to your environment. hdf/5-1.8.15-p1-ompi/gnu-4.9.2 Adds hdf5 1.8.5-p1 (built with Fortran and OpenMPI options) to your environment. hdf/5-1.8.15/gnu-4.9.2 adds HDF5 1.8.15 (Serial) for GCC 4.9.2 to your environment. hdf/5-1.8.15/intel-2015-update2 adds HDF5 1.8.15 (Serial) for Intel 2015 to your environment. hdf/5-1.10.2-impi/intel-2018 adds HDF5 1.10.2 (Parallel) for Intel 2018 to your environment. hdf/5-1.10.2/intel-2018 adds HDF5 1.10.2 (Serial) for Intel 2018 to your environment. hdf/5-1.10.5-ompi/gnu-4.9.2 Adds hdf 5-1.10.5-ompi to your environment. Built with OpenMPI and GNU. hdf/5-1.10.5/gnu-4.9.2 Adds hdf 5-1.10.5 to your environment. Serial version built with GNU. htslib/1.2.1 This module adds the HTSlib 1.2.1 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. htslib/1.3.1 This module adds the HTSlib 1.3.1 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. htslib/1.7 This module adds the HTSlib 1.7 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. hwloc/1.11.12 The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. This installation includes the optional libnuma dependency. hypre/2.11.2/openmpi-3.0.0/intel-2017 Adds HYPRE 2.11.2 to your environment. hypre/2.11.2/openmpi-3.1.1/intel-2018 Adds HYPRE 2.11.2 to your environment. jansson/2.11 This is a module with no description string. json-c/0.12/gnu-4.9.2 Adds json-c 0.12 to your environment. JSON-C is a library for converting between JSON-formatted strings and C representations of the equivalent objects. lapack/3.8.0/gnu-4.9.2 LAPACK is a reference library of routines for Linear Algebra. It is not recommended for use, as its ABI is replicated in the much higher-performance libraries OpenBLAS, MKL, or ATLAS instead. leptonica/1.74.4 Adds Leptonica 1.74.4 to your environment. leveldb/1.20 Adds Google leveldb 1.20 to your environment. libbdwgc/7.4.2/gnu-4.9.2 Adds libbdwgc (a garbage-collector library) to your environment. libbeef/0.1.3/intel-2018 Library for Bayesian error estimation functionals for use in density functional theory codes: libbeef 0.1.3 commit 2822afe libctl/3.2.2/gnu.4.9.2 Adds libctl (built using Intel compilers) to your environment. libctl/4.3.0/gnu-4.9.2 Adds libctl 4.3.0 to your environment. libdwarf/20170709/gnu-4.9.2 Adds libdwarf 20170709 to your environment. libdwarf is a library for interacting with debugging info in the DWARF 2, 3, and 5 formats. libelf/0.8.13/gnu-4.9.2 Adds libelf 0.8.13 to your environment. libetsfio/1.0.4/gnu-4.9.2 Adds libetsfio 1.0.4 to your environment. libetsfio/1.0.4/intel-2015-update2 Adds libetsfio 1.0.4 to your environment. libflac/1.3.1/gnu-4.9.2 Adds libflac 1.3.1 to your environment. libFLAC is the Xiph library for handling their lossless audio codec. libgd/2.1.1/gnu-4.9.2 Adds libgd 2.1.1 to your environment. libgd/2.1.1/intel-2015-update2 Adds libgd 2.1.1 to your environment. libgdsii/0.21/gnu-4.9.2 Adds libgdsii 0.21 to your environment. C++ library and command-line utility for reading GDSII geometry files. libint/1.1.4/gnu-4.9.2 Adds libint 1.1.4 to your environment. Libint is required for CP2K. libmatheval/1.1.11 Adds libmatheval 1.1.11 to your environment. GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libsodium/1.0.6/gnu-4.9.2 Adds libsodium 1.0.6 to your environment. libsodium is a crypto library primarily used by ZeroMQ. libsox/14.4.2/gnu-4.9.2 Adds libsox 14.4.2 to your environment. SoX is a library for reading, writing, and converting a variety of sound file formats. If you require support for a file format that is not installed, contact rc-support and the library can be rebuilt. libuuid/1.0.3/gnu-4.9.2 Adds a static libuuid 1.0.3 to your environment. libxc/2.1.2/intel-2015-update2 Adds libxc 2.1.2 to your environment. libxc/2.2.2/gnu-4.9.2 Adds libxc 2.2.2 to your environment. libxc/2.2.2/intel-2015-update2 Adds libxc 2.2.2 to your environment. libxc/3.0.0/gnu-4.9.2 Adds libxc 3.0.0 to your environment. libxc/3.0.0/intel-2015-update2 Adds libxc 3.0.0 to your environment. libxc/4.2.3/intel-2018 libxc is a library of routines implementing a range of exchange-correlation functionals for density-functional theory calculations. libxml2/2.9.4/gnu-4.9.2 Adds libxml2 2.9.4 to your environment. Libxml2 is an XML C parser and toolkit. Includes Python (2.7.9) bindings. llvm/3.3 This module adds the LLVM 3.3 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. llvm/3.9.1 This module adds the LLVM 3.9.1 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. llvm/6.0.1 This module adds the LLVM 6.0.1 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. llvm/8.0.0/gnu-4.9.2 The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. This installation includes clang, a C compiler based on LLVM. lmdb/0.9.22 Adds LMDB 0.9.22 to your environment. lz4/1.8.3 This is a module with no description string. magma/2.4.0 This is a module with no description string. med/4.0.0/gnu-4.9.2 Adds med 4.0.0 to your environment. Allows reading and writing of MED format files. mesa/6.5/gnu-4.9.2 Adds mesa 6.5 to your environment. Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. This is an old version installed to satisfy a particular dependency: please do not use for new builds. mesa/10.6.3 Adds Mesa 10.6.3 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built for offscreen rendering: OSMesa, Xlib GLX, no Gallium, no EGL, no llvm, no DRI). mesa/10.6.9/gnu-4.9.2 Adds Mesa 10.6.9 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built for offscreen rendering: OSMesa, Xlib GLX, no Gallium, no EGL, no llvm, no DRI). mesa/13.0.6/gnu-4.9.2 Adds Mesa 13.0.6 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built options: Gallium, LLVM, no EGL, no DRI, no GLX). The default driver is llvmpipe. You can use \"export GALLIUM_DRIVER\" to explicitly choose llvmpipe, softpipe, or swr metis/5.1.0/gnu-4.9.2 Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. metis/5.1.0/intel-2015-update2 Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. mpi/intel/2013/update1/intel adds Intel MPI 4.1.3.048 to your environment variables mpi/intel/2015/update3/gnu-4.9.2 adds Intel MPI to your environment variables mpi/intel/2015/update3/intel adds Intel MPI to your environment variables mpi/intel/2017/update1/gnu-4.9.2 adds Intel MPI to your environment variables mpi/intel/2017/update1/intel adds Intel MPI to your environment variables mpi/intel/2017/update2/gnu-4.9.2 adds Intel MPI to your environment variables configured to use GCC 4.9.2 mpi/intel/2017/update2/intel adds Intel MPI to your environment variables mpi/intel/2017/update3/gnu-4.9.2 [Intel MPI/2017.3.196] This is Intel's MPI implementation, version 2017.3.196, which is bundled with compiler package version 2017.Update4. This module sets up the compiler wrappers to use GCC 4.9.2 underneath. mpi/intel/2017/update3/intel [Intel MPI/2017.3.196] This is Intel's MPI implementation, version 2017.3.196, which is bundled with compiler package version 2017.Update4. mpi/intel/2018/update3/intel [Intel MPI/2018.3.222] This is Intel's MPI implementation, version 2018.3.222, which is bundled with compiler package version 2018.Update3. mpi/intel/2019/update4/intel [Intel MPI/2019.4.243] This is Intel's MPI implementation, version 2019.4.243, which is bundled with compiler package version 2019.Update4. mpi/intel/2019/update5/intel [Intel MPI/2019.5.281] This is Intel's MPI implementation, version 2019.5.281, which is bundled with compiler package version 2019.Update5. mpi/intel/2019/update6/intel [Intel MPI/2019.6.166] This is Intel's MPI implementation, version 2019.6.166, which is bundled with compiler package version 2020. mpi/openmpi/1.8.4/gnu-4.9.2 adds OpenMPI 1.8.4 for GCC 4.9.2 compilers to your environment variables mpi/openmpi/1.8.4/intel-2015-update2 adds OpenMPI 1.8.4 for Intel 2015 update 2 compilers to your environment variables mpi/openmpi/1.10.1/gnu-4.9.2 adds OpenMPI 1.10.1 for GCC 4.9.2 compilers to your environment variables mpi/openmpi/1.10.1/intel-2015-update2 adds OpenMPI 1.10.1 for Intel 2015 update 2 compilers to your environment variables mpi/openmpi/2.0.2/gnu-4.9.2 Adds openmpi 2.0.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/2.0.2/intel-2017 Adds openmpi 2.0.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/2.1.2/gnu-4.9.2 Adds openmpi 2.1.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/2.1.2/intel-2017 Adds openmpi 2.1.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.0.0/gnu-4.9.2 Adds openmpi 3.0.0 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.0.0/intel-2017 Adds openmpi 3.0.0 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.1/gnu-4.9.2 Adds openmpi 3.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.1/intel-2018 Adds openmpi 3.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.4/gnu-4.9.2 Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.4/intel-2018 Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.6/gnu-4.9.2 Adds openmpi 3.1.6 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/4.0.3/gnu-4.9.2 Adds openmpi 4.0.3 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi4py/2.0.0/python2 Adds Python2 mpi4py 2.0.0 to your environment. MPI for Python. mpi4py/2.0.0/python3 Adds Python3 mpi4py 2.0.0 to your environment. MPI for Python. mpi4py/3.0.0/python3 Adds Python3 mpi4py 3.0.0 to your environment. MPI for Python. mpi4py/3.0.2/gnu-4.9.2 Adds mpi4py 3.0.2 for Python 3.7 to your environment. mumps/5.2.1/intel-2018 Adds mumps 5.2.1 to your environment. Sequential version built with Intel and METIS. mysql-connector-python/2.0.4/python-3.5.2 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 mysql-connector-python/2.0.4/python-3.6.3 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 mysql-connector-python/2.0.4/python-3.7.4 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 mysql-connector-python/2.0.4/python-3.8.0 Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 nag/fortran/mark22/gnu-4.9.2 Adds NAG Fortran Library Mark 22 for GCC to your environment. nag/fortran/mark24/gnu-4.9.2 Adds NAG Fortran Library Mark 24 for GCC to your environment. nag/fortran/mark24/nag-6.0.1044 Adds NAG Fortran Library Mark 24 for NAG Fortran to your environment. nag/fortran/mark25/intel-2015-update2 Adds NAG Fortran Library Mark 25 for Intel 2015 to your environment. nag/fortran/mark26/gnu-4.9.2 Adds NAG Fortran Library Mark 26 for GCC to your environment. nag/fortran/mark26/intel-2017 Adds NAG Fortran Library Mark 26 for Intel 2017 to your environment. nag/fortran/mark26/nag-6.1.6106 Adds NAG Fortran Library Mark 26 for NAG Fortran to your environment. nag/fortran/mark26/nag-6.2.6223 Adds NAG Fortran Library Mark 26 for NAG Fortran to your environment. nag/mark27/intel-2019 Adds NAG Library Mark 27 for Intel 2019 to your environment. netcdf-c++/4.2/gnu-4.9.2 adds NetCDF C++ 4.2 for GCC to your environment. netcdf-c++/4.2/intel-2015-update2 adds NetCDF C++ 4.2 for Intel 2015 to your environment. netcdf-c++4/4.2/gnu-4.9.2 adds NetCDF C++ 4.2 for GCC to your environment. netcdf-c++4/4.2/intel-2015-update2 adds NetCDF C++ 4.2 for Intel 2015 to your environment. netcdf-fortran/4.4.1/gnu-4.9.2 adds NetCDF 4.4.1 for GCC to your environment. netcdf-fortran/4.4.1/intel-2015-update2 adds NetCDF 4.4.1 for Intel 2015 to your environment. netcdf/4.3.3.1/gnu-4.9.2 adds NetCDF 4.3.3.1 for GCC 4.9.2 to your environment. netcdf/4.3.3.1/intel-2015-update2 adds NetCDF 4.3.3.1 for Intel 2015 to your environment. numactl/2.0.12 numactl provides NUMA policy support, as well as tools and a library to display NUMA allocation statistics and debugging information. openblas/0.2.14-threads/gnu-4.9.2 adds OpenBLAS 0.2.14 for GCC 4.9.2 compilers to your environment variables openblas/0.2.14/gnu-4.9.2 adds OpenBLAS 0.2.14 for GCC 4.9.2 compilers to your environment variables openblas/0.2.14/intel-2015-update2 adds OpenBLAS 0.2.14 for Intel 2015 update 2compilers to your environment variables openblas/0.3.2-native-threads/gnu-4.9.2 This is a module with no description string. openblas/0.3.2-openmp/gnu-4.9.2 This is a module with no description string. openblas/0.3.2-serial/gnu-4.9.2 This is a module with no description string. openblas/0.3.7-native-threads/gnu-4.9.2 This is a module with no description string. openblas/0.3.7-openmp/gnu-4.9.2 This is a module with no description string. openblas/0.3.7-serial/gnu-4.9.2 This is a module with no description string. papi/5.5.1/gnu-4.9.2 Adds PAPI 5.5.1 to your environment. PAPI is a library for working with performance counters, often used in profiling applications. parmetis/4.0.3/intel-2015-update2 Adds parmetis 4.0.3 to your environment. ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. pcre2/10.21/gnu-4.9.2 Adds pcre2 10.21 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. pgplot/5.2.2/intel-2017 Adds PGPlot 5.2.2 to your environment. pgplot/5.2.2/intel-2018 Adds PGPlot 5.2.2 to your environment. pillow-simd/6.0.0.post0/python-3.7.4 Adds Pillow-SIMD to your environment. protobuf/3.5.1/gnu-4.9.2 adds Google Protocol Buffers for GCC 4.9.2 to your environment. protobuf/12-2017/gnu-4.9.2 adds Google Protocol Buffers for GCC 4.9.2 to your environment. pstreams/1.0.1/gnu-4.9.2 Adds pstreams 1.0.1 to your environment. PStreams is a C++ wrapper for process control and streaming using popen and pclose. pygsl/2.1.1-python3.6/gnu-4.9.2 Adds pygsl 2.1.1 to your environment. PyGSL provides Python bindings for the GNU Scientific Library. pyngl/1.4.0 Adds PyNGL to your environment. pynio/1.4.1 Adds PyNIO to your environment. quip/18c5440-threads/gnu-4.9.2 Adds QUIP to your environment. QUIP is required for CP2K. quip/18c5440/gnu-4.9.2 Adds QUIP to your environment. QUIP is required for CP2K. qutip/4.1.0/python-2.7.12 Adds qutip to your environment. scalapack/2.0.2/gnu-4.9.2/openblas Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. scalapack/2.0.2/gnu-4.9.2/openblas-0.3.2 Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. scalapack/2.0.2/gnu-4.9.2/openblas-0.3.7 Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. snappy/1.1.7 Adds Google snappy 1.1.7 to your environment. sparskit2/2009.11.18/gnu-4.9.2 Adds sparskit2 2009.11.18 to your environment. sparskit2/2009.11.18/intel-2015-update2 Adds sparskit2 2009.11.18 to your environment. spectral/3.4.0/bindist Adds spectral 3.4.0 to your environment. Spectral is a set of tools for performing spectral analysis on traces produced by the BSC profiling toolkit. spglib/1.7.4/gnu-4.9.2 Adds spglib 1.7.4 to your environment. Spglib is a library for finding and handling crystal symmetries written in C. squid/1.9g/gnu-4.9.2 Adds squid 1.9g to your environment. suitesparse/4.5.5/gnu-4.9.2-serial Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. suitesparse/4.5.5/gnu-4.9.2-threaded Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. suitesparse/4.5.5/intel-2017-update1 Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. superlu-dist/5.1.0/intel-2015-update2 Adds superlu-dist 5.1.0 to your environment. SuperLU_DIST is the distributed-memory parallel version of SuperLU, a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. superlu/5.2.1/intel-2015-update2 Adds superlu 5.2.1 to your environment. SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. szip/2.1 Adds szip to your environment. ucx/1.8.0/gnu-4.9.2 Adds ucx 1.8.0 to your environment. Unified Communication X (UCX) provides an optimized communication layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric applications. unixodbc/2.3.7 Unix ODBC driver vtk/5.10.1/gnu-4.9.2 adds VTK 5.10.1 for GCC 4.9.2 to your environment. vtk/6.2.0/gnu-4.9.2 adds VTK 6.2.0 for GCC 4.9.2 to your environment. wavpack/5.1.0/gnu-4.9.2 WavPack is a completely open audio compression format providing lossless, high-quality lossy, and a unique hybrid compression mode. webkitgtk/2.2.4-1 Adds the webkitgtk-1 with webkitgtk-devel library to your environment. 2.2.4-1 EL7 RPM binaries. webkitgtk/2.4.9-1 Adds the webkitgtk with webkitgtk-devel library to your environment. 2.4.9-1 EL7 RPM binaries. zeromq/4.1.4/gnu-4.9.2 Adds zeromq 4.1.4 to your environment. ZeroMQ is a distributed messaging library that supports many message-passing patterns and methods.","title":"Libraries"},{"location":"Installed_Software_Lists/module-packages/#development-tools","text":"This section is for modules for programs that are used in software development, profiling, or troubleshooting. It also contains language interpreters, like Python, Ruby, and Java. Module Description autoconf/2.69 Adds GNU Autoconf Version 2.69 to your environment. autogen/5.18.12/gnu-4.9.2 AutoGen is a tool designed to simplify the creation and maintenance of programs that contain large amounts of repetitious text. automake/1.16.1 Adds GNU Automake Version 1.16.1 to your environment. bazel/0.7.0 Adds bazek to your environment. bazel/0.14.1/gnu-4.9.2 Adds bazek to your environment. bazel/0.21.0/gnu-4.9.2 Adds bazek to your environment. bazel/0.24.0/gnu-4.9.2 Adds bazek to your environment. bazel/0.24.1/gnu-4.9.2 Adds bazek to your environment. bazel/0.26.1/gnu-4.9.2 Adds bazek to your environment. bazel/0.27.1/gnu-4.9.2 Adds bazek to your environment. binutils/2.29.1/gnu-4.9.2 Adds binutils 2.29.1 to your environment. The GNU binutils are a collection of tools for working with binary files and assembling and disassembling machine instructions. bison/3.0.4/gnu-4.9.2 Adds Bison 3.0.4 to your environment. Bison is a general-purpose parser generator. chicken/4.13.0 adds Chicken 4.13.0 to your environment variables clojure/1.10.0.411 This is a module with no description string. cmake/3.2.1 adds Cmake 3.2.1 compilers to your environment variables cmake/3.7.2 adds Cmake 3.7.2 compilers to your environment variables cmake/3.13.3 adds Cmake 3.13.3 compilers to your environment variables cuda/7.5.18/gnu-4.9.2 Adds cuda 7.5.18 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/8.0.61-patch2/gnu-4.9.2 Adds cuda 8.0.61 patch2 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/9.0.176-patch4/gnu-4.9.2 Adds cuda 9.0.176 patch4 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/10.0.130/gnu-4.9.2 Adds cuda 10.0.130 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cuda/10.1.243/gnu-4.9.2 Adds cuda 10.1.243 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. cudnn/5.1/cuda-7.5 Adds cuDNN to your environment. cudnn/5.1/cuda-8.0 Adds cuDNN to your environment. cudnn/6.0/cuda-7.5 Adds cuDNN to your environment. cudnn/6.0/cuda-8.0 Adds cuDNN to your environment. cudnn/7.0.4/cuda-8.0 Adds cuDNN to your environment. cudnn/7.1.4/cuda-9.0 Adds cuDNN to your environment. cudnn/7.4.2.24/cuda-9.0 Adds cuDNN to your environment. cudnn/7.4.2.24/cuda-10.0 Adds cuDNN to your environment. cudnn/7.5.0.56/cuda-10.0 Adds cuDNN to your environment. cudnn/7.5.0.56/cuda-10.1 Adds cuDNN to your environment. cudnn/7.6.5.32/cuda-10.0 Adds cuDNN to your environment. cudnn/7.6.5.32/cuda-10.1 Adds cuDNN to your environment. ddt/6.0.4 This module adds DDT 6.0.4 with MIC support to your environment. depot_tools/788d9e0d adds depot_tools to your environment variables depot_tools/c03a9cf adds depot_tools to your environment variables dimemas/5.3.3/bindist Adds dimemas 5.3.3 to your environment. Dimemas is an abstracted network simulator for message-passing programs. doxygen/1.8.14 This is a module with no description string. emacs/24.5 Adds Emacs 24.5 to your environment. An extensible text editor. emacs/26.3 Adds Emacs 26.3 to your environment. An extensible text editor. extrae/3.5.2/intel-2017 Adds extrae 3.5.2 to your environment. Extrae is an instrumentation framework to generate execution traces of the most used parallel runtimes. f2c/2013-09-26/gnu-4.9.2 Adds f2c 2013-09-26 to your environment. f2c is a source-to-source translator from Fortran 77 to C. It is not standards-compliant and is not recommended for use under any circumstances. flex/2.5.39 adds Flex 2.4.39 to your environment variables git/2.3.5 adds Git 2.3.5 to your environment variables git/2.10.2 adds Git 2.10.2 to your environment variables git/2.19.1 adds Git 2.19.1 to your environment variables gperf/3.0.4/gnu-4.9.2 Adds gperf 3.0.4 to your environment. GNU gperf is a perfect hash function generator. guile/2.0.11/gnu-4.9.2 Adds guile 2.0.11 to your environment. haskellplatform/2014.2.0.0 adds Haskell Platform to your environment variables htop/1.0.3/gnu-4.9.2 Adds htop 1.0.3 to your environment. java/1.8.0_45 adds Oracle JDK 1.8.0_45 compilers to your environment variables java/1.8.0_92 adds Oracle JDK 1.8.0_92 compilers to your environment variables java/openjdk-8/8u212/hotspot adds Oracle JDK 8 compilers to your environment variables java/openjdk-8/8u212/openj9 adds Oracle JDK 8 compilers to your environment variables java/openjdk-11/11.0.1 adds Oracle JDK 11.0.1 compilers to your environment variables java/openjdk-11/11.0.3u7/hotspot adds Oracle JDK 11.0.3 compilers to your environment variables java/openjdk-11/11.0.3u7/openj9 adds Oracle JDK 11.0.3 compilers to your environment variables julia/0.3.10 adds Julia 0.3.10 to your environment variables julia/0.4.0 adds Julia 0.4.0 to your environment variables julia/0.4.7 adds Julia 0.4.7 to your environment variables julia/0.5.0 adds Julia 0.5.0 to your environment variables julia/0.6.0 adds Julia 0.6.0 to your environment variables julia/0.7.0 adds Julia 0.7.0 to your environment variables julia/1.0.0 adds Julia 1.0.0 to your environment variables julia/1.1.0 adds Julia 1.1.0 to your environment variables julia/1.2.0 adds Julia 1.2.0 to your environment variables julia/1.3.1 adds Julia 1.3.1 to your environment variables libtool/2.4.6 Adds libtool 2.4.6 to your environment. GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. ltrace/0.7.3/gnu-4.9.2 Adds ltrace 0.7.3 to your environment. lua/5.3.1 This module adds the Lua 5.3.1 package to your environment. Lua is a powerful, fast, lightweight, embeddable scripting language. mc/4.8.14 This module adds Midnight Commander 4.8.14 to your environment. mono/3.12.1 adds Mono 3.12.1 compilers to your environment variables mono/5.20.1.27/gnu-4.9.2 This is a module with no description string. nano/2.4.2 Adds nano 2.4.2 to your environment. A simple text editor. nano/4.9 The nano text editor. nasm/2.13.01 The Netwide Assembler, NASM, is an 80x86 and x86-64 assembler. ncl/6.0.0 adds NCL 6.0.0 to your environment variables ncl/6.3.0 adds NCL 6.3.0 to your environment variables nedit/5.6-aug15 Adds the NEdit GUI text editor to your environment. netlogo/6.1.0 adds NetLogo tooklit compilers to your environment variables paraver/4.6.4.rc1/bindist Adds paraver 4.6.4.rc1 to your environment. Paraver is a trace visualizer for post-mortem trace analysis. perl/5.16.0 This module adds adds Perl 5.16.0 to your environment. perl/5.22.0 This module adds adds Perl 5.22.0 to your environment. perlbrew/0.73 This module adds the Perlbrew 0.73 package to your environment. Use Perlbrew to manage your own Perls and Perl modules pigz/2.4 pigz is a fully functional replacement for gzip that exploits multiple processors and multiple cores when compressing data. pycuda/2017.1/python2 Adds Python2 PyCuda to your environment. MPI for Python. pycuda/2017.1/python3 Adds Python3 PyCuda to your environment. MPI for Python. pypy3/6.0.0/gnu-4.9.2 Pypy is a JIT-ing interpreter for the Python language. This is the version intended to be compatible with CPython 3.5. python/2.7.9 adds Python 2.7.9 with pip and virtualenv to your environment variables python/2.7.12 adds Python 2.7.12 with pip and virtualenv to your environment variables python/3.4.3 adds Python 3.4.3 with pip and virtualenv to your environment variables python/3.5.2 adds Python 3.5.2 with pip and virtualenv to your environment variables python/3.6.1/gnu-4.9.2 Adds Python 3.6.1 with pip and virtualenv to your environment variables. python/3.6.3 Adds Python 3.6.3 with pip and virtualenv to your environment variables. python/3.7.0 Adds Python 3.7.0 with pip and virtualenv to your environment variables. python/3.7.2 Adds Python 3.7.2 with pip and virtualenv to your environment variables. python/3.7.4 Adds Python 3.7.4 with pip and virtualenv to your environment variables. python/3.8.0 Adds Python 3.8.0 with pip and virtualenv to your environment variables. python/idp3/2019/3.6.8 Adds Intel Distribution for Python to your environment variables. python/miniconda3/4.5.11 Adds Miniconda 4.5.11 to your environment variables. qt/4.8.6/gnu-4.9.2 Adds Qt 4.8.6 to your environment. Qt is a cross-platform development tool. qt/5.4.2/gnu-4.9.2 Adds Qt 5.4.2 to your environment. Qt is a cross-platform development tool. qt/5.12.1/gnu-4.9.2 Adds Qt 5.12.1 to your environment. Qt is a cross-platform development tool. qwt/6.1.4/gnu-4.9.2 Adds Qwt 6.1.4 to your environment. racket/6.8 Adds Racket 6.8 to your enviroment. rappture/20130903 Adds the Rappture toolkit to your environment. ruby/2.2.2 Ruby 2.2.2 with RubyGems 2.4.8 and libffi 3.2.1 ruse/1.0.1 A command-line utility to periodically measure the memory use of a process and its subprocesses. sbcl/1.3.19 Adds Steelbank Common LISP 1.3.19 to your environment. scons/2.3.4 adds scons 2.3.4 to your environment variables strace/4.12 Adds strace 4.12 to your environment. Trace system calls and signals. subversion/1.8.13 adds Subversion 1.8.13 to your environment variables swig/3.0.5/gnu-4.9.2 This module adds the SWIG 3.0.5 package to your environment. SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. swig/3.0.7/gnu-4.9.2 This module adds the SWIG 3.0.7 package to your environment. SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. tcl/8.6.8 This is a modulefile for Tcl/Tk 8.6.8 v8/3.15 adds v8 to your environment variables v8/5.6 adds v8 to your environment variables valgrind/3.11.0/gnu-4.9.2 Adds valgrind 3.11.0 to your environment. Valgrind is a framework for building dynamic analysis tools. It includes the memgrind and cachegrind tools. xbae/4.60.4 Adds the Xbae Matrix Widget to your environment. xorg-utils/X11R7.7 Adds xorg-utils from X11R7.7 to your environment. Includes util-macros-1.17, makedepend-1.0.5 libXdmcp-1.1.1 and libXScrnSaver-1.2.2 and imake-1.0.7.","title":"Development Tools"},{"location":"Installed_Software_Lists/module-packages/#core-modules","text":"These modules refer to groups of system tools, rather than applications. They're intended to help you use the system, and some are loaded by default. Module Description gerun adds gerun wrapper to your environment variables lm-utils/1.0 adds utilities to check license manager status to your environment. mrxvt/0.5.4 Adds Mrxvt a multi-tabbed xterm replacement to your environment. ops-tools/1.0.0 Tools for Ops work ops-tools/1.1.0 Tools for Ops work ops-tools/2.0.0 Tools for Ops work rcps-core/1.0.0 adds a core set of applications and libraries to your environment. rlwrap/0.43 adds rlwrap 0.43 to your environment variables screen/4.2.1 adds Screen 4.2.1 to your environment variables userscripts/1.0.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.1.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.2.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.3.0 Adds userscripts dir to your path. Provides jobhist among other utilities. userscripts/1.4.0 Adds userscripts dir to your path. Provides jobhist among other utilities.","title":"Core Modules"},{"location":"Installed_Software_Lists/module-packages/#beta-modules","text":"This section is for modules we're still trying out. They may or may not work with applications from other sections. Module Description compilers/gnu/7.3.0 The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). compilers/gnu/8.3.0 The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). compilers/gnu/9.2.0 The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). cuda/10.1.243/gnu-7.3.0 Adds cuda 10.1.243 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. fftw/3.3.8/gnu-9.2.0 Adds FFTW 3.3.8 for GCC 9.2.0 compilers to your environment variables. Includes single and double precision, plus long-double and quad. Includes OpenMP and POSIX threads libraries. gcc-libs/7.3.0 Base module for gcc 7.3.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). gcc-libs/8.3.0 Base module for gcc 8.3.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). gcc-libs/9.2.0 Base module for gcc 9.2.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). gdal/3.0.4/gnu-9.2.0 adds GDAL 3.0.4 with PROJ.4 7.0.0 to your environment variables. geos/3.8.1/gnu-9.2.0 Adds geos 3.8.1 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. gmt/6.0.0/gnu-9.2.0 adds GMT 6.0.0 to your environment variables gromacs/2020.1/cuda-10.1 Adds gromacs 2020.1 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gromacs/2020.1/intel-2020 Adds gromacs 2020.1 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. gsl/2.6/gnu-9.2.0 adds GSL 2.6 for GCC 9.2.0 to your environment. hdf/5-1.10.5/gnu-9.2.0 Adds hdf 5-1.10.5 to your environment. Serial version built with GNU. libpng/1.6.37/gnu-9.2.0 Adds libpng 1.6.37 to your environment. matlab/full/r2018a/9.4-prefdir-fix Adds Matlab R2018a to your environment. med/4.0.0/gnu-9.2.0 Adds med 4.0.0 to your environment. Allows reading and writing of MED format files. mpi/openmpi/3.1.4/gnu-7.3.0 Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mpi/openmpi/3.1.5/gnu-9.2.0 Adds openmpi 3.1.5 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. mumps/5.2.1/gnu-9.2.0 Adds mumps 5.2.1 to your environment. Sequential (threaded) version built with GNU, OpenBLAS and METIS. namd/2.13/intel-2018-update3/testing Adds NAMD 2.13 to your environment namd/2.13/plumed/intel-2018-update3/testing Adds NAMD 2.13 to your environment netcdf/4.7.4/gnu-9.2.0 adds NetCDF 4.7.4 for GCC 9.2.0 to your environment. openblas/0.3.7-native-threads/gnu-9.2.0 OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. openblas/0.3.7-openmp/gnu-9.2.0 OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. openblas/0.3.7-serial/gnu-9.2.0 OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. pcre2/10.35/gnu-9.2.0 Adds pcre2 10.35 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. proj.4/7.0.0/gnu-9.2.0 Adds the PROJ.4 Cartographic Projections library to your environment. r/3.6.3-openblas/gnu-9.2.0 Adds R 3.6.3 and Bioconductor 3.10 to your environment. r/4.0.2-openblas/gnu-9.2.0 Adds R 4.0.2 and Bioconductor 3.11 to your environment. r/r-3.6.3_bc-3.10 adds UCL recommended set of R packages to your environment variables stata/16 Adds Stata/MP 16 to your environment. udunits/2.2.26/gnu-9.2.0 adds the UDUNITS-2 package to your environment.","title":"Beta Modules"},{"location":"Installed_Software_Lists/module-packages/#workaround-modules","text":"Sometimes we'll find a problem that can't be fixed properly, but can be worked-around by doing something that can be loaded as a module. That kind of module goes in this section. Module Description bazel-compiler-helpers/intel-2018 Adds bazel compiler wrappers to your environment. getcwd-autoretry This module uses LD_PRELOAD to shadow the getcwd function with a version that retries on failure, and is intended to workaround a bug in the Lustre filesystem.","title":"Workaround Modules"},{"location":"Installed_Software_Lists/python-packages/","text":"Python Packages \u00a7 We provide a collection of installed Python packages for each minor version of Python, as a bundle module . This page lists the packages for the current recommended Python 3 bundle. This can be loaded using: module load python3/recommended The version of Python 3 provided with this bundle is currently Python 3.7.4. Note that some packages we do not provide this way, because they have complicated non-Python dependencies. These are usually provided using the normal application modules system. This includes TensorFlow . The following list was last updated at 15:31:10 (+0100) on 09 Sep 2020. Module Version Description acora 2.2 Fast multi-keyword search engine for text strings appdirs 1.4.3 A small Python module for determining appropriate platform-specific dirs, e.g. a \"user data dir\". args 0.1.0 Command Arguments for Humans. ase 3.18.0 Atomic Simulation Environment asn1crypto 0.24.0 Fast ASN.1 parser and serializer with definitions for private keys, public keys, certificates, CRL, OCSP, CMS, PKCS#3, PKCS#7, PKCS#8, PKCS#12, PKCS#5, X.509 and TSP astor 0.8.0 Read/rewrite/write Python ASTs astropy 3.2.1 Community-developed python astronomy tools atomicwrites 1.3.0 Atomic file writes. attrs 19.1.0 Classes Without Boilerplate backcall 0.1.0 Specifications for callback functions passed in to an API biopython 1.74 Freely available tools for computational molecular biology. BoltzTraP2 19.7.3 band-structure interpolator and transport coefficient calculator certifi 2019.6.16 Python package for providing Mozilla's CA Bundle. cffi 1.12.3 Foreign Function Interface for Python calling C code. cftime 1.0.3.4 Time-handling functionality from netcdf4-python chardet 3.0.4 Universal encoding detector for Python 2 and 3 Click 7.0 Composable command line interface toolkit clint 0.5.1 Python Command Line Interface Tools colormath 3.0.0 Color math and conversion library. cryptography 2.7 cryptography is a package which provides cryptographic recipes and primitives to Python developers. cutadapt 2.4 trim adapters from high-throughput sequencing reads cvxopt 1.2.3 Convex optimization package cycler 0.10.0 Composable style cycles Cython 0.29.13 The Cython compiler for writing C extensions for the Python language. cyvcf2 0.11.5 fast vcf parsing with cython + htslib deap 1.3.0 Distributed Evolutionary Algorithms in Python decorator 4.4.0 Better living through Python with decorators dnaio 0.3 Read FASTA and FASTQ files efficiently ecdsa 0.13.2 ECDSA cryptographic signature library (pure python) emcee 2.2.1 Kick ass affine-invariant ensemble MCMC sampling ephem 3.7.7.0 Compute positions of the planets and stars Flask 1.1.1 A simple framework for building complex web applications. funcparserlib 0.3.6 Recursive descent parsing library based on functional combinators future 0.17.1 Clean single-source support for Python 3 and 2 hankel 0.3.9 Hankel Transformations using method of Ogata 2005 hy 0.17.0 Lisp and Python love each other. idna 2.8 Internationalized Domain Names in Applications (IDNA) imageio 2.5.0 Library for reading and writing a wide range of image, video, scientific, and volumetric data formats. importlib-metadata 0.19 Read metadata from Python packages ipython 7.7.0 IPython: Productive Interactive Computing ipython-genutils 0.2.0 Vestigial utilities from IPython itsdangerous 1.1.0 Various helpers to pass data to untrusted environments and back. jedi 0.15.1 An autocompletion tool for Python that can be used for text editors. Jinja2 2.10.1 A small but fast and easy to use stand-alone template engine written in pure python. joblib 0.13.2 Lightweight pipelining: using Python functions as pipeline jobs. kiwisolver 1.1.0 A fast implementation of the Cassowary constraint solver ldap3 2.6 A strictly RFC 4510 conforming LDAP V3 pure Python client library llvmlite 0.29.0 lightweight wrapper around basic LLVM functionality lxml 4.4.1 Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API. lzstring 1.0.4 lz-string for python Mako 1.1.0 A super-fast templating language that borrows the best ideas from the existing templating languages. Markdown 3.1.1 Python implementation of Markdown. MarkupSafe 1.1.1 Safely add untrusted strings to HTML/XML markup. matplotlib 2.2.4 Python plotting package more-itertools 7.2.0 More routines for operating on iterables, beyond itertools mpmath 1.1.0 Python library for arbitrary-precision floating-point arithmetic multiqc 1.7 Create aggregate bioinformatics analysis reports across many samples and tools netCDF4 1.5.1.2 Provides an object-oriented python interface to the netCDF version 4 library. networkx 2.3 Python package for creating and manipulating graphs and networks nibabel 2.5.0 Access a multitude of neuroimaging data formats ninja 1.9.0.post1 Ninja is a small build system with a focus on speed nose 1.3.7 nose extends unittest to make testing easier numba 0.45.1 compiling Python code using LLVM numpy 1.17.0 NumPy is the fundamental package for array computing with Python. obspy 1.1.1 ObsPy - a Python framework for seismological observatories. packaging 19.1 Core utilities for Python packages pandas 0.25.0 Powerful data structures for data analysis, time series, and statistics parso 0.5.1 A Python Parser pexpect 4.7.0 Pexpect allows easy control of interactive console applications. pickleshare 0.7.5 Tiny 'shelve'-like database with concurrency support Pillow 6.1.0 Python Imaging Library (Fork) pip 19.2.3 The PyPA recommended tool for installing Python packages. pluggy 0.12.0 plugin and hook calling mechanisms for python Pmw 2.0.1 Python Mega Widgets prompt-toolkit 2.0.9 Library for building powerful interactive command lines in Python ptyprocess 0.6.0 Run a subprocess in a pseudo terminal py 1.8.0 library with cross-python path, ini-parsing, io, code, log facilities pyasn1 0.4.6 ASN.1 types and codecs pycparser 2.19 C parser in Python pyfastaq 3.17.0 Script to manipulate FASTA and FASTQ files, plus API for developers Pygments 2.4.2 Pygments is a syntax highlighting package written in Python. pyparsing 2.4.2 Python parsing module pysam 0.15.3 pysam pysamstats 1.1.2 A Python utility for calculating statistics against genome position based on sequence alignments from a SAM, BAM or CRAM file. pyspglib 1.8.3.1 This is the pyspglib module. pytest 5.1.1 pytest: simple powerful testing with Python python-dateutil 2.8.0 Extensions to the standard Python datetime module python-Levenshtein 0.12.0 Python extension for computing string edit distances and similarities. pytz 2019.2 World timezone definitions, modern and historical PyWavelets 1.0.3 PyWavelets, wavelet transform module PyYAML 5.1.2 YAML parser and emitter for Python qutip 4.4.1 QuTiP: The Quantum Toolbox in Python regex 2019.8.19 Alternative regular expression module, to replace re. requests 2.22.0 Python HTTP for Humans. rply 0.7.7 A pure Python Lex/Yacc that works with RPython scikit-image 0.15.0 Image processing routines for SciPy scikit-learn 0.21.3 A set of python modules for machine learning and data mining scipy 1.3.1 SciPy: Scientific Library for Python seaborn 0.9.0 seaborn: statistical data visualization setuptools 41.2.0 Easily download, build, install, upgrade, and uninstall Python packages setuptools-scm 3.3.3 the blessed package to manage your versions by scm tags simplejson 3.16.0 Simple, fast, extensible JSON encoder/decoder for Python six 1.12.0 Python 2 and 3 compatibility utilities spectra 0.0.11 Color scales and color conversion made easy for Python. spglib 1.14.1.post0 This is the spglib module. SQLAlchemy 1.3.7 Database Abstraction Library sshpubkeys 3.1.0 SSH public key parser tabulate 0.8.3 Pretty-print tabular data traitlets 4.3.2 Traitlets Python config system urllib3 1.25.3 HTTP library with thread-safe connection pooling, file post, and more. virtualenv 16.7.3 Virtual Python Environment builder wcwidth 0.1.7 Measures number of Terminal column cells of wide-character codes weblogo 3.7.1 WebLogo3 : Sequence Logos Redrawn Werkzeug 0.15.5 The comprehensive WSGI web application library. wheel 0.33.6 A built-package format for Python. xlrd 1.2.0 Library for developers to extract data from Microsoft Excel (tm) spreadsheet files XlsxWriter 1.1.9 A Python module for creating Excel XLSX files. xlutils 2.0.0 Utilities for working with Excel files that require both xlrd and xlwt xlwt 1.3.0 Library to create spreadsheet files compatible with MS Excel 97/2000/XP/2003 XLS files, on any platform, with Python 2.6, 2.7, 3.3+ xopen 0.8.1 Open compressed files transparently zipp 0.5.2 Backport of pathlib-compatible object wrapper for zip files","title":"Python Packages"},{"location":"Installed_Software_Lists/python-packages/#python-packages","text":"We provide a collection of installed Python packages for each minor version of Python, as a bundle module . This page lists the packages for the current recommended Python 3 bundle. This can be loaded using: module load python3/recommended The version of Python 3 provided with this bundle is currently Python 3.7.4. Note that some packages we do not provide this way, because they have complicated non-Python dependencies. These are usually provided using the normal application modules system. This includes TensorFlow . The following list was last updated at 15:31:10 (+0100) on 09 Sep 2020. Module Version Description acora 2.2 Fast multi-keyword search engine for text strings appdirs 1.4.3 A small Python module for determining appropriate platform-specific dirs, e.g. a \"user data dir\". args 0.1.0 Command Arguments for Humans. ase 3.18.0 Atomic Simulation Environment asn1crypto 0.24.0 Fast ASN.1 parser and serializer with definitions for private keys, public keys, certificates, CRL, OCSP, CMS, PKCS#3, PKCS#7, PKCS#8, PKCS#12, PKCS#5, X.509 and TSP astor 0.8.0 Read/rewrite/write Python ASTs astropy 3.2.1 Community-developed python astronomy tools atomicwrites 1.3.0 Atomic file writes. attrs 19.1.0 Classes Without Boilerplate backcall 0.1.0 Specifications for callback functions passed in to an API biopython 1.74 Freely available tools for computational molecular biology. BoltzTraP2 19.7.3 band-structure interpolator and transport coefficient calculator certifi 2019.6.16 Python package for providing Mozilla's CA Bundle. cffi 1.12.3 Foreign Function Interface for Python calling C code. cftime 1.0.3.4 Time-handling functionality from netcdf4-python chardet 3.0.4 Universal encoding detector for Python 2 and 3 Click 7.0 Composable command line interface toolkit clint 0.5.1 Python Command Line Interface Tools colormath 3.0.0 Color math and conversion library. cryptography 2.7 cryptography is a package which provides cryptographic recipes and primitives to Python developers. cutadapt 2.4 trim adapters from high-throughput sequencing reads cvxopt 1.2.3 Convex optimization package cycler 0.10.0 Composable style cycles Cython 0.29.13 The Cython compiler for writing C extensions for the Python language. cyvcf2 0.11.5 fast vcf parsing with cython + htslib deap 1.3.0 Distributed Evolutionary Algorithms in Python decorator 4.4.0 Better living through Python with decorators dnaio 0.3 Read FASTA and FASTQ files efficiently ecdsa 0.13.2 ECDSA cryptographic signature library (pure python) emcee 2.2.1 Kick ass affine-invariant ensemble MCMC sampling ephem 3.7.7.0 Compute positions of the planets and stars Flask 1.1.1 A simple framework for building complex web applications. funcparserlib 0.3.6 Recursive descent parsing library based on functional combinators future 0.17.1 Clean single-source support for Python 3 and 2 hankel 0.3.9 Hankel Transformations using method of Ogata 2005 hy 0.17.0 Lisp and Python love each other. idna 2.8 Internationalized Domain Names in Applications (IDNA) imageio 2.5.0 Library for reading and writing a wide range of image, video, scientific, and volumetric data formats. importlib-metadata 0.19 Read metadata from Python packages ipython 7.7.0 IPython: Productive Interactive Computing ipython-genutils 0.2.0 Vestigial utilities from IPython itsdangerous 1.1.0 Various helpers to pass data to untrusted environments and back. jedi 0.15.1 An autocompletion tool for Python that can be used for text editors. Jinja2 2.10.1 A small but fast and easy to use stand-alone template engine written in pure python. joblib 0.13.2 Lightweight pipelining: using Python functions as pipeline jobs. kiwisolver 1.1.0 A fast implementation of the Cassowary constraint solver ldap3 2.6 A strictly RFC 4510 conforming LDAP V3 pure Python client library llvmlite 0.29.0 lightweight wrapper around basic LLVM functionality lxml 4.4.1 Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API. lzstring 1.0.4 lz-string for python Mako 1.1.0 A super-fast templating language that borrows the best ideas from the existing templating languages. Markdown 3.1.1 Python implementation of Markdown. MarkupSafe 1.1.1 Safely add untrusted strings to HTML/XML markup. matplotlib 2.2.4 Python plotting package more-itertools 7.2.0 More routines for operating on iterables, beyond itertools mpmath 1.1.0 Python library for arbitrary-precision floating-point arithmetic multiqc 1.7 Create aggregate bioinformatics analysis reports across many samples and tools netCDF4 1.5.1.2 Provides an object-oriented python interface to the netCDF version 4 library. networkx 2.3 Python package for creating and manipulating graphs and networks nibabel 2.5.0 Access a multitude of neuroimaging data formats ninja 1.9.0.post1 Ninja is a small build system with a focus on speed nose 1.3.7 nose extends unittest to make testing easier numba 0.45.1 compiling Python code using LLVM numpy 1.17.0 NumPy is the fundamental package for array computing with Python. obspy 1.1.1 ObsPy - a Python framework for seismological observatories. packaging 19.1 Core utilities for Python packages pandas 0.25.0 Powerful data structures for data analysis, time series, and statistics parso 0.5.1 A Python Parser pexpect 4.7.0 Pexpect allows easy control of interactive console applications. pickleshare 0.7.5 Tiny 'shelve'-like database with concurrency support Pillow 6.1.0 Python Imaging Library (Fork) pip 19.2.3 The PyPA recommended tool for installing Python packages. pluggy 0.12.0 plugin and hook calling mechanisms for python Pmw 2.0.1 Python Mega Widgets prompt-toolkit 2.0.9 Library for building powerful interactive command lines in Python ptyprocess 0.6.0 Run a subprocess in a pseudo terminal py 1.8.0 library with cross-python path, ini-parsing, io, code, log facilities pyasn1 0.4.6 ASN.1 types and codecs pycparser 2.19 C parser in Python pyfastaq 3.17.0 Script to manipulate FASTA and FASTQ files, plus API for developers Pygments 2.4.2 Pygments is a syntax highlighting package written in Python. pyparsing 2.4.2 Python parsing module pysam 0.15.3 pysam pysamstats 1.1.2 A Python utility for calculating statistics against genome position based on sequence alignments from a SAM, BAM or CRAM file. pyspglib 1.8.3.1 This is the pyspglib module. pytest 5.1.1 pytest: simple powerful testing with Python python-dateutil 2.8.0 Extensions to the standard Python datetime module python-Levenshtein 0.12.0 Python extension for computing string edit distances and similarities. pytz 2019.2 World timezone definitions, modern and historical PyWavelets 1.0.3 PyWavelets, wavelet transform module PyYAML 5.1.2 YAML parser and emitter for Python qutip 4.4.1 QuTiP: The Quantum Toolbox in Python regex 2019.8.19 Alternative regular expression module, to replace re. requests 2.22.0 Python HTTP for Humans. rply 0.7.7 A pure Python Lex/Yacc that works with RPython scikit-image 0.15.0 Image processing routines for SciPy scikit-learn 0.21.3 A set of python modules for machine learning and data mining scipy 1.3.1 SciPy: Scientific Library for Python seaborn 0.9.0 seaborn: statistical data visualization setuptools 41.2.0 Easily download, build, install, upgrade, and uninstall Python packages setuptools-scm 3.3.3 the blessed package to manage your versions by scm tags simplejson 3.16.0 Simple, fast, extensible JSON encoder/decoder for Python six 1.12.0 Python 2 and 3 compatibility utilities spectra 0.0.11 Color scales and color conversion made easy for Python. spglib 1.14.1.post0 This is the spglib module. SQLAlchemy 1.3.7 Database Abstraction Library sshpubkeys 3.1.0 SSH public key parser tabulate 0.8.3 Pretty-print tabular data traitlets 4.3.2 Traitlets Python config system urllib3 1.25.3 HTTP library with thread-safe connection pooling, file post, and more. virtualenv 16.7.3 Virtual Python Environment builder wcwidth 0.1.7 Measures number of Terminal column cells of wide-character codes weblogo 3.7.1 WebLogo3 : Sequence Logos Redrawn Werkzeug 0.15.5 The comprehensive WSGI web application library. wheel 0.33.6 A built-package format for Python. xlrd 1.2.0 Library for developers to extract data from Microsoft Excel (tm) spreadsheet files XlsxWriter 1.1.9 A Python module for creating Excel XLSX files. xlutils 2.0.0 Utilities for working with Excel files that require both xlrd and xlwt xlwt 1.3.0 Library to create spreadsheet files compatible with MS Excel 97/2000/XP/2003 XLS files, on any platform, with Python 2.6, 2.7, 3.3+ xopen 0.8.1 Open compressed files transparently zipp 0.5.2 Backport of pathlib-compatible object wrapper for zip files","title":"Python Packages"},{"location":"Installed_Software_Lists/r-packages/","text":"R Packages \u00a7 We provide a collection of installed R packages for each release of R, as a bundle module . This page lists the packages for the current recommended R bundle. This can be loaded using: module load r/recommended The version of R provided with this bundle is currently R version 3.6.0 (2019-04-26). The following list was last updated at: 15:31:26 (+0100) on 09 Sep 2020. Module Version Description AnnotationForge 1.26.0 Tools for building SQLite-based annotation data packages BiocManager 1.30.4 Access the Bioconductor Project Package Repository BiocVersion 3.9.0 Set the appropriate version of Bioconductor packages Deriv 3.8.5 Symbolic Differentiation dgo 0.2.15 Dynamic Estimation of Group-Level Opinion dgodata 0.0.2 Data for the 'dgo' Package digest 0.6.21 Create Compact Hash Digests of R Objects GO.db 3.8.2 A set of annotation maps describing the entire Gene Ontology hgu95av2.db 3.2.3 Affymetrix Human Genome U95 Set annotation data (chip hgu95av2) HKprocess 0.0-2 Hurst-Kolmogorov Process hugene10stprobeset.db 8.7.0 Affymetrix hugene10 annotation data (chip hugene10stprobeset) hugene10sttranscriptcluster.db 8.7.0 Affymetrix hugene10 annotation data (chip hugene10sttranscriptcluster) Illumina450ProbeVariants.db 1.20.0 Annotation Package combining variant data from 1000 Genomes Project for Illumina HumanMethylation450 Bead Chip probes IlluminaHumanMethylation450kmanifest 0.4.0 Annotation for Illumina's 450k methylation arrays illuminaHumanv4.db 1.26.0 Illumina HumanHT12v4 annotation data (chip illuminaHumanv4) INLA 19.09.03 Full Bayesian Analysis of Latent Gaussian Models using Integrated Nested Laplace Approximations KEGG.db 3.2.3 A set of annotation maps for KEGG MCMCpack 1.4-4 Markov Chain Monte Carlo (MCMC) Package missMethyl 1.18.0 Analysing Illumina HumanMethylation BeadChip Data org.Hs.eg.db 3.8.2 Genome wide annotation for Human orthopolynom 1.0-5 Collection of functions for orthogonal and orthonormal polynomials polynom 1.4-0 A Collection of Functions to Implement a Class for Univariate Polynomial Manipulations quanteda.corpora 0.87 A collection of corpora for quanteda RcppZiggurat 0.1.5 'Rcpp' Integration of Different \"Ziggurat\" Normal RNG Implementations Rfast 1.9.9 A Collection of Efficient and Extremely Fast R Functions sf 0.7-7 Simple Features for R splancs 2.01-40 Spatial and Space-Time Point Pattern Analysis abc 2.1 Tools for Approximate Bayesian Computation (ABC) abc.data 1.0 Data Only: Tools for Approximate Bayesian Computation (ABC) abind 1.4-5 Combine Multidimensional Arrays acepack 1.4.1 ACE and AVAS for Selecting Multiple Regression Transformations adapt 1.0-4 adapt -- multidimensional numerical integration ade4 1.7-13 Analysis of Ecological Data: Exploratory and Euclidean Methods in Environmental Sciences adegenet 2.1.1 Exploratory Analysis of Genetic and Genomic Data ADGofTest 0.3 Anderson-Darling GoF test affxparser 1.56.0 Affymetrix File Parsing SDK affy 1.62.0 Methods for Affymetrix Oligonucleotide Arrays affydata 1.32.0 Affymetrix Data for Demonstration Purpose affyio 1.54.0 Tools for parsing Affymetrix data files affylmGUI 1.58.0 GUI for limma Package with Affymetrix Microarrays affyPLM 1.60.0 Methods for fitting probe-level models affyQCReport 1.62.0 QC Report Generation for affyBatch objects akima 0.6-2 Interpolation of Irregularly and Regularly Spaced Data annaffy 1.56.0 Annotation tools for Affymetrix biological metadata annmap 1.26.0 Genome annotation and visualisation package pertaining to Affymetrix arrays and NGS analysis. annotate 1.62.0 Annotation for microarrays AnnotationDbi 1.46.0 Manipulation of SQLite-based annotations in Bioconductor AnnotationFilter 1.8.0 Facilities for Filtering Bioconductor Annotation Resources ape 5.3 Analyses of Phylogenetics and Evolution arm 1.10-1 Data Analysis Using Regression and Multilevel/Hierarchical Models aroma.affymetrix 3.2.0 Analysis of Large Affymetrix Microarray Data Sets aroma.apd 0.6.0 A Probe-Level Data File Format Used by 'aroma.affymetrix' [deprecated] aroma.core 3.2.0 Core Methods and Classes Used by 'aroma.*' Packages Part of the Aroma Framework aroma.light 3.14.0 Light-Weight Methods for Normalization and Visualization of Microarray Data using Only Basic R Data Types askpass 1.1 Safe Password Entry for R, Git, and SSH assertthat 0.2.1 Easy Pre and Post Assertions backports 1.1.4 Reimplementations of Functions Introduced Since R-3.0.0 base 3.6.0 The R Base Package base64 2.0 Base64 Encoder and Decoder base64enc 0.1-3 Tools for base64 encoding BaSTA 1.9.4 Age-Specific Survival Analysis from Incomplete Capture-Recapture/Recovery Data BatchJobs 1.8 Batch Computing with R bayesplot 1.7.0 Plotting for Bayesian Models BBmisc 1.11 Miscellaneous Helper Functions for B. Bischl beachmat 2.0.0 Compiling Bioconductor to Handle Each Matrix Type beadarray 2.34.0 Quality assessment and low-level analysis for Illumina BeadArray data beadarrayExampleData 1.22.0 Example data for the beadarray package BeadDataPackR 1.36.0 Compression of Illumina BeadArray data beanplot 1.2 Visualization via Beanplots (like Boxplot/Stripchart/Violin Plot) BH 1.69.0-1 Boost C++ Header Files BiasedUrn 1.07 Biased Urn Model Distributions bibtex 0.4.2 Bibtex Parser bio3d 2.3-4 Biological Structure Analysis Biobase 2.44.0 Biobase: Base functions for Bioconductor BiocFileCache 1.8.0 Manage Files Across Sessions BiocGenerics 0.30.0 S4 generic functions used in Bioconductor BiocParallel 1.18.0 Bioconductor facilities for parallel evaluation biomaRt 2.40.1 Interface to BioMart databases (i.e. Ensembl) Biostrings 2.52.0 Efficient manipulation of biological strings biovizBase 1.32.0 Basic graphic utilities for visualization of genomic data. bit 1.1-14 A Class for Vectors of 1-Bit Booleans bit64 0.9-7 A S3 Class for Vectors of 64bit Integers bitops 1.0-6 Bitwise Operations blob 1.1.1 A Simple S3 Class for Representing Vectors of Binary Data ('BLOBS') blockmodeling 0.3.4 Generalized and Classical Blockmodeling of Valued Networks boot 1.3-22 Bootstrap Functions (Originally by Angelo Canty for S) brew 1.0-6 Templating Framework for Report Generation broom 0.5.2 Convert Statistical Analysis Objects into Tidy Tibbles BSgenome 1.52.0 Software infrastructure for efficient representation of full genomes and their SNPs BSgenome.Hsapiens.UCSC.hg19 1.4.0 Full genome sequences for Homo sapiens (UCSC version hg19) bsseq 1.20.0 Analyze, manage and store bisulfite sequencing data bumphunter 1.26.0 Bump Hunter callr 3.2.0 Call R from R car 3.0-3 Companion to Applied Regression carData 3.0-2 Companion to Applied Regression Data Sets caret 6.0-84 Classification and Regression Training Category 2.50.0 Category Analysis caTools 1.17.1.2 Tools: moving window statistics, GIF, Base64, ROC AUC, etc. CDM 7.3-17 Cognitive Diagnosis Modeling cellranger 1.1.0 Translate Spreadsheet Cell Ranges to Rows and Columns ChAMP 2.14.0 Chip Analysis Methylation Pipeline for Illumina HumanMethylation450 and EPIC ChAMPdata 2.16.0 Data Packages for ChAMP package checkmate 1.9.4 Fast and Versatile Argument Checks class 7.3-15 Functions for Classification classInt 0.3-3 Choose Univariate Class Intervals cli 1.1.0 Helpers for Developing Command Line Interfaces clipr 0.6.0 Read and Write from the System Clipboard clisymbols 1.2.0 Unicode Symbols at the R Prompt clue 0.3-57 Cluster Ensembles cluster 2.0.8 \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. cmprsk 2.2-8 Subdistribution Analysis of Competing Risks coda 0.19-3 Output Analysis and Diagnostics for MCMC codetools 0.2-16 Code Analysis Tools for R colorRamps 2.3 Builds color tables colorspace 1.4-1 A Toolbox for Manipulating and Assessing Colors and Palettes colourpicker 1.0 A Colour Picker Tool for Shiny and for Selecting Colours in Plots combinat 0.0-8 combinatorics utilities compiler 3.6.0 The R Compiler Package copula 0.999-19.1 Multivariate Dependence with Copulas copynumber 1.24.0 Segmentation of single- and multi-track copy number data by penalized least squares regression. corpcor 1.6.9 Efficient Estimation of Covariance and (Partial) Correlation corrplot 0.84 Visualization of a Correlation Matrix crayon 1.3.4 Colored Terminal Output crosstalk 1.0.0 Inter-Widget Interactivity for HTML Widgets curl 3.3 A Modern and Flexible Web Client for R data.table 1.12.2 Extension of data.frame datasets 3.6.0 The R Datasets Package DBI 1.0.0 R Database Interface dbplyr 1.4.0 A 'dplyr' Back End for Databases DelayedArray 0.10.0 A unified framework for working transparently with on-disk and in-memory array-like datasets DelayedMatrixStats 1.6.0 Functions that Apply to Rows and Columns of 'DelayedMatrix' Objects deldir 0.1-16 Delaunay Triangulation and Dirichlet (Voronoi) Tessellation dendextend 1.12.0 Extending 'dendrogram' Functionality in R DEoptimR 1.0-8 Differential Evolution Optimization in Pure R desc 1.2.0 Manipulate DESCRIPTION Files DESeq 1.36.0 Differential gene expression analysis based on the negative binomial distribution DESeq2 1.24.0 Differential gene expression analysis based on the negative binomial distribution Design 2.3-0 Design Package devtools 2.0.2 Tools to Make Developing R Packages Easier DEXSeq 1.30.0 Inference of differential exon usage in RNA-Seq dichromat 2.0-0 Color Schemes for Dichromats digest 0.6.20 Create Compact Hash Digests of R Objects diptest 0.75-7 Hartigan's Dip Test Statistic for Unimodality - Corrected distr 2.8.0 Object Oriented Implementation of Distributions distrEx 2.8.0 Extensions of Package 'distr' DMRcate 1.20.0 Methylation array and sequencing spatial analysis methods DMRcatedata 1.20.0 Data Package for DMRcate package DNAcopy 1.58.0 DNA copy number data analysis doMC 1.3.5 Foreach Parallel Adaptor for 'parallel' doParallel 1.0.14 Foreach Parallel Adaptor for the 'parallel' Package doRNG 1.7.1 Generic Reproducible Parallel Backend for 'foreach' Loops dotCall64 1.0-0 Enhanced Foreign Function Interface Supporting Long Vectors dplyr 0.8.1 A Grammar of Data Manipulation DSS 2.32.0 Dispersion shrinkage for sequencing data DT 0.6 A Wrapper of the JavaScript Library 'DataTables' dygraphs 1.1.1.6 Interface to 'Dygraphs' Interactive Time Series Charting Library dynamicTreeCut 1.63-1 Methods for Detection of Clusters in Hierarchical Clustering Dendrograms DynDoc 1.62.0 Dynamic document tools e1071 1.7-2 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien easyRNASeq 2.20.0 Count summarization and normalization for RNA-Seq data EBSeq 1.24.0 An R package for gene and isoform differential expression analysis of RNA-seq data edgeR 3.26.5 Empirical Analysis of Digital Gene Expression Data in R effects 4.1-1 Effect Displays for Linear, Generalized Linear, and Other Models ellipse 0.4.1 Functions for Drawing Ellipses and Ellipse-Like Confidence Regions ellipsis 0.1.0 Tools for Working with ... ensembldb 2.8.0 Utilities to create and use Ensembl-based annotation databases Epi 2.37 A Package for Statistical Analysis in Epidemiology erer 2.5 Empirical Research in Economics with R estimability 1.3 Tools for Assessing Estimability of Linear Predictions etm 1.0.5 Empirical Transition Matrix evaluate 0.14 Parsing and Evaluation Tools that Provide More Details than the Default evd 2.3-3 Functions for Extreme Value Distributions expm 0.999-4 Matrix Exponential, Log, 'etc' extrafont 0.17 Tools for using fonts extrafontdb 1.0 Package for holding the database for the extrafont package FactoMineR 1.42 Multivariate Exploratory Data Analysis and Data Mining fail 1.3 File Abstraction Interface Layer (FAIL) fansi 0.4.0 ANSI Control Sequence Aware String Functions fastcluster 1.1.25 Fast Hierarchical Clustering Routines for R and 'Python' fastICA 1.2-1 FastICA Algorithms to Perform ICA and Projection Pursuit fastmatch 1.1-0 Fast match() function FDb.InfiniumMethylation.hg19 2.2.0 Annotation package for Illumina Infinium DNA methylation probes fdrtool 1.2.15 Estimation of (Local) False Discovery Rates and Higher Criticism FEM 3.12.0 Identification of Functional Epigenetic Modules fields 9.8-3 Tools for Spatial Data fit.models 0.5-14 Compare Fitted Models flashClust 1.01-2 Implementation of optimal hierarchical clustering flexmix 2.3-15 Flexible Mixture Modeling forcats 0.4.0 Tools for Working with Categorical Variables (Factors) foreach 1.4.4 Provides Foreach Looping Construct for R foreign 0.8-71 Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... formatR 1.6 Format R Code Automatically Formula 1.2-3 Extended Model Formulas fpc 2.2-3 Flexible Procedures for Clustering fs 1.3.1 Cross-Platform File System Operations Based on 'libuv' futile.logger 1.4.3 A Logging Utility for R futile.options 1.0.1 Futile Options Management future 1.13.0 Unified Parallel and Distributed Processing in R for Everyone gam 1.16.1 Generalized Additive Models gamlss 5.1-4 Generalised Additive Models for Location Scale and Shape gamlss.data 5.1-4 GAMLSS Data gamlss.dist 5.1-4 Distributions for Generalized Additive Models for Location Scale and Shape gamlss.mx 4.3-5 Fitting Mixture Distributions with GAMLSS gamlss.nl 4.1-0 Fitting non linear parametric GAMLSS models gcrma 2.56.0 Background Adjustment Using Sequence Information gdalUtils 2.0.1.14 Wrappers for the Geospatial Data Abstraction Library (GDAL) Utilities gdata 2.18.0 Various R Programming Tools for Data Manipulation gdtools 0.1.8 Utilities for Graphical Rendering genefilter 1.66.0 genefilter: methods for filtering genes from high-throughput experiments geneLenDataBase 1.20.0 Lengths of mRNA transcripts for a number of genomes GeneNet 1.2.13 Modeling and Inferring Gene Networks geneplotter 1.62.0 Graphics related functions for Bioconductor generics 0.0.2 Common S3 Generics not Provided by Base R Methods Related to Model Fitting genetics 1.3.8.1.2 Population Genetics GenomeGraphs 1.44.0 Plotting genomic information from Ensembl GenomeInfoDb 1.20.0 Utilities for manipulating chromosome names, including modifying them to follow a particular naming style GenomeInfoDbData 1.2.1 Species and taxonomy ID look up tables used by GenomeInfoDb genomeIntervals 1.40.0 Operations on genomic intervals GenomicAlignments 1.20.1 Representation and manipulation of short genomic alignments GenomicFeatures 1.36.3 Conveniently import and query gene models GenomicRanges 1.36.0 Representation and manipulation of genomic intervals GEOquery 2.52.0 Get data from NCBI Gene Expression Omnibus (GEO) GGally 1.4.0 Extension to 'ggplot2' ggplot2 3.2.0 Create Elegant Data Visualisations Using the Grammar of Graphics ggrepel 0.8.1 Automatically Position Non-Overlapping Text Labels with 'ggplot2' ggridges 0.5.1 Ridgeline Plots in 'ggplot2' gh 1.0.1 'GitHub' 'API' git2r 0.25.2 Provides Access to Git Repositories GJRM 0.2 Generalised Joint Regression Modelling glmnet 2.0-18 Lasso and Elastic-Net Regularized Generalized Linear Models globals 0.12.4 Identify Global Objects in R Expressions globaltest 5.38.0 Testing Groups of Covariates/Features for Association with a Response Variable, with Applications to Gene Set Testing glue 1.3.1 Interpreted String Literals gmodels 2.18.1 Various R Programming Tools for Model Fitting gmp 0.5-13.5 Multiple Precision Arithmetic goseq 1.36.0 Gene Ontology analyser for RNA-seq and other length biased data GOstats 2.50.0 Tools for manipulating GO and microarrays gower 0.2.1 Gower's Distance gplots 3.0.1.1 Various R Programming Tools for Plotting Data graph 1.62.0 graph: A package to handle graph data structures graphics 3.6.0 The R Graphics Package grDevices 3.6.0 The R Graphics Devices and Support for Colours and Fonts grid 3.6.0 The Grid Graphics Package gridExtra 2.3 Miscellaneous Functions for \"Grid\" Graphics GSEABase 1.46.0 Gene set enrichment data structures and methods gsl 2.1-6 Wrapper for the Gnu Scientific Library gsmoothr 0.1.7 Smoothing tools gtable 0.3.0 Arrange 'Grobs' in Tables gtools 3.8.1 Various R Programming Tools Gviz 1.28.0 Plotting data and annotation information along genomic coordinates HAC 1.0-5 Estimation, Simulation and Visualization of Hierarchical Archimedean Copulae (HAC) haplo.stats 1.7.9 Statistical Analysis of Haplotypes with Traits and Covariates when Linkage Phase is Ambiguous haven 2.1.0 Import and Export 'SPSS', 'Stata' and 'SAS' Files HDF5Array 1.12.1 HDF5 backend for DelayedArray objects hexbin 1.27.3 Hexagonal Binning Routines HI 0.4 Simulation from distributions supported by nested hyperplanes highr 0.8 Syntax Highlighting for R Source Code Hmisc 4.2-0 Harrell Miscellaneous hms 0.4.2 Pretty Time of Day HotDeckImputation 1.1.0 Hot Deck Imputation Methods for Missing Data htmlTable 1.13.1 Advanced Tables for Markdown/HTML htmltools 0.3.6 Tools for HTML htmlwidgets 1.3 HTML Widgets for R httpuv 1.5.1 HTTP and WebSocket Server Library httr 1.4.0 Tools for Working with URLs and HTTP hwriter 1.3.2 HTML Writer - Outputs R objects in HTML format igraph 1.2.4.1 Network Analysis and Visualization IlluminaHumanMethylation450kanno.ilmn12.hg19 0.6.0 Annotation for Illumina's 450k methylation arrays IlluminaHumanMethylationEPICanno.ilm10b2.hg19 0.6.0 Annotation for Illumina's EPIC methylation arrays IlluminaHumanMethylationEPICanno.ilm10b4.hg19 0.6.0 Annotation for Illumina's EPIC methylation arrays IlluminaHumanMethylationEPICmanifest 0.3.0 Manifest for Illumina's EPIC methylation arrays illuminaio 0.26.0 Parsing Illumina Microarray Output Files impute 1.58.0 impute: Imputation for microarray data ini 0.3.1 Read and Write '.ini' Files inline 0.3.15 Functions to Inline C, C++, Fortran Function Calls from R intervals 0.15.1 Tools for Working with Points and Intervals ipred 0.9-9 Improved Predictors IRanges 2.18.1 Foundation of integer range manipulation in Bioconductor ISOcodes 2019.04.22 Selected ISO Codes isva 1.9 Independent Surrogate Variable Analysis iterators 1.0.10 Provides Iterator Construct for R JADE 2.0-1 Blind Source Separation Methods Based on Joint Diagonalization and Some BSS Performance Criteria jsonlite 1.6 A Robust, High Performance JSON Parser and Generator for R kdecopula 0.9.2 Kernel Smoothing for Bivariate Copula Densities kernlab 0.9-27 Kernel-Based Machine Learning Lab KernSmooth 2.23-15 Functions for Kernel Smoothing Supporting Wand & Jones (1995) knitr 1.23 A General-Purpose Package for Dynamic Report Generation in R kohonen 3.0.8 Supervised and Unsupervised Self-Organising Maps kpmt 0.1.0 Known Population Median Test labeling 0.3 Axis Labeling lambda.r 1.2.3 Modeling Data with Functional Programming later 0.8.0 Utilities for Delaying Function Execution lattice 0.20-38 Trellis Graphics for R latticeExtra 0.6-28 Extra Graphical Utilities Based on Lattice lava 1.6.5 Latent Variable Models lazyeval 0.2.2 Lazy (Non-Standard) Evaluation leafem 0.0.1 'leaflet' Extensions for 'mapview' leaflet 2.0.2 Create Interactive Web Maps with the JavaScript 'Leaflet' Library leafpop 0.0.1 Include Tables, Images and Graphs in Leaflet Pop-Ups leaps 3.0 Regression Subset Selection LearnBayes 2.15.1 Functions for Learning Bayesian Inference limma 3.40.2 Linear Models for Microarray Data listenv 0.7.0 Environments Behaving (Almost) as Lists lme4 1.1-21 Linear Mixed-Effects Models using 'Eigen' and S4 lmtest 0.9-37 Testing Linear Regression Models locfit 1.5-9.1 Local Regression, Likelihood and Density Estimation. longitudinal 1.1.12 Analysis of Multiple Time Course Data loo 2.1.0 Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models lpSolve 5.6.13.1 Interface to 'Lp_solve' v. 5.5 to Solve Linear/Integer Programs LSD 4.0-0 Lots of Superior Depictions ltm 1.1-1 Latent Trait Models under IRT lubridate 1.7.4 Make Dealing with Dates a Little Easier lumi 2.36.0 BeadArray Specific Methods for Illumina Methylation and Expression Microarrays lwgeom 0.1-7 Bindings to Selected 'liblwgeom' Functions for Simple Features made4 1.58.0 Multivariate analysis of microarray data using ADE4 magic 1.5-9 Create and Investigate Magic Squares magrittr 1.5 A Forward-Pipe Operator for R manipulateWidget 0.10.0 Add Even More Interactivity to Interactive Charts maps 3.3.0 Draw Geographical Maps maptools 0.9-5 Tools for Handling Spatial Objects maptree 1.4-7 Mapping, pruning, and graphing tree models mapview 2.7.0 Interactive Viewing of Spatial Data in R markdown 0.9 'Markdown' Rendering for R marray 1.62.0 Exploratory analysis for two-color spotted microarray data MASS 7.3-51.4 Support Functions and Datasets for Venables and Ripley's MASS Matrix 1.2-17 Sparse and Dense Matrix Classes and Methods matrixcalc 1.0-3 Collection of functions for matrix calculations MatrixModels 0.4-1 Modelling with Sparse And Dense Matrices matrixStats 0.54.0 Functions that Apply to Rows and Columns of Matrices (and to Vectors) mclust 5.4.4 Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation mcmc 0.9-6 Markov Chain Monte Carlo memoise 1.1.0 Memoisation of Functions metafor 2.1-0 Meta-Analysis Package for R methods 3.6.0 Formal Methods and Classes methylumi 2.30.0 Handle Illumina methylation data mgcv 1.8-28 Mixed GAM Computation Vehicle with Automatic Smoothness Estimation mime 0.6 Map Filenames to MIME Types minfi 1.30.0 Analyze Illumina Infinium DNA methylation arrays miniUI 0.1.1.1 Shiny UI Widgets for Small Screens minqa 1.2.4 Derivative-free optimization algorithms by quadratic approximation missMethyl 1.18.0 Analysing Illumina HumanMethylation BeadChip Data mitools 2.4 Tools for Multiple Imputation of Missing Data mlr 2.14.0 Machine Learning in R mnormt 1.5-5 The Multivariate Normal and t Distributions ModelMetrics 1.2.2 Rapid Calculation of Model Metrics modelr 0.1.4 Modelling Functions that Work with the Pipe modeltools 0.2-22 Tools and Classes for Statistical Models msm 1.6.7 Multi-State Markov and Hidden Markov Models in Continuous Time mstate 0.2.11 Data Preparation, Estimation and Prediction in Multi-State Models multcomp 1.4-10 Simultaneous Inference in General Parametric Models multtest 2.40.0 Resampling-based multiple hypothesis testing munsell 0.5.0 Utilities for Using Munsell Colours mvtnorm 1.0-11 Multivariate Normal and t Distributions network 1.15 Classes for Relational Data nleqslv 3.3.2 Solve Systems of Nonlinear Equations nlme 3.1-139 Linear and Nonlinear Mixed Effects Models nloptr 1.2.1 R Interface to NLopt NLP 0.2-0 Natural Language Processing Infrastructure nnet 7.3-12 Feed-Forward Neural Networks and Multinomial Log-Linear Models nor1mix 1.3-0 Normal aka Gaussian (1-d) Mixture Models (S3 Classes and Methods) numDeriv 2016.8-1.1 Accurate Numerical Derivatives nutshell 2.0 Data for \"R in a Nutshell\" nutshell.audioscrobbler 1.0 Audioscrobbler data for \"R in a Nutshell\" nutshell.bbdb 1.0 Baseball Database for \"R in a Nutshell\" OPE 0.7 Outer-product emulator openssl 1.4 Toolkit for Encryption, Signatures and Certificates Based on OpenSSL openxlsx 4.1.0.1 Read, Write and Edit XLSX Files packrat 0.5.0 A Dependency Management System for Projects and their R Package Dependencies panelAR 0.1 Estimation of Linear AR(1) Panel Data Models with Cross-Sectional Heteroskedasticity and/or Correlation parallel 3.6.0 Support for Parallel computation in R parallelMap 1.4 Unified Interface to Parallelization Back-Ends ParamHelpers 1.12 Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning parcor 0.2-6 Regularized estimation of partial correlation matrices pbkrtest 0.4-7 Parametric Bootstrap and Kenward Roger Based Methods for Mixed Model Comparison pcaPP 1.9-73 Robust PCA by Projection Pursuit pegas 0.11 Population and Evolutionary Genetics Analysis System permute 0.9-5 Functions for Generating Restricted Permutations of Data phangorn 2.5.5 Phylogenetic Reconstruction and Analysis pillar 1.4.1 Coloured Formatting for Columns pixmap 0.4-11 Bitmap Images (``Pixel Maps'') pkgbuild 1.0.3 Find Tools Needed to Build R Packages pkgconfig 2.0.2 Private Configuration for 'R' Packages pkgload 1.0.2 Simulate Package Installation and Attach pkgmaker 0.27 Development Utilities for R Packages plogr 0.2.0 The 'plog' C++ Logging Library plotly 4.9.0 Create Interactive Web Graphics via 'plotly.js' pls 2.7-1 Partial Least Squares and Principal Component Regression plyr 1.8.4 Tools for Splitting, Applying and Combining Data png 0.1-7 Read and write PNG images poLCA 1.4.1 Polytomous variable Latent Class Analysis polspline 1.1.14 Polynomial Spline Routines polycor 0.7-9 Polychoric and Polyserial Correlations poweRlaw 0.70.2 Analysis of Heavy Tailed Distributions ppls 1.6-1.1 Penalized Partial Least Squares prabclus 2.3-1 Functions for Clustering and Testing of Presence-Absence, Abundance and Multilocus Genetic Data pracma 2.2.5 Practical Numerical Math Functions praise 1.0.0 Praise Users preprocessCore 1.46.0 A collection of pre-processing functions prettydoc 0.2.1 Creating Pretty Documents from R Markdown prettyunits 1.0.2 Pretty, Human Readable Formatting of Quantities processx 3.3.1 Execute and Control System Processes prodlim 2018.04.18 Product-Limit Estimation for Censored Event History Analysis progress 1.2.2 Terminal Progress Bars promises 1.0.1 Abstractions for Promise-Based Asynchronous Programming ProtGenerics 1.16.0 S4 generic functions for Bioconductor proteomics infrastructure proto 1.0.0 Prototype Object-Based Programming proxyC 0.1.5 Computes Proximity in Large Sparse Matrices ps 1.3.0 List, Query, Manipulate System Processes PSCBS 0.65.0 Analysis of Parent-Specific DNA Copy Numbers pspline 1.0-18 Penalized Smoothing Splines psych 1.8.12 Procedures for Psychological, Psychometric, and Personality Research purrr 0.3.2 Functional Programming Tools qrng 0.0-5 (Randomized) Quasi-Random Number Generators quadprog 1.5-7 Functions to Solve Quadratic Programming Problems quanteda 1.5.1 Quantitative Analysis of Textual Data quantmod 0.4-14 Quantitative Financial Modelling Framework quantreg 5.41 Quantile Regression qvalue 2.16.0 Q-value estimation for false discovery rate control R.cache 0.13.0 Fast and Light-Weight Caching (Memoization) of Objects and Results to Speed Up Computations R.devices 2.16.0 Unified Handling of Graphics Devices R.filesets 2.13.0 Easy Handling of and Access to Files Organized in Structured Directories R.huge 0.9.0 Methods for Accessing Huge Amounts of Data [deprecated] R.methodsS3 1.7.1 S3 Methods Simplified R.oo 1.22.0 R Object-Oriented Programming with or without References R.rsp 0.43.1 Dynamic Generation of Scientific Reports R.utils 2.9.0 Various Programming Utilities R2HTML 2.3.2 HTML Exportation for R Objects R2jags 0.5-7 Using R to Run 'JAGS' R2WinBUGS 2.1-21 Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS' R6 2.4.0 Encapsulated Classes with Reference Semantics randomForest 4.6-14 Breiman and Cutler's Random Forests for Classification and Regression rappdirs 0.3.1 Application Directories: Determine Where to Save Data, Caches, and Logs raster 2.9-5 Geographic Data Analysis and Modeling RBGL 1.60.0 An interface to the BOOST graph library rcmdcheck 1.3.3 Run 'R CMD check' from 'R' and Capture Results RColorBrewer 1.1-2 ColorBrewer Palettes Rcpp 1.0.1 Seamless R and C++ Integration RcppArmadillo 0.9.500.2.0 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra Library RcppEigen 0.3.3.5.0 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra Library RcppGSL 0.3.6 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices RcppParallel 4.4.3 Parallel Programming Tools for 'Rcpp' RcppRoll 0.3.0 Efficient Rolling / Windowed Operations RCurl 1.95-4.12 General Network (HTTP/FTP/...) Client Interface for R readr 1.3.1 Read Rectangular Text Data readxl 1.3.1 Read Excel Files recipes 0.1.5 Preprocessing Tools to Create Design Matrices registry 0.5-1 Infrastructure for R Package Registries relaimpo 2.2-3 Relative Importance of Regressors in Linear Models rematch 1.0.1 Match Regular Expressions with a Nicer 'API' remotes 2.0.4 R Package Installation from Remote Repositories, Including 'GitHub' Repitools 1.30.0 Epigenomic tools reprex 0.3.0 Prepare Reproducible Example Code via the Clipboard reshape 0.8.8 Flexibly Reshape Data reshape2 1.4.3 Flexibly Reshape Data: A Reboot of the Reshape Package reticulate 1.13 Interface to 'Python' rgdal 1.4-4 Bindings for the 'Geospatial' Data Abstraction Library rgeos 0.4-3 Interface to Geometry Engine - Open Source ('GEOS') rgl 0.100.24 3D Visualization Using OpenGL Rglpk 0.6-4 R/GNU Linear Programming Kit Interface Rgraphviz 2.28.0 Provides plotting capabilities for R graph objects rhdf5 2.28.0 R Interface to HDF5 Rhdf5lib 1.6.0 hdf5 library as an R package Rhtslib 1.16.1 HTSlib high-throughput sequencing library as an R package Ringo 1.48.0 R Investigation of ChIP-chip Oligoarrays rio 0.5.16 A Swiss-Army Knife for Data I/O rjags 4-8 Bayesian Graphical Models using MCMC RJSONIO 1.3-1.2 Serialize R Objects to JSON, JavaScript Object Notation rlang 0.3.4 Functions for Base Types and Core R and 'Tidyverse' Features rlecuyer 0.3-4 R Interface to RNG with Multiple Streams rlist 0.4.6.1 A Toolbox for Non-Tabular Data Manipulation rmarkdown 1.13 Dynamic Documents for R Rmpfr 0.7-2 R MPFR - Multiple Precision Floating-Point Reliable Rmpi 0.6-9 Interface (Wrapper) to MPI (Message-Passing Interface) rms 5.1-3.1 Regression Modeling Strategies RMySQL 0.10.17 Database Interface and 'MySQL' Driver for R RNetCDF 1.9-1 Interface to NetCDF Datasets rngtools 1.4 Utility Functions for Working with Random Number Generators robust 0.4-18 Port of the S+ \"Robust Library\" robustbase 0.93-5 Basic Robust Statistics ROC 1.60.0 utilities for ROC, with uarray focus rpart 4.1-15 Recursive Partitioning and Regression Trees RPMM 1.25 Recursively Partitioned Mixture Model rprojroot 1.3-2 Finding Files in Project Subdirectories rrcov 1.4-7 Scalable Robust Estimators with High Breakdown Point Rsamtools 2.0.0 Binary alignment (BAM), FASTA, variant call (BCF), and tabix file import rsconnect 0.8.13 Deployment Interface for R Markdown Documents and Shiny Applications Rsolnp 1.16 General Non-Linear Optimization RSpectra 0.15-0 Solvers for Large-Scale Eigenvalue and SVD Problems RSQLite 2.1.1 'SQLite' Interface for R rstan 2.18.2 R Interface to Stan rstanarm 2.18.2 Bayesian Applied Regression Modeling via Stan rstantools 1.5.1 Tools for Developing R Packages Interfacing with 'Stan' rstudioapi 0.10 Safely Access the RStudio API Rsubread 1.34.4 Subread Sequence Alignment and Counting for R rtracklayer 1.44.0 R interface to genome annotation files and the UCSC genome browser Rttf2pt1 1.3.7 'ttf2pt1' Program ruv 0.9.7 Detect and Remove Unwanted Variation using Negative Controls rvest 0.3.4 Easily Harvest (Scrape) Web Pages S4Vectors 0.22.0 Foundation of vector-like and list-like containers in Bioconductor saemix 2.2 Stochastic Approximation Expectation Maximization (SAEM) Algorithm sampling 2.8 Survey Sampling sandwich 2.5-1 Robust Covariance Matrix Estimators satellite 1.0.1 Handling and Manipulating Remote Sensing Data scales 1.0.0 Scale Functions for Visualization scam 1.2-4 Shape Constrained Additive Models scatterplot3d 0.3-41 3D Scatter Plot schoolmath 0.4 Functions and datasets for math used in school scrime 1.3.5 Analysis of High-Dimensional Categorical Data Such as SNP Data segmented 0.5-4.0 Regression Models with Break-Points / Change-Points Estimation selectr 0.4-1 Translate CSS Selectors to XPath Expressions sendmailR 1.2-1 send email using R seqinr 3.4-5 Biological Sequences Retrieval and Analysis seqLogo 1.50.0 Sequence logos for DNA sequence alignments sessioninfo 1.1.1 R Session Information sf 0.7-4 Simple Features for R sfsmisc 1.1-4 Utilities from 'Seminar fuer Statistik' ETH Zurich shiny 1.3.2 Web Application Framework for R shinyjs 1.0 Easily Improve the User Experience of Your Shiny Apps in Seconds shinystan 2.5.0 Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models shinythemes 1.1.2 Themes for Shiny ShortRead 1.42.0 FASTQ input and manipulation siggenes 1.58.0 Multiple Testing using SAM and Efron's Empirical Bayes Approaches simpleaffy 2.60.0 Very simple high level analysis of Affymetrix data slam 0.1-45 Sparse Lightweight Arrays and Matrices sn 1.5-4 The Skew-Normal and Related Distributions Such as the Skew-t sna 2.4 Tools for Social Network Analysis snow 0.4-3 Simple Network of Workstations SnowballC 0.6.0 Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library snowfall 1.84-6.1 Easier cluster computing (based on snow). softImpute 1.4 Matrix Completion via Iterative Soft-Thresholded SVD sourcetools 0.1.7 Tools for Reading, Tokenizing and Parsing R Code sp 1.3-1 Classes and Methods for Spatial Data spacyr 1.2 Wrapper to the 'spaCy' 'NLP' Library spam 2.2-2 SPArse Matrix SparseM 1.77 Sparse Linear Algebra spatial 7.3-11 Functions for Kriging and Point Pattern Analysis spData 0.3.0 Datasets for Spatial Analysis spdep 1.1-2 Spatial Dependence: Weighting Schemes, Statistics and Models splines 3.6.0 Regression Spline Functions and Classes SQUAREM 2017.10-1 Squared Extrapolation Methods for Accelerating EM-Like Monotone Algorithms stabledist 0.7-1 Stable Distribution Functions StanHeaders 2.18.1 C++ Header Files for Stan startupmsg 0.9.6 Utilities for Start-Up Messages statmod 1.4.32 Statistical Modeling statnet.common 4.3.0 Common R Scripts and Utilities Used by the Statnet Project Software stats 3.6.0 The R Stats Package stats4 3.6.0 Statistical Functions using S4 Classes stopwords 1.0 Multilingual Stopword Lists stringdist 0.9.5.2 Approximate String Matching and String Distance Functions stringi 1.4.3 Character String Processing Facilities stringr 1.4.0 Simple, Consistent Wrappers for Common String Operations SummarizedExperiment 1.14.0 SummarizedExperiment container survey 3.36 Analysis of Complex Survey Samples survival 2.44-1.1 Survival Analysis sva 3.32.1 Surrogate Variable Analysis svglite 1.2.2 An 'SVG' Graphics Device sys 3.2 Powerful and Reliable Tools for Running System Commands in R systemfit 1.1-22 Estimating Systems of Simultaneous Equations TAM 3.2-24 Test Analysis Modules tcltk 3.6.0 Tcl/Tk Interface testthat 2.1.1 Unit Testing for R tgp 2.4-14 Bayesian Treed Gaussian Process Models TH.data 1.0-10 TH's Data Archive threejs 0.3.1 Interactive 3D Scatter Plots, Networks and Globes tibble 2.1.2 Simple Data Frames tidyr 0.8.3 Easily Tidy Data with 'spread()' and 'gather()' Functions tidyselect 0.2.5 Select from a Set of Strings tidyverse 1.2.1 Easily Install and Load the 'Tidyverse' timeDate 3043.102 Rmetrics - Chronological and Calendar Objects tinytex 0.13 Helper Functions to Install and Maintain 'TeX Live', and Compile 'LaTeX' Documents tkrplot 0.0-24 TK Rplot tm 0.7-6 Text Mining Package tmap 2.2 Thematic Maps tmaptools 2.0-1 Thematic Map Tools tools 3.6.0 Tools for Package Development topicmodels 0.2-8 Topic Models truncnorm 1.0-8 Truncated Normal Distribution trust 0.1-7 Trust Region Optimization trustOptim 0.8.6.2 Trust Region Optimization for Nonlinear Functions with Sparse Hessians tseries 0.10-47 Time Series Analysis and Computational Finance TTR 0.23-4 Technical Trading Rules TxDb.Hsapiens.UCSC.hg19.knownGene 3.2.2 Annotation package for TxDb object(s) udunits2 0.13 Udunits-2 Bindings for R units 0.6-3 Measurement Units for R Vectors urca 1.3-0 Unit Root and Cointegration Tests for Time Series Data usethis 1.5.0 Automate Package and Project Setup utf8 1.1.4 Unicode Text Processing utils 3.6.0 The R Utils Package uuid 0.1-2 Tools for generating and handling of UUIDs V8 2.3 Embedded JavaScript Engine for R VariantAnnotation 1.30.1 Annotation of Genetic Variants vctrs 0.1.0 Vector Helpers vegan 2.5-5 Community Ecology Package VGAM 1.1-1 Vector Generalized Linear and Additive Models VineCopula 2.1.8 Statistical Inference of Vine Copulas viridis 0.5.1 Default Color Maps from 'matplotlib' viridisLite 0.3.0 Default Color Maps from 'matplotlib' (Lite Version) vsn 3.52.0 Variance stabilization and calibration for microarray data wateRmelon 1.28.0 Illumina 450 methylation array normalization and metrics webshot 0.5.1 Take Screenshots of Web Pages WGCNA 1.68 Weighted Correlation Network Analysis whisker 0.3-2 {{mustache}} for R, logicless templating widgetTools 1.62.0 Creates an interactive tcltk widget withr 2.1.2 Run Code 'With' Temporarily Modified Global State xfun 0.7 Miscellaneous Functions by 'Yihui Xie' XML 3.98-1.20 Tools for Parsing and Generating XML Within R and S-Plus xml2 1.2.0 Parse XML xopen 1.0.0 Open System Files, 'URLs', Anything xps 1.44.0 Processing and Analysis of Affymetrix Oligonucleotide Arrays including Exon Arrays, Whole Genome Arrays and Plate Arrays xtable 1.8-4 Export Tables to LaTeX or HTML xts 0.11-2 eXtensible Time Series XVector 0.24.0 Foundation of external vector representation and manipulation in Bioconductor yaml 2.2.0 Methods to Convert R Data to YAML and Back zeallot 0.1.0 Multiple, Unpacking, and Destructuring Assignment zip 2.0.2 Cross-Platform 'zip' Compression zlibbioc 1.30.0 An R packaged zlib-1.2.5 zoo 1.8-6 S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)","title":"R Packages"},{"location":"Installed_Software_Lists/r-packages/#r-packages","text":"We provide a collection of installed R packages for each release of R, as a bundle module . This page lists the packages for the current recommended R bundle. This can be loaded using: module load r/recommended The version of R provided with this bundle is currently R version 3.6.0 (2019-04-26). The following list was last updated at: 15:31:26 (+0100) on 09 Sep 2020. Module Version Description AnnotationForge 1.26.0 Tools for building SQLite-based annotation data packages BiocManager 1.30.4 Access the Bioconductor Project Package Repository BiocVersion 3.9.0 Set the appropriate version of Bioconductor packages Deriv 3.8.5 Symbolic Differentiation dgo 0.2.15 Dynamic Estimation of Group-Level Opinion dgodata 0.0.2 Data for the 'dgo' Package digest 0.6.21 Create Compact Hash Digests of R Objects GO.db 3.8.2 A set of annotation maps describing the entire Gene Ontology hgu95av2.db 3.2.3 Affymetrix Human Genome U95 Set annotation data (chip hgu95av2) HKprocess 0.0-2 Hurst-Kolmogorov Process hugene10stprobeset.db 8.7.0 Affymetrix hugene10 annotation data (chip hugene10stprobeset) hugene10sttranscriptcluster.db 8.7.0 Affymetrix hugene10 annotation data (chip hugene10sttranscriptcluster) Illumina450ProbeVariants.db 1.20.0 Annotation Package combining variant data from 1000 Genomes Project for Illumina HumanMethylation450 Bead Chip probes IlluminaHumanMethylation450kmanifest 0.4.0 Annotation for Illumina's 450k methylation arrays illuminaHumanv4.db 1.26.0 Illumina HumanHT12v4 annotation data (chip illuminaHumanv4) INLA 19.09.03 Full Bayesian Analysis of Latent Gaussian Models using Integrated Nested Laplace Approximations KEGG.db 3.2.3 A set of annotation maps for KEGG MCMCpack 1.4-4 Markov Chain Monte Carlo (MCMC) Package missMethyl 1.18.0 Analysing Illumina HumanMethylation BeadChip Data org.Hs.eg.db 3.8.2 Genome wide annotation for Human orthopolynom 1.0-5 Collection of functions for orthogonal and orthonormal polynomials polynom 1.4-0 A Collection of Functions to Implement a Class for Univariate Polynomial Manipulations quanteda.corpora 0.87 A collection of corpora for quanteda RcppZiggurat 0.1.5 'Rcpp' Integration of Different \"Ziggurat\" Normal RNG Implementations Rfast 1.9.9 A Collection of Efficient and Extremely Fast R Functions sf 0.7-7 Simple Features for R splancs 2.01-40 Spatial and Space-Time Point Pattern Analysis abc 2.1 Tools for Approximate Bayesian Computation (ABC) abc.data 1.0 Data Only: Tools for Approximate Bayesian Computation (ABC) abind 1.4-5 Combine Multidimensional Arrays acepack 1.4.1 ACE and AVAS for Selecting Multiple Regression Transformations adapt 1.0-4 adapt -- multidimensional numerical integration ade4 1.7-13 Analysis of Ecological Data: Exploratory and Euclidean Methods in Environmental Sciences adegenet 2.1.1 Exploratory Analysis of Genetic and Genomic Data ADGofTest 0.3 Anderson-Darling GoF test affxparser 1.56.0 Affymetrix File Parsing SDK affy 1.62.0 Methods for Affymetrix Oligonucleotide Arrays affydata 1.32.0 Affymetrix Data for Demonstration Purpose affyio 1.54.0 Tools for parsing Affymetrix data files affylmGUI 1.58.0 GUI for limma Package with Affymetrix Microarrays affyPLM 1.60.0 Methods for fitting probe-level models affyQCReport 1.62.0 QC Report Generation for affyBatch objects akima 0.6-2 Interpolation of Irregularly and Regularly Spaced Data annaffy 1.56.0 Annotation tools for Affymetrix biological metadata annmap 1.26.0 Genome annotation and visualisation package pertaining to Affymetrix arrays and NGS analysis. annotate 1.62.0 Annotation for microarrays AnnotationDbi 1.46.0 Manipulation of SQLite-based annotations in Bioconductor AnnotationFilter 1.8.0 Facilities for Filtering Bioconductor Annotation Resources ape 5.3 Analyses of Phylogenetics and Evolution arm 1.10-1 Data Analysis Using Regression and Multilevel/Hierarchical Models aroma.affymetrix 3.2.0 Analysis of Large Affymetrix Microarray Data Sets aroma.apd 0.6.0 A Probe-Level Data File Format Used by 'aroma.affymetrix' [deprecated] aroma.core 3.2.0 Core Methods and Classes Used by 'aroma.*' Packages Part of the Aroma Framework aroma.light 3.14.0 Light-Weight Methods for Normalization and Visualization of Microarray Data using Only Basic R Data Types askpass 1.1 Safe Password Entry for R, Git, and SSH assertthat 0.2.1 Easy Pre and Post Assertions backports 1.1.4 Reimplementations of Functions Introduced Since R-3.0.0 base 3.6.0 The R Base Package base64 2.0 Base64 Encoder and Decoder base64enc 0.1-3 Tools for base64 encoding BaSTA 1.9.4 Age-Specific Survival Analysis from Incomplete Capture-Recapture/Recovery Data BatchJobs 1.8 Batch Computing with R bayesplot 1.7.0 Plotting for Bayesian Models BBmisc 1.11 Miscellaneous Helper Functions for B. Bischl beachmat 2.0.0 Compiling Bioconductor to Handle Each Matrix Type beadarray 2.34.0 Quality assessment and low-level analysis for Illumina BeadArray data beadarrayExampleData 1.22.0 Example data for the beadarray package BeadDataPackR 1.36.0 Compression of Illumina BeadArray data beanplot 1.2 Visualization via Beanplots (like Boxplot/Stripchart/Violin Plot) BH 1.69.0-1 Boost C++ Header Files BiasedUrn 1.07 Biased Urn Model Distributions bibtex 0.4.2 Bibtex Parser bio3d 2.3-4 Biological Structure Analysis Biobase 2.44.0 Biobase: Base functions for Bioconductor BiocFileCache 1.8.0 Manage Files Across Sessions BiocGenerics 0.30.0 S4 generic functions used in Bioconductor BiocParallel 1.18.0 Bioconductor facilities for parallel evaluation biomaRt 2.40.1 Interface to BioMart databases (i.e. Ensembl) Biostrings 2.52.0 Efficient manipulation of biological strings biovizBase 1.32.0 Basic graphic utilities for visualization of genomic data. bit 1.1-14 A Class for Vectors of 1-Bit Booleans bit64 0.9-7 A S3 Class for Vectors of 64bit Integers bitops 1.0-6 Bitwise Operations blob 1.1.1 A Simple S3 Class for Representing Vectors of Binary Data ('BLOBS') blockmodeling 0.3.4 Generalized and Classical Blockmodeling of Valued Networks boot 1.3-22 Bootstrap Functions (Originally by Angelo Canty for S) brew 1.0-6 Templating Framework for Report Generation broom 0.5.2 Convert Statistical Analysis Objects into Tidy Tibbles BSgenome 1.52.0 Software infrastructure for efficient representation of full genomes and their SNPs BSgenome.Hsapiens.UCSC.hg19 1.4.0 Full genome sequences for Homo sapiens (UCSC version hg19) bsseq 1.20.0 Analyze, manage and store bisulfite sequencing data bumphunter 1.26.0 Bump Hunter callr 3.2.0 Call R from R car 3.0-3 Companion to Applied Regression carData 3.0-2 Companion to Applied Regression Data Sets caret 6.0-84 Classification and Regression Training Category 2.50.0 Category Analysis caTools 1.17.1.2 Tools: moving window statistics, GIF, Base64, ROC AUC, etc. CDM 7.3-17 Cognitive Diagnosis Modeling cellranger 1.1.0 Translate Spreadsheet Cell Ranges to Rows and Columns ChAMP 2.14.0 Chip Analysis Methylation Pipeline for Illumina HumanMethylation450 and EPIC ChAMPdata 2.16.0 Data Packages for ChAMP package checkmate 1.9.4 Fast and Versatile Argument Checks class 7.3-15 Functions for Classification classInt 0.3-3 Choose Univariate Class Intervals cli 1.1.0 Helpers for Developing Command Line Interfaces clipr 0.6.0 Read and Write from the System Clipboard clisymbols 1.2.0 Unicode Symbols at the R Prompt clue 0.3-57 Cluster Ensembles cluster 2.0.8 \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. cmprsk 2.2-8 Subdistribution Analysis of Competing Risks coda 0.19-3 Output Analysis and Diagnostics for MCMC codetools 0.2-16 Code Analysis Tools for R colorRamps 2.3 Builds color tables colorspace 1.4-1 A Toolbox for Manipulating and Assessing Colors and Palettes colourpicker 1.0 A Colour Picker Tool for Shiny and for Selecting Colours in Plots combinat 0.0-8 combinatorics utilities compiler 3.6.0 The R Compiler Package copula 0.999-19.1 Multivariate Dependence with Copulas copynumber 1.24.0 Segmentation of single- and multi-track copy number data by penalized least squares regression. corpcor 1.6.9 Efficient Estimation of Covariance and (Partial) Correlation corrplot 0.84 Visualization of a Correlation Matrix crayon 1.3.4 Colored Terminal Output crosstalk 1.0.0 Inter-Widget Interactivity for HTML Widgets curl 3.3 A Modern and Flexible Web Client for R data.table 1.12.2 Extension of data.frame datasets 3.6.0 The R Datasets Package DBI 1.0.0 R Database Interface dbplyr 1.4.0 A 'dplyr' Back End for Databases DelayedArray 0.10.0 A unified framework for working transparently with on-disk and in-memory array-like datasets DelayedMatrixStats 1.6.0 Functions that Apply to Rows and Columns of 'DelayedMatrix' Objects deldir 0.1-16 Delaunay Triangulation and Dirichlet (Voronoi) Tessellation dendextend 1.12.0 Extending 'dendrogram' Functionality in R DEoptimR 1.0-8 Differential Evolution Optimization in Pure R desc 1.2.0 Manipulate DESCRIPTION Files DESeq 1.36.0 Differential gene expression analysis based on the negative binomial distribution DESeq2 1.24.0 Differential gene expression analysis based on the negative binomial distribution Design 2.3-0 Design Package devtools 2.0.2 Tools to Make Developing R Packages Easier DEXSeq 1.30.0 Inference of differential exon usage in RNA-Seq dichromat 2.0-0 Color Schemes for Dichromats digest 0.6.20 Create Compact Hash Digests of R Objects diptest 0.75-7 Hartigan's Dip Test Statistic for Unimodality - Corrected distr 2.8.0 Object Oriented Implementation of Distributions distrEx 2.8.0 Extensions of Package 'distr' DMRcate 1.20.0 Methylation array and sequencing spatial analysis methods DMRcatedata 1.20.0 Data Package for DMRcate package DNAcopy 1.58.0 DNA copy number data analysis doMC 1.3.5 Foreach Parallel Adaptor for 'parallel' doParallel 1.0.14 Foreach Parallel Adaptor for the 'parallel' Package doRNG 1.7.1 Generic Reproducible Parallel Backend for 'foreach' Loops dotCall64 1.0-0 Enhanced Foreign Function Interface Supporting Long Vectors dplyr 0.8.1 A Grammar of Data Manipulation DSS 2.32.0 Dispersion shrinkage for sequencing data DT 0.6 A Wrapper of the JavaScript Library 'DataTables' dygraphs 1.1.1.6 Interface to 'Dygraphs' Interactive Time Series Charting Library dynamicTreeCut 1.63-1 Methods for Detection of Clusters in Hierarchical Clustering Dendrograms DynDoc 1.62.0 Dynamic document tools e1071 1.7-2 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien easyRNASeq 2.20.0 Count summarization and normalization for RNA-Seq data EBSeq 1.24.0 An R package for gene and isoform differential expression analysis of RNA-seq data edgeR 3.26.5 Empirical Analysis of Digital Gene Expression Data in R effects 4.1-1 Effect Displays for Linear, Generalized Linear, and Other Models ellipse 0.4.1 Functions for Drawing Ellipses and Ellipse-Like Confidence Regions ellipsis 0.1.0 Tools for Working with ... ensembldb 2.8.0 Utilities to create and use Ensembl-based annotation databases Epi 2.37 A Package for Statistical Analysis in Epidemiology erer 2.5 Empirical Research in Economics with R estimability 1.3 Tools for Assessing Estimability of Linear Predictions etm 1.0.5 Empirical Transition Matrix evaluate 0.14 Parsing and Evaluation Tools that Provide More Details than the Default evd 2.3-3 Functions for Extreme Value Distributions expm 0.999-4 Matrix Exponential, Log, 'etc' extrafont 0.17 Tools for using fonts extrafontdb 1.0 Package for holding the database for the extrafont package FactoMineR 1.42 Multivariate Exploratory Data Analysis and Data Mining fail 1.3 File Abstraction Interface Layer (FAIL) fansi 0.4.0 ANSI Control Sequence Aware String Functions fastcluster 1.1.25 Fast Hierarchical Clustering Routines for R and 'Python' fastICA 1.2-1 FastICA Algorithms to Perform ICA and Projection Pursuit fastmatch 1.1-0 Fast match() function FDb.InfiniumMethylation.hg19 2.2.0 Annotation package for Illumina Infinium DNA methylation probes fdrtool 1.2.15 Estimation of (Local) False Discovery Rates and Higher Criticism FEM 3.12.0 Identification of Functional Epigenetic Modules fields 9.8-3 Tools for Spatial Data fit.models 0.5-14 Compare Fitted Models flashClust 1.01-2 Implementation of optimal hierarchical clustering flexmix 2.3-15 Flexible Mixture Modeling forcats 0.4.0 Tools for Working with Categorical Variables (Factors) foreach 1.4.4 Provides Foreach Looping Construct for R foreign 0.8-71 Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... formatR 1.6 Format R Code Automatically Formula 1.2-3 Extended Model Formulas fpc 2.2-3 Flexible Procedures for Clustering fs 1.3.1 Cross-Platform File System Operations Based on 'libuv' futile.logger 1.4.3 A Logging Utility for R futile.options 1.0.1 Futile Options Management future 1.13.0 Unified Parallel and Distributed Processing in R for Everyone gam 1.16.1 Generalized Additive Models gamlss 5.1-4 Generalised Additive Models for Location Scale and Shape gamlss.data 5.1-4 GAMLSS Data gamlss.dist 5.1-4 Distributions for Generalized Additive Models for Location Scale and Shape gamlss.mx 4.3-5 Fitting Mixture Distributions with GAMLSS gamlss.nl 4.1-0 Fitting non linear parametric GAMLSS models gcrma 2.56.0 Background Adjustment Using Sequence Information gdalUtils 2.0.1.14 Wrappers for the Geospatial Data Abstraction Library (GDAL) Utilities gdata 2.18.0 Various R Programming Tools for Data Manipulation gdtools 0.1.8 Utilities for Graphical Rendering genefilter 1.66.0 genefilter: methods for filtering genes from high-throughput experiments geneLenDataBase 1.20.0 Lengths of mRNA transcripts for a number of genomes GeneNet 1.2.13 Modeling and Inferring Gene Networks geneplotter 1.62.0 Graphics related functions for Bioconductor generics 0.0.2 Common S3 Generics not Provided by Base R Methods Related to Model Fitting genetics 1.3.8.1.2 Population Genetics GenomeGraphs 1.44.0 Plotting genomic information from Ensembl GenomeInfoDb 1.20.0 Utilities for manipulating chromosome names, including modifying them to follow a particular naming style GenomeInfoDbData 1.2.1 Species and taxonomy ID look up tables used by GenomeInfoDb genomeIntervals 1.40.0 Operations on genomic intervals GenomicAlignments 1.20.1 Representation and manipulation of short genomic alignments GenomicFeatures 1.36.3 Conveniently import and query gene models GenomicRanges 1.36.0 Representation and manipulation of genomic intervals GEOquery 2.52.0 Get data from NCBI Gene Expression Omnibus (GEO) GGally 1.4.0 Extension to 'ggplot2' ggplot2 3.2.0 Create Elegant Data Visualisations Using the Grammar of Graphics ggrepel 0.8.1 Automatically Position Non-Overlapping Text Labels with 'ggplot2' ggridges 0.5.1 Ridgeline Plots in 'ggplot2' gh 1.0.1 'GitHub' 'API' git2r 0.25.2 Provides Access to Git Repositories GJRM 0.2 Generalised Joint Regression Modelling glmnet 2.0-18 Lasso and Elastic-Net Regularized Generalized Linear Models globals 0.12.4 Identify Global Objects in R Expressions globaltest 5.38.0 Testing Groups of Covariates/Features for Association with a Response Variable, with Applications to Gene Set Testing glue 1.3.1 Interpreted String Literals gmodels 2.18.1 Various R Programming Tools for Model Fitting gmp 0.5-13.5 Multiple Precision Arithmetic goseq 1.36.0 Gene Ontology analyser for RNA-seq and other length biased data GOstats 2.50.0 Tools for manipulating GO and microarrays gower 0.2.1 Gower's Distance gplots 3.0.1.1 Various R Programming Tools for Plotting Data graph 1.62.0 graph: A package to handle graph data structures graphics 3.6.0 The R Graphics Package grDevices 3.6.0 The R Graphics Devices and Support for Colours and Fonts grid 3.6.0 The Grid Graphics Package gridExtra 2.3 Miscellaneous Functions for \"Grid\" Graphics GSEABase 1.46.0 Gene set enrichment data structures and methods gsl 2.1-6 Wrapper for the Gnu Scientific Library gsmoothr 0.1.7 Smoothing tools gtable 0.3.0 Arrange 'Grobs' in Tables gtools 3.8.1 Various R Programming Tools Gviz 1.28.0 Plotting data and annotation information along genomic coordinates HAC 1.0-5 Estimation, Simulation and Visualization of Hierarchical Archimedean Copulae (HAC) haplo.stats 1.7.9 Statistical Analysis of Haplotypes with Traits and Covariates when Linkage Phase is Ambiguous haven 2.1.0 Import and Export 'SPSS', 'Stata' and 'SAS' Files HDF5Array 1.12.1 HDF5 backend for DelayedArray objects hexbin 1.27.3 Hexagonal Binning Routines HI 0.4 Simulation from distributions supported by nested hyperplanes highr 0.8 Syntax Highlighting for R Source Code Hmisc 4.2-0 Harrell Miscellaneous hms 0.4.2 Pretty Time of Day HotDeckImputation 1.1.0 Hot Deck Imputation Methods for Missing Data htmlTable 1.13.1 Advanced Tables for Markdown/HTML htmltools 0.3.6 Tools for HTML htmlwidgets 1.3 HTML Widgets for R httpuv 1.5.1 HTTP and WebSocket Server Library httr 1.4.0 Tools for Working with URLs and HTTP hwriter 1.3.2 HTML Writer - Outputs R objects in HTML format igraph 1.2.4.1 Network Analysis and Visualization IlluminaHumanMethylation450kanno.ilmn12.hg19 0.6.0 Annotation for Illumina's 450k methylation arrays IlluminaHumanMethylationEPICanno.ilm10b2.hg19 0.6.0 Annotation for Illumina's EPIC methylation arrays IlluminaHumanMethylationEPICanno.ilm10b4.hg19 0.6.0 Annotation for Illumina's EPIC methylation arrays IlluminaHumanMethylationEPICmanifest 0.3.0 Manifest for Illumina's EPIC methylation arrays illuminaio 0.26.0 Parsing Illumina Microarray Output Files impute 1.58.0 impute: Imputation for microarray data ini 0.3.1 Read and Write '.ini' Files inline 0.3.15 Functions to Inline C, C++, Fortran Function Calls from R intervals 0.15.1 Tools for Working with Points and Intervals ipred 0.9-9 Improved Predictors IRanges 2.18.1 Foundation of integer range manipulation in Bioconductor ISOcodes 2019.04.22 Selected ISO Codes isva 1.9 Independent Surrogate Variable Analysis iterators 1.0.10 Provides Iterator Construct for R JADE 2.0-1 Blind Source Separation Methods Based on Joint Diagonalization and Some BSS Performance Criteria jsonlite 1.6 A Robust, High Performance JSON Parser and Generator for R kdecopula 0.9.2 Kernel Smoothing for Bivariate Copula Densities kernlab 0.9-27 Kernel-Based Machine Learning Lab KernSmooth 2.23-15 Functions for Kernel Smoothing Supporting Wand & Jones (1995) knitr 1.23 A General-Purpose Package for Dynamic Report Generation in R kohonen 3.0.8 Supervised and Unsupervised Self-Organising Maps kpmt 0.1.0 Known Population Median Test labeling 0.3 Axis Labeling lambda.r 1.2.3 Modeling Data with Functional Programming later 0.8.0 Utilities for Delaying Function Execution lattice 0.20-38 Trellis Graphics for R latticeExtra 0.6-28 Extra Graphical Utilities Based on Lattice lava 1.6.5 Latent Variable Models lazyeval 0.2.2 Lazy (Non-Standard) Evaluation leafem 0.0.1 'leaflet' Extensions for 'mapview' leaflet 2.0.2 Create Interactive Web Maps with the JavaScript 'Leaflet' Library leafpop 0.0.1 Include Tables, Images and Graphs in Leaflet Pop-Ups leaps 3.0 Regression Subset Selection LearnBayes 2.15.1 Functions for Learning Bayesian Inference limma 3.40.2 Linear Models for Microarray Data listenv 0.7.0 Environments Behaving (Almost) as Lists lme4 1.1-21 Linear Mixed-Effects Models using 'Eigen' and S4 lmtest 0.9-37 Testing Linear Regression Models locfit 1.5-9.1 Local Regression, Likelihood and Density Estimation. longitudinal 1.1.12 Analysis of Multiple Time Course Data loo 2.1.0 Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models lpSolve 5.6.13.1 Interface to 'Lp_solve' v. 5.5 to Solve Linear/Integer Programs LSD 4.0-0 Lots of Superior Depictions ltm 1.1-1 Latent Trait Models under IRT lubridate 1.7.4 Make Dealing with Dates a Little Easier lumi 2.36.0 BeadArray Specific Methods for Illumina Methylation and Expression Microarrays lwgeom 0.1-7 Bindings to Selected 'liblwgeom' Functions for Simple Features made4 1.58.0 Multivariate analysis of microarray data using ADE4 magic 1.5-9 Create and Investigate Magic Squares magrittr 1.5 A Forward-Pipe Operator for R manipulateWidget 0.10.0 Add Even More Interactivity to Interactive Charts maps 3.3.0 Draw Geographical Maps maptools 0.9-5 Tools for Handling Spatial Objects maptree 1.4-7 Mapping, pruning, and graphing tree models mapview 2.7.0 Interactive Viewing of Spatial Data in R markdown 0.9 'Markdown' Rendering for R marray 1.62.0 Exploratory analysis for two-color spotted microarray data MASS 7.3-51.4 Support Functions and Datasets for Venables and Ripley's MASS Matrix 1.2-17 Sparse and Dense Matrix Classes and Methods matrixcalc 1.0-3 Collection of functions for matrix calculations MatrixModels 0.4-1 Modelling with Sparse And Dense Matrices matrixStats 0.54.0 Functions that Apply to Rows and Columns of Matrices (and to Vectors) mclust 5.4.4 Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation mcmc 0.9-6 Markov Chain Monte Carlo memoise 1.1.0 Memoisation of Functions metafor 2.1-0 Meta-Analysis Package for R methods 3.6.0 Formal Methods and Classes methylumi 2.30.0 Handle Illumina methylation data mgcv 1.8-28 Mixed GAM Computation Vehicle with Automatic Smoothness Estimation mime 0.6 Map Filenames to MIME Types minfi 1.30.0 Analyze Illumina Infinium DNA methylation arrays miniUI 0.1.1.1 Shiny UI Widgets for Small Screens minqa 1.2.4 Derivative-free optimization algorithms by quadratic approximation missMethyl 1.18.0 Analysing Illumina HumanMethylation BeadChip Data mitools 2.4 Tools for Multiple Imputation of Missing Data mlr 2.14.0 Machine Learning in R mnormt 1.5-5 The Multivariate Normal and t Distributions ModelMetrics 1.2.2 Rapid Calculation of Model Metrics modelr 0.1.4 Modelling Functions that Work with the Pipe modeltools 0.2-22 Tools and Classes for Statistical Models msm 1.6.7 Multi-State Markov and Hidden Markov Models in Continuous Time mstate 0.2.11 Data Preparation, Estimation and Prediction in Multi-State Models multcomp 1.4-10 Simultaneous Inference in General Parametric Models multtest 2.40.0 Resampling-based multiple hypothesis testing munsell 0.5.0 Utilities for Using Munsell Colours mvtnorm 1.0-11 Multivariate Normal and t Distributions network 1.15 Classes for Relational Data nleqslv 3.3.2 Solve Systems of Nonlinear Equations nlme 3.1-139 Linear and Nonlinear Mixed Effects Models nloptr 1.2.1 R Interface to NLopt NLP 0.2-0 Natural Language Processing Infrastructure nnet 7.3-12 Feed-Forward Neural Networks and Multinomial Log-Linear Models nor1mix 1.3-0 Normal aka Gaussian (1-d) Mixture Models (S3 Classes and Methods) numDeriv 2016.8-1.1 Accurate Numerical Derivatives nutshell 2.0 Data for \"R in a Nutshell\" nutshell.audioscrobbler 1.0 Audioscrobbler data for \"R in a Nutshell\" nutshell.bbdb 1.0 Baseball Database for \"R in a Nutshell\" OPE 0.7 Outer-product emulator openssl 1.4 Toolkit for Encryption, Signatures and Certificates Based on OpenSSL openxlsx 4.1.0.1 Read, Write and Edit XLSX Files packrat 0.5.0 A Dependency Management System for Projects and their R Package Dependencies panelAR 0.1 Estimation of Linear AR(1) Panel Data Models with Cross-Sectional Heteroskedasticity and/or Correlation parallel 3.6.0 Support for Parallel computation in R parallelMap 1.4 Unified Interface to Parallelization Back-Ends ParamHelpers 1.12 Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning parcor 0.2-6 Regularized estimation of partial correlation matrices pbkrtest 0.4-7 Parametric Bootstrap and Kenward Roger Based Methods for Mixed Model Comparison pcaPP 1.9-73 Robust PCA by Projection Pursuit pegas 0.11 Population and Evolutionary Genetics Analysis System permute 0.9-5 Functions for Generating Restricted Permutations of Data phangorn 2.5.5 Phylogenetic Reconstruction and Analysis pillar 1.4.1 Coloured Formatting for Columns pixmap 0.4-11 Bitmap Images (``Pixel Maps'') pkgbuild 1.0.3 Find Tools Needed to Build R Packages pkgconfig 2.0.2 Private Configuration for 'R' Packages pkgload 1.0.2 Simulate Package Installation and Attach pkgmaker 0.27 Development Utilities for R Packages plogr 0.2.0 The 'plog' C++ Logging Library plotly 4.9.0 Create Interactive Web Graphics via 'plotly.js' pls 2.7-1 Partial Least Squares and Principal Component Regression plyr 1.8.4 Tools for Splitting, Applying and Combining Data png 0.1-7 Read and write PNG images poLCA 1.4.1 Polytomous variable Latent Class Analysis polspline 1.1.14 Polynomial Spline Routines polycor 0.7-9 Polychoric and Polyserial Correlations poweRlaw 0.70.2 Analysis of Heavy Tailed Distributions ppls 1.6-1.1 Penalized Partial Least Squares prabclus 2.3-1 Functions for Clustering and Testing of Presence-Absence, Abundance and Multilocus Genetic Data pracma 2.2.5 Practical Numerical Math Functions praise 1.0.0 Praise Users preprocessCore 1.46.0 A collection of pre-processing functions prettydoc 0.2.1 Creating Pretty Documents from R Markdown prettyunits 1.0.2 Pretty, Human Readable Formatting of Quantities processx 3.3.1 Execute and Control System Processes prodlim 2018.04.18 Product-Limit Estimation for Censored Event History Analysis progress 1.2.2 Terminal Progress Bars promises 1.0.1 Abstractions for Promise-Based Asynchronous Programming ProtGenerics 1.16.0 S4 generic functions for Bioconductor proteomics infrastructure proto 1.0.0 Prototype Object-Based Programming proxyC 0.1.5 Computes Proximity in Large Sparse Matrices ps 1.3.0 List, Query, Manipulate System Processes PSCBS 0.65.0 Analysis of Parent-Specific DNA Copy Numbers pspline 1.0-18 Penalized Smoothing Splines psych 1.8.12 Procedures for Psychological, Psychometric, and Personality Research purrr 0.3.2 Functional Programming Tools qrng 0.0-5 (Randomized) Quasi-Random Number Generators quadprog 1.5-7 Functions to Solve Quadratic Programming Problems quanteda 1.5.1 Quantitative Analysis of Textual Data quantmod 0.4-14 Quantitative Financial Modelling Framework quantreg 5.41 Quantile Regression qvalue 2.16.0 Q-value estimation for false discovery rate control R.cache 0.13.0 Fast and Light-Weight Caching (Memoization) of Objects and Results to Speed Up Computations R.devices 2.16.0 Unified Handling of Graphics Devices R.filesets 2.13.0 Easy Handling of and Access to Files Organized in Structured Directories R.huge 0.9.0 Methods for Accessing Huge Amounts of Data [deprecated] R.methodsS3 1.7.1 S3 Methods Simplified R.oo 1.22.0 R Object-Oriented Programming with or without References R.rsp 0.43.1 Dynamic Generation of Scientific Reports R.utils 2.9.0 Various Programming Utilities R2HTML 2.3.2 HTML Exportation for R Objects R2jags 0.5-7 Using R to Run 'JAGS' R2WinBUGS 2.1-21 Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS' R6 2.4.0 Encapsulated Classes with Reference Semantics randomForest 4.6-14 Breiman and Cutler's Random Forests for Classification and Regression rappdirs 0.3.1 Application Directories: Determine Where to Save Data, Caches, and Logs raster 2.9-5 Geographic Data Analysis and Modeling RBGL 1.60.0 An interface to the BOOST graph library rcmdcheck 1.3.3 Run 'R CMD check' from 'R' and Capture Results RColorBrewer 1.1-2 ColorBrewer Palettes Rcpp 1.0.1 Seamless R and C++ Integration RcppArmadillo 0.9.500.2.0 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra Library RcppEigen 0.3.3.5.0 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra Library RcppGSL 0.3.6 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices RcppParallel 4.4.3 Parallel Programming Tools for 'Rcpp' RcppRoll 0.3.0 Efficient Rolling / Windowed Operations RCurl 1.95-4.12 General Network (HTTP/FTP/...) Client Interface for R readr 1.3.1 Read Rectangular Text Data readxl 1.3.1 Read Excel Files recipes 0.1.5 Preprocessing Tools to Create Design Matrices registry 0.5-1 Infrastructure for R Package Registries relaimpo 2.2-3 Relative Importance of Regressors in Linear Models rematch 1.0.1 Match Regular Expressions with a Nicer 'API' remotes 2.0.4 R Package Installation from Remote Repositories, Including 'GitHub' Repitools 1.30.0 Epigenomic tools reprex 0.3.0 Prepare Reproducible Example Code via the Clipboard reshape 0.8.8 Flexibly Reshape Data reshape2 1.4.3 Flexibly Reshape Data: A Reboot of the Reshape Package reticulate 1.13 Interface to 'Python' rgdal 1.4-4 Bindings for the 'Geospatial' Data Abstraction Library rgeos 0.4-3 Interface to Geometry Engine - Open Source ('GEOS') rgl 0.100.24 3D Visualization Using OpenGL Rglpk 0.6-4 R/GNU Linear Programming Kit Interface Rgraphviz 2.28.0 Provides plotting capabilities for R graph objects rhdf5 2.28.0 R Interface to HDF5 Rhdf5lib 1.6.0 hdf5 library as an R package Rhtslib 1.16.1 HTSlib high-throughput sequencing library as an R package Ringo 1.48.0 R Investigation of ChIP-chip Oligoarrays rio 0.5.16 A Swiss-Army Knife for Data I/O rjags 4-8 Bayesian Graphical Models using MCMC RJSONIO 1.3-1.2 Serialize R Objects to JSON, JavaScript Object Notation rlang 0.3.4 Functions for Base Types and Core R and 'Tidyverse' Features rlecuyer 0.3-4 R Interface to RNG with Multiple Streams rlist 0.4.6.1 A Toolbox for Non-Tabular Data Manipulation rmarkdown 1.13 Dynamic Documents for R Rmpfr 0.7-2 R MPFR - Multiple Precision Floating-Point Reliable Rmpi 0.6-9 Interface (Wrapper) to MPI (Message-Passing Interface) rms 5.1-3.1 Regression Modeling Strategies RMySQL 0.10.17 Database Interface and 'MySQL' Driver for R RNetCDF 1.9-1 Interface to NetCDF Datasets rngtools 1.4 Utility Functions for Working with Random Number Generators robust 0.4-18 Port of the S+ \"Robust Library\" robustbase 0.93-5 Basic Robust Statistics ROC 1.60.0 utilities for ROC, with uarray focus rpart 4.1-15 Recursive Partitioning and Regression Trees RPMM 1.25 Recursively Partitioned Mixture Model rprojroot 1.3-2 Finding Files in Project Subdirectories rrcov 1.4-7 Scalable Robust Estimators with High Breakdown Point Rsamtools 2.0.0 Binary alignment (BAM), FASTA, variant call (BCF), and tabix file import rsconnect 0.8.13 Deployment Interface for R Markdown Documents and Shiny Applications Rsolnp 1.16 General Non-Linear Optimization RSpectra 0.15-0 Solvers for Large-Scale Eigenvalue and SVD Problems RSQLite 2.1.1 'SQLite' Interface for R rstan 2.18.2 R Interface to Stan rstanarm 2.18.2 Bayesian Applied Regression Modeling via Stan rstantools 1.5.1 Tools for Developing R Packages Interfacing with 'Stan' rstudioapi 0.10 Safely Access the RStudio API Rsubread 1.34.4 Subread Sequence Alignment and Counting for R rtracklayer 1.44.0 R interface to genome annotation files and the UCSC genome browser Rttf2pt1 1.3.7 'ttf2pt1' Program ruv 0.9.7 Detect and Remove Unwanted Variation using Negative Controls rvest 0.3.4 Easily Harvest (Scrape) Web Pages S4Vectors 0.22.0 Foundation of vector-like and list-like containers in Bioconductor saemix 2.2 Stochastic Approximation Expectation Maximization (SAEM) Algorithm sampling 2.8 Survey Sampling sandwich 2.5-1 Robust Covariance Matrix Estimators satellite 1.0.1 Handling and Manipulating Remote Sensing Data scales 1.0.0 Scale Functions for Visualization scam 1.2-4 Shape Constrained Additive Models scatterplot3d 0.3-41 3D Scatter Plot schoolmath 0.4 Functions and datasets for math used in school scrime 1.3.5 Analysis of High-Dimensional Categorical Data Such as SNP Data segmented 0.5-4.0 Regression Models with Break-Points / Change-Points Estimation selectr 0.4-1 Translate CSS Selectors to XPath Expressions sendmailR 1.2-1 send email using R seqinr 3.4-5 Biological Sequences Retrieval and Analysis seqLogo 1.50.0 Sequence logos for DNA sequence alignments sessioninfo 1.1.1 R Session Information sf 0.7-4 Simple Features for R sfsmisc 1.1-4 Utilities from 'Seminar fuer Statistik' ETH Zurich shiny 1.3.2 Web Application Framework for R shinyjs 1.0 Easily Improve the User Experience of Your Shiny Apps in Seconds shinystan 2.5.0 Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models shinythemes 1.1.2 Themes for Shiny ShortRead 1.42.0 FASTQ input and manipulation siggenes 1.58.0 Multiple Testing using SAM and Efron's Empirical Bayes Approaches simpleaffy 2.60.0 Very simple high level analysis of Affymetrix data slam 0.1-45 Sparse Lightweight Arrays and Matrices sn 1.5-4 The Skew-Normal and Related Distributions Such as the Skew-t sna 2.4 Tools for Social Network Analysis snow 0.4-3 Simple Network of Workstations SnowballC 0.6.0 Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library snowfall 1.84-6.1 Easier cluster computing (based on snow). softImpute 1.4 Matrix Completion via Iterative Soft-Thresholded SVD sourcetools 0.1.7 Tools for Reading, Tokenizing and Parsing R Code sp 1.3-1 Classes and Methods for Spatial Data spacyr 1.2 Wrapper to the 'spaCy' 'NLP' Library spam 2.2-2 SPArse Matrix SparseM 1.77 Sparse Linear Algebra spatial 7.3-11 Functions for Kriging and Point Pattern Analysis spData 0.3.0 Datasets for Spatial Analysis spdep 1.1-2 Spatial Dependence: Weighting Schemes, Statistics and Models splines 3.6.0 Regression Spline Functions and Classes SQUAREM 2017.10-1 Squared Extrapolation Methods for Accelerating EM-Like Monotone Algorithms stabledist 0.7-1 Stable Distribution Functions StanHeaders 2.18.1 C++ Header Files for Stan startupmsg 0.9.6 Utilities for Start-Up Messages statmod 1.4.32 Statistical Modeling statnet.common 4.3.0 Common R Scripts and Utilities Used by the Statnet Project Software stats 3.6.0 The R Stats Package stats4 3.6.0 Statistical Functions using S4 Classes stopwords 1.0 Multilingual Stopword Lists stringdist 0.9.5.2 Approximate String Matching and String Distance Functions stringi 1.4.3 Character String Processing Facilities stringr 1.4.0 Simple, Consistent Wrappers for Common String Operations SummarizedExperiment 1.14.0 SummarizedExperiment container survey 3.36 Analysis of Complex Survey Samples survival 2.44-1.1 Survival Analysis sva 3.32.1 Surrogate Variable Analysis svglite 1.2.2 An 'SVG' Graphics Device sys 3.2 Powerful and Reliable Tools for Running System Commands in R systemfit 1.1-22 Estimating Systems of Simultaneous Equations TAM 3.2-24 Test Analysis Modules tcltk 3.6.0 Tcl/Tk Interface testthat 2.1.1 Unit Testing for R tgp 2.4-14 Bayesian Treed Gaussian Process Models TH.data 1.0-10 TH's Data Archive threejs 0.3.1 Interactive 3D Scatter Plots, Networks and Globes tibble 2.1.2 Simple Data Frames tidyr 0.8.3 Easily Tidy Data with 'spread()' and 'gather()' Functions tidyselect 0.2.5 Select from a Set of Strings tidyverse 1.2.1 Easily Install and Load the 'Tidyverse' timeDate 3043.102 Rmetrics - Chronological and Calendar Objects tinytex 0.13 Helper Functions to Install and Maintain 'TeX Live', and Compile 'LaTeX' Documents tkrplot 0.0-24 TK Rplot tm 0.7-6 Text Mining Package tmap 2.2 Thematic Maps tmaptools 2.0-1 Thematic Map Tools tools 3.6.0 Tools for Package Development topicmodels 0.2-8 Topic Models truncnorm 1.0-8 Truncated Normal Distribution trust 0.1-7 Trust Region Optimization trustOptim 0.8.6.2 Trust Region Optimization for Nonlinear Functions with Sparse Hessians tseries 0.10-47 Time Series Analysis and Computational Finance TTR 0.23-4 Technical Trading Rules TxDb.Hsapiens.UCSC.hg19.knownGene 3.2.2 Annotation package for TxDb object(s) udunits2 0.13 Udunits-2 Bindings for R units 0.6-3 Measurement Units for R Vectors urca 1.3-0 Unit Root and Cointegration Tests for Time Series Data usethis 1.5.0 Automate Package and Project Setup utf8 1.1.4 Unicode Text Processing utils 3.6.0 The R Utils Package uuid 0.1-2 Tools for generating and handling of UUIDs V8 2.3 Embedded JavaScript Engine for R VariantAnnotation 1.30.1 Annotation of Genetic Variants vctrs 0.1.0 Vector Helpers vegan 2.5-5 Community Ecology Package VGAM 1.1-1 Vector Generalized Linear and Additive Models VineCopula 2.1.8 Statistical Inference of Vine Copulas viridis 0.5.1 Default Color Maps from 'matplotlib' viridisLite 0.3.0 Default Color Maps from 'matplotlib' (Lite Version) vsn 3.52.0 Variance stabilization and calibration for microarray data wateRmelon 1.28.0 Illumina 450 methylation array normalization and metrics webshot 0.5.1 Take Screenshots of Web Pages WGCNA 1.68 Weighted Correlation Network Analysis whisker 0.3-2 {{mustache}} for R, logicless templating widgetTools 1.62.0 Creates an interactive tcltk widget withr 2.1.2 Run Code 'With' Temporarily Modified Global State xfun 0.7 Miscellaneous Functions by 'Yihui Xie' XML 3.98-1.20 Tools for Parsing and Generating XML Within R and S-Plus xml2 1.2.0 Parse XML xopen 1.0.0 Open System Files, 'URLs', Anything xps 1.44.0 Processing and Analysis of Affymetrix Oligonucleotide Arrays including Exon Arrays, Whole Genome Arrays and Plate Arrays xtable 1.8-4 Export Tables to LaTeX or HTML xts 0.11-2 eXtensible Time Series XVector 0.24.0 Foundation of external vector representation and manipulation in Bioconductor yaml 2.2.0 Methods to Convert R Data to YAML and Back zeallot 0.1.0 Multiple, Unpacking, and Destructuring Assignment zip 2.0.2 Cross-Platform 'zip' Compression zlibbioc 1.30.0 An R packaged zlib-1.2.5 zoo 1.8-6 S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)","title":"R Packages"},{"location":"Other_Services/Aristotle/","text":"Aristotle \u00a7 Overview \u00a7 Aristotle is an interactive, Linux-based compute service for teaching, running on three nodes of the same specification as Legion's U-type nodes , each with 64 gigabytes of RAM and 16 cores. The nodes run the Red Hat Enterprise Linux operating system ( RHEL 7 ) and have a subset of the RCPS software stack available. The main aim of this service is to allow specific teaching courses to run that need to run Linux/UNIX applications, but it is available to all UCL users. Warning Aristotle is made available but is provided with minimal staff time and no budget. Any user may completely occupy the service and there is no system in place to prevent that. Access \u00a7 Anyone with a UCL userid and within the UCL institutional firewall can access Aristotle by connecting via ssh to: aristotle.rc.ucl.ac.uk This address can point to more than one actual server (via DNS round-robin); usually there are two available. To connect to a specific server from the set, you will need to know its number: for example, the second server has the address aristotle02.rc.ucl.ac.uk . When you connect, you should be shown which one you are connected to on your command line. The userid and password you need to connect with are those provided to you by Information Services Division . If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you still cannot get access, please contact us at rc-support@ucl.ac.uk . If you are outside the UCL firewall, you will need to connect to Socrates first and then SSH in to Aristotle from there. User Environment \u00a7 Aristotle runs Red Hat Enterprise Linux 7 and NFS mounts the RCPS Software Stack . As this machine is intended for teaching, work has focused on getting specific applications required for specific courses to work and these are: SAC Phon GMT Fortran compilers (of which there are a large variety) Packages are available through modules and users should consult the relevant modules documentation .","title":"Aristotle"},{"location":"Other_Services/Aristotle/#aristotle","text":"","title":"Aristotle"},{"location":"Other_Services/Aristotle/#overview","text":"Aristotle is an interactive, Linux-based compute service for teaching, running on three nodes of the same specification as Legion's U-type nodes , each with 64 gigabytes of RAM and 16 cores. The nodes run the Red Hat Enterprise Linux operating system ( RHEL 7 ) and have a subset of the RCPS software stack available. The main aim of this service is to allow specific teaching courses to run that need to run Linux/UNIX applications, but it is available to all UCL users. Warning Aristotle is made available but is provided with minimal staff time and no budget. Any user may completely occupy the service and there is no system in place to prevent that.","title":"Overview"},{"location":"Other_Services/Aristotle/#access","text":"Anyone with a UCL userid and within the UCL institutional firewall can access Aristotle by connecting via ssh to: aristotle.rc.ucl.ac.uk This address can point to more than one actual server (via DNS round-robin); usually there are two available. To connect to a specific server from the set, you will need to know its number: for example, the second server has the address aristotle02.rc.ucl.ac.uk . When you connect, you should be shown which one you are connected to on your command line. The userid and password you need to connect with are those provided to you by Information Services Division . If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you still cannot get access, please contact us at rc-support@ucl.ac.uk . If you are outside the UCL firewall, you will need to connect to Socrates first and then SSH in to Aristotle from there.","title":"Access"},{"location":"Other_Services/Aristotle/#user-environment","text":"Aristotle runs Red Hat Enterprise Linux 7 and NFS mounts the RCPS Software Stack . As this machine is intended for teaching, work has focused on getting specific applications required for specific courses to work and these are: SAC Phon GMT Fortran compilers (of which there are a large variety) Packages are available through modules and users should consult the relevant modules documentation .","title":"User Environment"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/","text":"UCL Information Services serves as a local Registration Authority for the authentication of applications for e-Science Certificates. A valid e-Science certificate is required to gain access to the resources of the National e-Infrastructure Service (NES) (amongst others). Brief information to help you in applying for an e-Science Certificate is provided below. More detailed information can be found on the NGS Support website . Scope of the UCL Registration Authority \u00a7 In general, the UCL Registration Authority (RA) can only approve personal and server certificate requests for members of UCL and those associated with projects based at UCL. However we have approved personal certificate requests for members of other London institutions without local RAs on request. Before you Apply for a Certificate \u00a7 The recommended method for applying for a certificate is to use the Certificate Wizard. So: Download the CertWizard java application or use WebStart from the Certificate Wizard page on the NES website. Install the application on your computer. Run the Certificate Wizard application. Applying for a Certificate \u00a7 You will be asked for a number of items when completing your request including: Certificate Wizard keystore password. Your certificates are stored in keystore which is password protected. The password (or passphrase) you choose should be at least 8 characters long and should conform to common secure password guidelines (e.g. include upper and lower-case letters, numbers, and punctuation symbols). Note that this password is the only thing that protects the private key part of your certificate(s) from being compromised, and thus rendered invalid. Keep this password to yourself, and don't forget it. If you forget this password, or if it is compromised, your certificate(s) will have to be revoked, and you will need to re-apply for them (at considerable inconvenience to both you and the CA/RA). Your given name and family name. You must enter your real name here. Names of roles will be rejected by the CA, for example you cannot use Biochem GRID. Your first name must be a word not just your your initial. Registration Authority. Use UCL EISD. This is the only valid Registration Authority (RA) for UCL. Your e-mail address. Make sure you get this right as it will be used by the e-Science CA to contact you when your certificate is ready. Your PIN. ( Not your bank PIN.) This should be, at minimum, 10 characters long. You will be asked for your PIN by your Registration Authority so it needs to be something you can remember or show. It should not be any of your normal passwords. Using one of these as your PIN (and thus revealing it to both your RA and the e-Science CA) will compromise its use as a password. Extra items for Server Certificates \u00a7 There are two extra items for certificates for servers: Host Name. The fully qualified DNS name (not numeric IP address) of your server. Host Admin Email. To apply for a server certificate you must have a user certificate for yourself and be responsible for the server. After Your Request has been Submitted \u00a7 After you have submitted your request, it has to be authenticated by your Registration Authority (RA) before the certificate is issued by the UK e-Science Certification Authority (CA). For authentication the UK e-Science CA requires that you present yourself in person to your RA with an appropriate form of photo-ID and your PIN. You will be asked to explain why you need a UK e-Science Certificate. The RA for UCL is based in Informations Services Division (ISD). To arrange an appointment please email grid-ra AT ucl.ac.uk in the first instance. Valid forms of Photo-ID are any of the following: Valid UCL ID card (It has to be a complete ID card with photo; authorisation for an ID card is not sufficient.) Current passport UK style photocard driving licence We are required to make and log a photocopy of your photo-ID. If you have none of the above forms of photo-ID available, contact us for advice by e-mail at grid-ra AT ucl.ac.uk. Please don't just turn up with an alternative as we may not be able to accept it. Extra Requirements for Students \u00a7 In addition to the above, students should provide a letter (on department paper) from their project supervisor explaining why they need a certificate. Extra Requirements for Servers \u00a7 In addition to the above, you need to provide a letter from your department explaining that you are responsible for this server. The letter should be on departmental stationary and be signed by your head of department. Obtaining Your Certificate \u00a7 After your request has been authenticated by your Registration Authority, it is forwarded to the UK e-Science Certification Authority for final creation (this stage is called signing the certificate). Signing is normally done on the same or next working day. When your certificate is ready the CA will e-mail you using the e-mail address that you provided with details of how to download your certificate. If you used the recommend method to request it, then you can download it into the Certificate Wizard application using the Refresh button. You should now make a backup of your certificate using the Export button in the Certificate Wizard application.","title":"UCL UK e-Science Certificates"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#scope-of-the-ucl-registration-authority","text":"In general, the UCL Registration Authority (RA) can only approve personal and server certificate requests for members of UCL and those associated with projects based at UCL. However we have approved personal certificate requests for members of other London institutions without local RAs on request.","title":"Scope of the UCL Registration Authority"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#before-you-apply-for-a-certificate","text":"The recommended method for applying for a certificate is to use the Certificate Wizard. So: Download the CertWizard java application or use WebStart from the Certificate Wizard page on the NES website. Install the application on your computer. Run the Certificate Wizard application.","title":"Before you Apply for a Certificate"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#applying-for-a-certificate","text":"You will be asked for a number of items when completing your request including: Certificate Wizard keystore password. Your certificates are stored in keystore which is password protected. The password (or passphrase) you choose should be at least 8 characters long and should conform to common secure password guidelines (e.g. include upper and lower-case letters, numbers, and punctuation symbols). Note that this password is the only thing that protects the private key part of your certificate(s) from being compromised, and thus rendered invalid. Keep this password to yourself, and don't forget it. If you forget this password, or if it is compromised, your certificate(s) will have to be revoked, and you will need to re-apply for them (at considerable inconvenience to both you and the CA/RA). Your given name and family name. You must enter your real name here. Names of roles will be rejected by the CA, for example you cannot use Biochem GRID. Your first name must be a word not just your your initial. Registration Authority. Use UCL EISD. This is the only valid Registration Authority (RA) for UCL. Your e-mail address. Make sure you get this right as it will be used by the e-Science CA to contact you when your certificate is ready. Your PIN. ( Not your bank PIN.) This should be, at minimum, 10 characters long. You will be asked for your PIN by your Registration Authority so it needs to be something you can remember or show. It should not be any of your normal passwords. Using one of these as your PIN (and thus revealing it to both your RA and the e-Science CA) will compromise its use as a password.","title":"Applying for a Certificate"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#extra-items-for-server-certificates","text":"There are two extra items for certificates for servers: Host Name. The fully qualified DNS name (not numeric IP address) of your server. Host Admin Email. To apply for a server certificate you must have a user certificate for yourself and be responsible for the server.","title":"Extra items for Server Certificates"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#after-your-request-has-been-submitted","text":"After you have submitted your request, it has to be authenticated by your Registration Authority (RA) before the certificate is issued by the UK e-Science Certification Authority (CA). For authentication the UK e-Science CA requires that you present yourself in person to your RA with an appropriate form of photo-ID and your PIN. You will be asked to explain why you need a UK e-Science Certificate. The RA for UCL is based in Informations Services Division (ISD). To arrange an appointment please email grid-ra AT ucl.ac.uk in the first instance. Valid forms of Photo-ID are any of the following: Valid UCL ID card (It has to be a complete ID card with photo; authorisation for an ID card is not sufficient.) Current passport UK style photocard driving licence We are required to make and log a photocopy of your photo-ID. If you have none of the above forms of photo-ID available, contact us for advice by e-mail at grid-ra AT ucl.ac.uk. Please don't just turn up with an alternative as we may not be able to accept it.","title":"After Your Request has been Submitted"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#extra-requirements-for-students","text":"In addition to the above, students should provide a letter (on department paper) from their project supervisor explaining why they need a certificate.","title":"Extra Requirements for Students"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#extra-requirements-for-servers","text":"In addition to the above, you need to provide a letter from your department explaining that you are responsible for this server. The letter should be on departmental stationary and be signed by your head of department.","title":"Extra Requirements for Servers"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#obtaining-your-certificate","text":"After your request has been authenticated by your Registration Authority, it is forwarded to the UK e-Science Certification Authority for final creation (this stage is called signing the certificate). Signing is normally done on the same or next working day. When your certificate is ready the CA will e-mail you using the e-mail address that you provided with details of how to download your certificate. If you used the recommend method to request it, then you can download it into the Certificate Wizard application using the Refresh button. You should now make a backup of your certificate using the Export button in the Certificate Wizard application.","title":"Obtaining Your Certificate"},{"location":"Paid-For_Resources/How_to_Use/","text":"Using Paid-For Resources \u00a7 Users with access to paid resources have some extra flags and a tool for monitoring their nodes. Job Script Additions \u00a7 For a job to be eligible to run on your nodes, you will need to specify your project in your jobscript: # Specify project #$ -P <project> This will allow a job to run on your nodes, but it can also be scheduled on general-use nodes if some are available first. This should be the main way you run jobs. If you need to, you can force jobs to run on your nodes only. This is suitable when you have arranged policies on your nodes that are different from the normal policies, as it allows you to override them. # Specify paid flag to force running on your nodes only, with your policies #$ -l paid=1 Check what is running on your nodes \u00a7 We have a script named whatsonmynode , that runs qhost -j for all the nodes belonging to your project, so you can see which nodes you have, what is running on them and from which user. module load userscripts whatsonmynode <project> Backfill \u00a7 In our usual arrangement, paid-for nodes become available for short jobs from other users when you have not been using them. The policy is set by the CRAG: this is the current policy as of November 2015. When you have not run a job on an individual node for 48hrs, then it becomes available for general use jobs of up to 12hrs. When you submit a new job, the queues will not allow any other general use jobs on to your nodes, but the ones currently running will complete. The maximum wait time for your job is hence 12hrs. Other options may be discussed at time of purchase if this is not suitable.","title":"How to Use"},{"location":"Paid-For_Resources/How_to_Use/#using-paid-for-resources","text":"Users with access to paid resources have some extra flags and a tool for monitoring their nodes.","title":"Using Paid-For Resources"},{"location":"Paid-For_Resources/How_to_Use/#job-script-additions","text":"For a job to be eligible to run on your nodes, you will need to specify your project in your jobscript: # Specify project #$ -P <project> This will allow a job to run on your nodes, but it can also be scheduled on general-use nodes if some are available first. This should be the main way you run jobs. If you need to, you can force jobs to run on your nodes only. This is suitable when you have arranged policies on your nodes that are different from the normal policies, as it allows you to override them. # Specify paid flag to force running on your nodes only, with your policies #$ -l paid=1","title":"Job Script Additions"},{"location":"Paid-For_Resources/How_to_Use/#check-what-is-running-on-your-nodes","text":"We have a script named whatsonmynode , that runs qhost -j for all the nodes belonging to your project, so you can see which nodes you have, what is running on them and from which user. module load userscripts whatsonmynode <project>","title":"Check what is running on your nodes"},{"location":"Paid-For_Resources/How_to_Use/#backfill","text":"In our usual arrangement, paid-for nodes become available for short jobs from other users when you have not been using them. The policy is set by the CRAG: this is the current policy as of November 2015. When you have not run a job on an individual node for 48hrs, then it becomes available for general use jobs of up to 12hrs. When you submit a new job, the queues will not allow any other general use jobs on to your nodes, but the ones currently running will complete. The maximum wait time for your job is hence 12hrs. Other options may be discussed at time of purchase if this is not suitable.","title":"Backfill"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/","text":"Purchasing in Myriad \u00a7 Researchers may purchase additional resources to be used as part of the Myriad High Performance Computing cluster if the free service does not meet their needs. These resources can be made available in one of two ways: Nodes purchased by researchers can be converted into a quarterly allocation of \u201cpriority cycles\u201d equivalent to the amount of computation provided by the nodes, but usable across the whole cluster. We calculate how much time exists on your nodes for three months, and every three months you receive an allocation of that much priority time to use however you want on Myriad (GPU/large memory nodes cost more to use, and give you more if you buy them). We recommend this route. The purchaser may request to buy nodes to be reserved for their group or department, restricting usage to the owned nodes. There may be an additional cost implication to this option (price on application). Costs will include backend infrastructure \u2013 racks, switches, cables etc \u2013 needed to integrate the compute nodes into the facility. Both options can be costed into research proposals. FAQs \u00a7 Can I add the cost of purchasing nodes to a grant application? \u00a7 If you are putting together a grant application and think that you may need to ask for the cost of additional computing resources to be covered, please contact us. We will be able to assess your requirements, recommend appropriate hardware, and then provide an estimate and a supporting statement. Can you give me advice on what hardware to buy? \u00a7 Yes, we\u2019d be happy to discuss this with you. Please contact rits@ucl.ac.uk . Please note that this is not the same address used for support requests. What type of nodes can we purchase? \u00a7 We currently have three types of node in Myriad, which are well tested and work reliably. These are: Standard compute nodes: 36 Cascade Lake Xeon cores, 192GB RAM GPU nodes: 36 Cascade Lake Xeon cores, 192GB RAM, 2x nVidia Tesla V100 High memory: 36 Cascade Lake Xeon cores, 1.5TB RAM If you require an alternative/custom specification, we can\u2019t guarantee that we will be able to accommodate this on the cluster, but we\u2019re happy to look into it. I want to apply for more than \u00a350k worth of equipment, will we have to go through a tender process? \u00a7 No. We have a framework agreement with the vendor which covers all hardware purchases. I know what hardware I need. Can you send me a quote? \u00a7 Even if you know what hardware you need, before we can send you a quote, we will need to agree on a detailed specification. Please email rits@ucl.ac.uk with the following information: Budget holder: name and contact details Type and number of nodes you\u2019d like to purchase We will then either send you a specification to approve, or ask to meet to discuss your requirements further. Once this is agreed, we aim to get back to you with a quote within two weeks. How do I manage who has permission to use our nodes/priority queue? \u00a7 When you purchase nodes or priority cycles, we will ask you for a list of usernames of people who have permission to use the resource \u2014 access is managed using access control lists on Myriad. If your list of users is an entire department, we can automatically generate this list nightly. Resource owners or designated resource administrators can request a change of membership of these groups by submitting a ticket in Remedy Force or emailing [mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk]. What is the difference between paying for priority cycles and purchasing dedicated nodes? \u00a7 Priority cycles is the better option for most people as it provides greater flexibility: priority cycles can be used across many nodes at once, and there is no time limit to using them. Dedicated hardware however would need to be in use 24/7 in order to get the most out of it. Researchers might want dedicated nodes if they have particular requirements which mean they can only run their work on their own nodes; e.g., they have purchased non-standard nodes, or the software they are using requires a static licence tied to a particular node. Will my 3-month priority cycle allocation roll over to the next quarter if I don\u2019t use it? \u00a7 No. I want the flexibility of priority cycles, but my funder requires an invoice for specific hardware. \u00a7 Even if you require an invoice for specific hardware, you can still convert the physical hardware into priority cycles. We will add the nodes you purchased to the general pool and give you the equivalent in priority cycles. What happens to nodes I have purchased once they\u2019ve reached the end of warranty? \u00a7 The hardware is run for the life-time of the service, however if you buy dedicated nodes and they run out of warranty, they will not be replaced.","title":"Purchasing in Myriad"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#purchasing-in-myriad","text":"Researchers may purchase additional resources to be used as part of the Myriad High Performance Computing cluster if the free service does not meet their needs. These resources can be made available in one of two ways: Nodes purchased by researchers can be converted into a quarterly allocation of \u201cpriority cycles\u201d equivalent to the amount of computation provided by the nodes, but usable across the whole cluster. We calculate how much time exists on your nodes for three months, and every three months you receive an allocation of that much priority time to use however you want on Myriad (GPU/large memory nodes cost more to use, and give you more if you buy them). We recommend this route. The purchaser may request to buy nodes to be reserved for their group or department, restricting usage to the owned nodes. There may be an additional cost implication to this option (price on application). Costs will include backend infrastructure \u2013 racks, switches, cables etc \u2013 needed to integrate the compute nodes into the facility. Both options can be costed into research proposals.","title":"Purchasing in Myriad"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#faqs","text":"","title":"FAQs"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#can-i-add-the-cost-of-purchasing-nodes-to-a-grant-application","text":"If you are putting together a grant application and think that you may need to ask for the cost of additional computing resources to be covered, please contact us. We will be able to assess your requirements, recommend appropriate hardware, and then provide an estimate and a supporting statement.","title":"Can I add the cost of purchasing nodes to a grant application?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#can-you-give-me-advice-on-what-hardware-to-buy","text":"Yes, we\u2019d be happy to discuss this with you. Please contact rits@ucl.ac.uk . Please note that this is not the same address used for support requests.","title":"Can you give me advice on what hardware to buy?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#what-type-of-nodes-can-we-purchase","text":"We currently have three types of node in Myriad, which are well tested and work reliably. These are: Standard compute nodes: 36 Cascade Lake Xeon cores, 192GB RAM GPU nodes: 36 Cascade Lake Xeon cores, 192GB RAM, 2x nVidia Tesla V100 High memory: 36 Cascade Lake Xeon cores, 1.5TB RAM If you require an alternative/custom specification, we can\u2019t guarantee that we will be able to accommodate this on the cluster, but we\u2019re happy to look into it.","title":"What type of nodes can we purchase?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#i-want-to-apply-for-more-than-50k-worth-of-equipment-will-we-have-to-go-through-a-tender-process","text":"No. We have a framework agreement with the vendor which covers all hardware purchases.","title":"I want to apply for more than \u00a350k worth of equipment, will we have to go through a tender process?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#i-know-what-hardware-i-need-can-you-send-me-a-quote","text":"Even if you know what hardware you need, before we can send you a quote, we will need to agree on a detailed specification. Please email rits@ucl.ac.uk with the following information: Budget holder: name and contact details Type and number of nodes you\u2019d like to purchase We will then either send you a specification to approve, or ask to meet to discuss your requirements further. Once this is agreed, we aim to get back to you with a quote within two weeks.","title":"I know what hardware I need. Can you send me a quote?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#how-do-i-manage-who-has-permission-to-use-our-nodespriority-queue","text":"When you purchase nodes or priority cycles, we will ask you for a list of usernames of people who have permission to use the resource \u2014 access is managed using access control lists on Myriad. If your list of users is an entire department, we can automatically generate this list nightly. Resource owners or designated resource administrators can request a change of membership of these groups by submitting a ticket in Remedy Force or emailing [mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk].","title":"How do I manage who has permission to use our nodes/priority queue?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#what-is-the-difference-between-paying-for-priority-cycles-and-purchasing-dedicated-nodes","text":"Priority cycles is the better option for most people as it provides greater flexibility: priority cycles can be used across many nodes at once, and there is no time limit to using them. Dedicated hardware however would need to be in use 24/7 in order to get the most out of it. Researchers might want dedicated nodes if they have particular requirements which mean they can only run their work on their own nodes; e.g., they have purchased non-standard nodes, or the software they are using requires a static licence tied to a particular node.","title":"What is the difference between paying for priority cycles and purchasing dedicated nodes?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#will-my-3-month-priority-cycle-allocation-roll-over-to-the-next-quarter-if-i-dont-use-it","text":"No.","title":"Will my 3-month priority cycle allocation roll over to the next quarter if I don\u2019t use it?"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#i-want-the-flexibility-of-priority-cycles-but-my-funder-requires-an-invoice-for-specific-hardware","text":"Even if you require an invoice for specific hardware, you can still convert the physical hardware into priority cycles. We will add the nodes you purchased to the general pool and give you the equivalent in priority cycles.","title":"I want the flexibility of priority cycles, but my funder requires an invoice for specific hardware."},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#what-happens-to-nodes-i-have-purchased-once-theyve-reached-the-end-of-warranty","text":"The hardware is run for the life-time of the service, however if you buy dedicated nodes and they run out of warranty, they will not be replaced.","title":"What happens to nodes I have purchased once they\u2019ve reached the end of warranty?"},{"location":"Software_Guides/ANSYS/","text":"ANSYS \u00a7 ANSYS/CFX and ANSYS/Fluent are commercial fluid dynamics packages. ANSYS/CFX and ANSYS/Fluent version 17.2, 18.0, 19.1 and later are available. The ANSYS Electromagnetics Suite (AnsysEM) is available from version 19.1 onwards. ANSYS Mechanical and Autodyn are available from 2019.r3 onwards. Before these applications can be run, the user needs to go though a number of set up steps. These are detailed here. To see the versions available, type module avail ansys The desired ANSYS module needs to be loaded by issuing a command like: module load ansys/19.1 The first time this is done, users should run the shell script setup_cfx.sh to configure licensing and HP-MPI options on a login node: setup_cfx.sh Running this script is required regardless of which ANSYS application you are running.. The ANSYS applications are intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer (up to two hours) on the User Test Nodes. Interactive work can be done using the ANSYS interactive tools provided you have X-windows functionality enabled though your ssh connection. See our User Guide for more information about enabling X-windows functionality and the User Test nodes. UCL's campus-wide license covers 125 instances with 512 HPC licenses (for parallel jobs) available for running CFX, Fluent and AnsysEM jobs and in order to make sure that jobs only run if there are licenses available, it is necessary for users to request ANSYS licenses with their jobs, by adding -ac app=cfx to their job submission. ANSYS/CFX \u00a7 CFX handles its own parallelisation, so a number of complex options need to be passed in job scripts to make it run correctly. Example single node multi-threaded ANSYS/CFX jobscript \u00a7 Here is an example runscript for running cfx5solve multi-threaded on a given .def file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash -l # ANSYS 19.1: Batch script to run cfx5solve on the StaticMixer.def example # file, single node multi-threaded (12 threads), # Force bash as the executing shell. #$ -S /bin/bash # Request 15 munutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:15:0 # Request 1 gigabyte of RAM per core. #$ -l mem=1G # Set the name of the job. #$ -N StaticMixer_thread_12 # Select 12 threads. #$ -pe smp 12 # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-19.1 #$ -wd /home/<your_UCL_id>/Scratch/cfxtests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy the .def file into the working (current) directory cp /home/<your userid>/cfx_examples/StaticMixer.def . # Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"15 [min]\" -def StaticMixer.def -par-local -partition $OMP_NUM_THREADS You will need to change the -wd /home/<your_UCL_id>/Scratch/cfxtests-19.1 location and may need to change the memory, wallclock time, number of threads and job name directives as well. Replace the .def file with your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-thr.sh Output files will be saved in the job's working directory. Example multi-node MPI ANSYS/CFX jobscript \u00a7 Here is an example runscript for running cfx5solve on more than one node (using MPI) on a given .def file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/bin/bash -l # ANSYS 19.1: Batch script to run cfx5solve on the StaticMixer.def example # file, distributed parallel (36 cores). # Request one hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:00:0 # Request 2 gigabyte of RAM per core. #$ -l mem=2G # Set the name of the job. #$ -N StaticMixer_P_dist_36 # Select the MPI parallel environment and 36 processors. #$ -pe mpi 36 # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-19.1 #$ -wd /home/<your_UCL_userid>/Scratch/cfxtests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy the .def file into the working (current) directory cp /home/<your_UCL_userid>/cfx_examples/StaticMixer.def . # SGE puts the machine file in $TMPDIR/machines. Use this to generate the # string CFX_NODES needed by cfx5solve export CFX_NODES=$(cfxnodes $TMPDIR/machines) # Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"60 [min]\" -def StaticMixer.def -par-dist $CFX_NODES Please copy if you wish and edit it to suit your jobs. You will need to change the -wd /home/<your_UCL_userid>/Scratch/cfxtests-19.1 location and may need to change the memory, wallclock time, number of MPI processors and job name directives as well. Replace the .def file with your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-par.sh Output files will be saved in the job's working directory. Running CFX with MPI on Myriad \u00a7 The default supplied Intel MPI doesn't work on Myriad. Instead you need to use the supplied IBM MPI. This can be done by adding: -start-method \"IBM MPI Distributed Parallel\" to the cfx5solve command. Troubleshooting CFX \u00a7 If you are getting licensing errors when trying to run a parallel job and you have an older version's ~/.ansys/v161/licensing/license.preferences.xml file, delete it. It does not work with the newer license server. (This applies to all older versions, not just v161 ). ANSYS/Fluent \u00a7 Fluent handles its own parallelisation, so a number of complex options need to be passed in job scripts to make it run correctly. The .in file mentioned in the scripts below is a Fluent journal file , giving it the list of commands to carry out in batch mode. Example serial ANSYS/Fluent jobscript \u00a7 Here is an example jobscript for running Fluent in serial mode (1 core). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/bin/bash -l # ANSYS 19.1: Batch script to run ANSYS/fluent in serial mode # (1 core). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabytes of RAM. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_ser1 # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-19.1 #$ -wd /home/<your_UCL_userid>/Scratch/fluent-tests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 2D single precision (-g no GUI). For double precision use # 2ddp. For 3D use 3d or 3ddp. fluent 2d -g < test-1.in Please copy if you wish and edit it to suit your jobs. You will need to change the -wd /home/<your_UCL_id>/Scratch/fluent-tests-19.1 location and may need to change the memory, wallclock time, and job name as well. Replace the .cas and .in files with your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-ser.sh Output files will be saved in the job's working directory. Example parallel (MPI) ANSYS/Fluent jobscript \u00a7 Here is an example runscript for running Fluent in parallel potentially across more than one node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/bin/bash -l # ANSYS 19.1: Batch script to run ANSYS/fluent distributed parallel # (32 cores). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabytes of RAM per core. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_par32 # Select the MPI parallel environment and 32 processors. #$ -pe mpi 32 # Request 25 Gb TMPDIR space (if on a cluster that supports this) #$ -l tmpfs=25G # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-19.1 #$ -wd /home/<your_UCL_userid>/Scratch/fluent-tests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 3D single precision (-g no GUI). For double precision use # 3ddp. For 2D use 2d or 2ddp. # Do not change -t, -mpi, -pinfiniband and -cnf options. fluent 3ddp -t$NSLOTS -mpi=ibmmpi -cnf=$TMPDIR/machines -g < test-1.in Please copy if you wish and edit it to suit your jobs. You will need to change the -wd /home/<your_UCL_id>/Scratch/fluent-tests-19.1 location and may need to change the memory, wallclock time, number of MPI processors and job name as well. Replace the .cas and .in files with your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-par-32.sh Output files will be saved in the job's working directory. Troubleshooting Fluent \u00a7 If you are getting licensing errors when trying to run a parallel job and you have an older version's ~/.ansys/v161/licensing/license.preferences.xml file, delete it. It does not work with the newer license server. (This applies to all older versions, not just v161 ). Fluent 14 required -mpi=pcmpi -pinfiniband in the parallel options: if you have older scripts remember to remove this. ANSYS Mechanical \u00a7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #!/bin/bash -l # ANSYS 2019.R3: Batch script to run ANSYS Mechanical solver # file, distributed parallel (32 cores). # Using ANSYS 2019 licence manager running on UCL central licence server. # Force bash as the executing shell. #$ -S /bin/bash # Request one hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:00:0 # Request 2 gigabyte of RAM per core. (Must be an integer) #$ -l mem=2G # Set the name of the job. #$ -N Mech_P_dist_32 # Select the MPI parallel environment and 32 processors. #$ -pe mpi 32 # Request ANSYS licences $ inserted so currently active.Job will queue until # suficient licences are available. #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory ANSYS_Mech #$ -wd /home/<your_UCL_username>/Scratch/ANSYS_Mech # Load the ANSYS module to set up your environment module load ansys/2019.r3 # Copy the .in file into the working (current) directory cp ~/ANSYS/steady_state_input_file.dat . # SGE puts the machine file in $TMPDIR/machines. Use this to generate the # string CFX_NODES needed by ansys195 which requires : as the separator. export CFX_NODES=$(cfxnodes $TMPDIR/machines | sed -e 's/\\*/:/g' -e 's/,/:/g') echo $CFX_NODES # Run ansys mechanical - Note: use ansys195 instead of ansys and -p argument # needed to switch to a valid UCL license. ansys195 -dis -b -p aa_r -machines $CFX_NODES < steady_state_input_file.dat If you have access to Grace, this test input and jobscript are available at /home/ccaabaa/Software/ANSYS/steady_state_input_file.dat and /home/ccaabaa/Software/ANSYS/ansys-mech-ex.sh","title":"ANSYS"},{"location":"Software_Guides/ANSYS/#ansys","text":"ANSYS/CFX and ANSYS/Fluent are commercial fluid dynamics packages. ANSYS/CFX and ANSYS/Fluent version 17.2, 18.0, 19.1 and later are available. The ANSYS Electromagnetics Suite (AnsysEM) is available from version 19.1 onwards. ANSYS Mechanical and Autodyn are available from 2019.r3 onwards. Before these applications can be run, the user needs to go though a number of set up steps. These are detailed here. To see the versions available, type module avail ansys The desired ANSYS module needs to be loaded by issuing a command like: module load ansys/19.1 The first time this is done, users should run the shell script setup_cfx.sh to configure licensing and HP-MPI options on a login node: setup_cfx.sh Running this script is required regardless of which ANSYS application you are running.. The ANSYS applications are intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer (up to two hours) on the User Test Nodes. Interactive work can be done using the ANSYS interactive tools provided you have X-windows functionality enabled though your ssh connection. See our User Guide for more information about enabling X-windows functionality and the User Test nodes. UCL's campus-wide license covers 125 instances with 512 HPC licenses (for parallel jobs) available for running CFX, Fluent and AnsysEM jobs and in order to make sure that jobs only run if there are licenses available, it is necessary for users to request ANSYS licenses with their jobs, by adding -ac app=cfx to their job submission.","title":"ANSYS"},{"location":"Software_Guides/ANSYS/#ansyscfx","text":"CFX handles its own parallelisation, so a number of complex options need to be passed in job scripts to make it run correctly.","title":"ANSYS/CFX"},{"location":"Software_Guides/ANSYS/#example-single-node-multi-threaded-ansyscfx-jobscript","text":"Here is an example runscript for running cfx5solve multi-threaded on a given .def file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash -l # ANSYS 19.1: Batch script to run cfx5solve on the StaticMixer.def example # file, single node multi-threaded (12 threads), # Force bash as the executing shell. #$ -S /bin/bash # Request 15 munutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:15:0 # Request 1 gigabyte of RAM per core. #$ -l mem=1G # Set the name of the job. #$ -N StaticMixer_thread_12 # Select 12 threads. #$ -pe smp 12 # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-19.1 #$ -wd /home/<your_UCL_id>/Scratch/cfxtests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy the .def file into the working (current) directory cp /home/<your userid>/cfx_examples/StaticMixer.def . # Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"15 [min]\" -def StaticMixer.def -par-local -partition $OMP_NUM_THREADS You will need to change the -wd /home/<your_UCL_id>/Scratch/cfxtests-19.1 location and may need to change the memory, wallclock time, number of threads and job name directives as well. Replace the .def file with your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-thr.sh Output files will be saved in the job's working directory.","title":"Example single node multi-threaded ANSYS/CFX jobscript"},{"location":"Software_Guides/ANSYS/#example-multi-node-mpi-ansyscfx-jobscript","text":"Here is an example runscript for running cfx5solve on more than one node (using MPI) on a given .def file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/bin/bash -l # ANSYS 19.1: Batch script to run cfx5solve on the StaticMixer.def example # file, distributed parallel (36 cores). # Request one hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:00:0 # Request 2 gigabyte of RAM per core. #$ -l mem=2G # Set the name of the job. #$ -N StaticMixer_P_dist_36 # Select the MPI parallel environment and 36 processors. #$ -pe mpi 36 # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-19.1 #$ -wd /home/<your_UCL_userid>/Scratch/cfxtests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy the .def file into the working (current) directory cp /home/<your_UCL_userid>/cfx_examples/StaticMixer.def . # SGE puts the machine file in $TMPDIR/machines. Use this to generate the # string CFX_NODES needed by cfx5solve export CFX_NODES=$(cfxnodes $TMPDIR/machines) # Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"60 [min]\" -def StaticMixer.def -par-dist $CFX_NODES Please copy if you wish and edit it to suit your jobs. You will need to change the -wd /home/<your_UCL_userid>/Scratch/cfxtests-19.1 location and may need to change the memory, wallclock time, number of MPI processors and job name directives as well. Replace the .def file with your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-par.sh Output files will be saved in the job's working directory.","title":"Example multi-node MPI ANSYS/CFX jobscript"},{"location":"Software_Guides/ANSYS/#running-cfx-with-mpi-on-myriad","text":"The default supplied Intel MPI doesn't work on Myriad. Instead you need to use the supplied IBM MPI. This can be done by adding: -start-method \"IBM MPI Distributed Parallel\" to the cfx5solve command.","title":"Running CFX with MPI on Myriad"},{"location":"Software_Guides/ANSYS/#troubleshooting-cfx","text":"If you are getting licensing errors when trying to run a parallel job and you have an older version's ~/.ansys/v161/licensing/license.preferences.xml file, delete it. It does not work with the newer license server. (This applies to all older versions, not just v161 ).","title":"Troubleshooting CFX"},{"location":"Software_Guides/ANSYS/#ansysfluent","text":"Fluent handles its own parallelisation, so a number of complex options need to be passed in job scripts to make it run correctly. The .in file mentioned in the scripts below is a Fluent journal file , giving it the list of commands to carry out in batch mode.","title":"ANSYS/Fluent"},{"location":"Software_Guides/ANSYS/#example-serial-ansysfluent-jobscript","text":"Here is an example jobscript for running Fluent in serial mode (1 core). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/bin/bash -l # ANSYS 19.1: Batch script to run ANSYS/fluent in serial mode # (1 core). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabytes of RAM. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_ser1 # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-19.1 #$ -wd /home/<your_UCL_userid>/Scratch/fluent-tests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 2D single precision (-g no GUI). For double precision use # 2ddp. For 3D use 3d or 3ddp. fluent 2d -g < test-1.in Please copy if you wish and edit it to suit your jobs. You will need to change the -wd /home/<your_UCL_id>/Scratch/fluent-tests-19.1 location and may need to change the memory, wallclock time, and job name as well. Replace the .cas and .in files with your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-ser.sh Output files will be saved in the job's working directory.","title":"Example serial ANSYS/Fluent jobscript"},{"location":"Software_Guides/ANSYS/#example-parallel-mpi-ansysfluent-jobscript","text":"Here is an example runscript for running Fluent in parallel potentially across more than one node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/bin/bash -l # ANSYS 19.1: Batch script to run ANSYS/fluent distributed parallel # (32 cores). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabytes of RAM per core. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_par32 # Select the MPI parallel environment and 32 processors. #$ -pe mpi 32 # Request 25 Gb TMPDIR space (if on a cluster that supports this) #$ -l tmpfs=25G # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-19.1 #$ -wd /home/<your_UCL_userid>/Scratch/fluent-tests-19.1 # Load the ANSYS module to set up your environment module load ansys/19.1 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 3D single precision (-g no GUI). For double precision use # 3ddp. For 2D use 2d or 2ddp. # Do not change -t, -mpi, -pinfiniband and -cnf options. fluent 3ddp -t$NSLOTS -mpi=ibmmpi -cnf=$TMPDIR/machines -g < test-1.in Please copy if you wish and edit it to suit your jobs. You will need to change the -wd /home/<your_UCL_id>/Scratch/fluent-tests-19.1 location and may need to change the memory, wallclock time, number of MPI processors and job name as well. Replace the .cas and .in files with your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-par-32.sh Output files will be saved in the job's working directory.","title":"Example parallel (MPI) ANSYS/Fluent jobscript"},{"location":"Software_Guides/ANSYS/#troubleshooting-fluent","text":"If you are getting licensing errors when trying to run a parallel job and you have an older version's ~/.ansys/v161/licensing/license.preferences.xml file, delete it. It does not work with the newer license server. (This applies to all older versions, not just v161 ). Fluent 14 required -mpi=pcmpi -pinfiniband in the parallel options: if you have older scripts remember to remove this.","title":"Troubleshooting Fluent"},{"location":"Software_Guides/ANSYS/#ansys-mechanical","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #!/bin/bash -l # ANSYS 2019.R3: Batch script to run ANSYS Mechanical solver # file, distributed parallel (32 cores). # Using ANSYS 2019 licence manager running on UCL central licence server. # Force bash as the executing shell. #$ -S /bin/bash # Request one hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:00:0 # Request 2 gigabyte of RAM per core. (Must be an integer) #$ -l mem=2G # Set the name of the job. #$ -N Mech_P_dist_32 # Select the MPI parallel environment and 32 processors. #$ -pe mpi 32 # Request ANSYS licences $ inserted so currently active.Job will queue until # suficient licences are available. #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory ANSYS_Mech #$ -wd /home/<your_UCL_username>/Scratch/ANSYS_Mech # Load the ANSYS module to set up your environment module load ansys/2019.r3 # Copy the .in file into the working (current) directory cp ~/ANSYS/steady_state_input_file.dat . # SGE puts the machine file in $TMPDIR/machines. Use this to generate the # string CFX_NODES needed by ansys195 which requires : as the separator. export CFX_NODES=$(cfxnodes $TMPDIR/machines | sed -e 's/\\*/:/g' -e 's/,/:/g') echo $CFX_NODES # Run ansys mechanical - Note: use ansys195 instead of ansys and -p argument # needed to switch to a valid UCL license. ansys195 -dis -b -p aa_r -machines $CFX_NODES < steady_state_input_file.dat If you have access to Grace, this test input and jobscript are available at /home/ccaabaa/Software/ANSYS/steady_state_input_file.dat and /home/ccaabaa/Software/ANSYS/ansys-mech-ex.sh","title":"ANSYS Mechanical"},{"location":"Software_Guides/Installing_Software/","text":"If you want to request that software be installed centrally, you can email us at rc-support@ucl.ac.uk or open a GitHub issue in our buildscripts repository . That is also where you can see all the software requests we are currently working on, and the progress on them. If something has already been requested, feel free to add a comment saying you also wish it to be installed. Our buildscripts are in the code section of that repository, so you can see how we built and installed all our central software stack. You can install software yourself in your space on the cluster. Below are some tips for installing packages for languages such as Python or Perl as well as compiling software. No sudo! \u00a7 You cannot install anything using sudo (and neither can we!). If the instructions tell you to do that, read further to see if they also have instructions for installing in user space, or for doing an install from source if they are RPMs. Alternatively, just leave off the sudo from the command they tell you to run and look for an alternative way to give it an install location if it tries to install somewhere that isn't in your space (examples for some common build systems are below). Python \u00a7 There are python2/recommended and python3/recommended module bundles you will see if you type module avail python . These use a virtualenv, have a lot of Python packages installed already, like numpy and scipy (see the Python package list ) and have pip set up for you. Load the GNU compiler \u00a7 Our Python installs were built with GCC. You can run them without problems with the default Intel compilers loaded because it also depends on the gcc-libs/4.9.2 module. However, when you are installing your own Python packages you should make sure you have the GNU compiler module loaded. This is to avoid the situation where you build your package with the Intel compiler and then try to run it with our GNU-based Python. If it compiled any C code, it will be unable to find Intel-specific instructions and give you errors. Change your compiler module: module unload compilers module load compilers/gnu/4.9.2 If you get an error like this when trying to run something, you built a package with the Intel compiler. undefined symbol: __intel_sse2_strrchr Install your own packages in the same virtualenv \u00a7 This will use our central virtualenv and the packages we have already installed. # for Python 2 pip install --user <python2pkg> # for Python 3 pip3 install --user <python3pkg> These will install into .python2local or .python3local in your home directory. If your own installed Python packages get into a mess, you can delete (or rename) the whole .python3local and start again. Using your own virtualenv \u00a7 If you need different packages that are not compatible with the centrally installed versions (eg. what you are trying to install depends on a different version of something we have already installed) then you can create a new virtualenv and only packages you are installing yourself will be in it. # create the new virtualenv, with any name you want virtualenv <DIR> # activate it source <DIR>/bin/activate You only need to create it the first time. Your bash prompt will change to show you that a different virtualenv is active. Installing via setup.py \u00a7 If you need to install by downloading a package and using setup.py , you can use the --user flag and as long as one of our python module bundles are loaded, it will install into the same .python2local or .python3local as pip does and your packages will be found automatically. python setup.py install --user If you want to install to a different directory in your space to keep this package separate, you can use --prefix instead. You'll need to add that location to your $PYTHONPATH and $PATH as well so it can be found. Some install methods won't create the prefix directory you requested for you automatically, so you would need to create it yourself first. This type of install makes it easier for you to only have this package in your paths when you want to use it, which is helpful if it conflicts with something else. # add location to PYTHONPATH so Python can find it export PYTHONPATH=/home/username/your/path/lib/python3.7/site-packages:$PYTHONPATH # if necessary, create lib/pythonx.x/site-packages in your desired install location mkdir -p /home/username/your/path/lib/python3.7/site-packages # do the install python setup.py install --prefix=/home/username/your/path It will tend to tell you at install time if you need to change or create the $PYTHONPATH directory. To use this package, you'll need to add it to your paths in your jobscript or .bashrc . Check that the PATH is where your Python executables were installed. export PYTHONPATH=/home/username/your/path/lib/python3.7/site-packages:$PYTHONPATH export PATH=/home/username/your/path/bin:$PATH It is very important that you keep the :$PYTHONPATH or :$PATH at the end of these - you are putting your location at the front of the existing contents of the path. If you leave them out, then only your package location will be found and nothing else. Troubleshooting: remove your pip cache \u00a7 If you built something and it went wrong, and are trying to reinstall it with pip and keep getting errors that you think you should have fixed, you may still be using a previous cached version. The cache is in .cache/pip in your home directory, and you can delete it. You can prevent caching entirely by installing using pip3 install --user --no-cache-dir <python3pkg> Troubleshooting: Python script executable paths \u00a7 If you have an executable Python script (eg. something you run using pyutility and not python pyutility.py ) that begins like this: 1 #!/usr/bin/python2.6 and fails because that Python doesn't exist in that location or isn't the one that has the additional packages installed, then you should change it so it uses the first Python found in your environment instead, which will be the one from the Python module you've loaded. 1 #!/usr/bin/env python","title":"Installing Software"},{"location":"Software_Guides/Installing_Software/#no-sudo","text":"You cannot install anything using sudo (and neither can we!). If the instructions tell you to do that, read further to see if they also have instructions for installing in user space, or for doing an install from source if they are RPMs. Alternatively, just leave off the sudo from the command they tell you to run and look for an alternative way to give it an install location if it tries to install somewhere that isn't in your space (examples for some common build systems are below).","title":"No sudo!"},{"location":"Software_Guides/Installing_Software/#python","text":"There are python2/recommended and python3/recommended module bundles you will see if you type module avail python . These use a virtualenv, have a lot of Python packages installed already, like numpy and scipy (see the Python package list ) and have pip set up for you.","title":"Python"},{"location":"Software_Guides/Installing_Software/#load-the-gnu-compiler","text":"Our Python installs were built with GCC. You can run them without problems with the default Intel compilers loaded because it also depends on the gcc-libs/4.9.2 module. However, when you are installing your own Python packages you should make sure you have the GNU compiler module loaded. This is to avoid the situation where you build your package with the Intel compiler and then try to run it with our GNU-based Python. If it compiled any C code, it will be unable to find Intel-specific instructions and give you errors. Change your compiler module: module unload compilers module load compilers/gnu/4.9.2 If you get an error like this when trying to run something, you built a package with the Intel compiler. undefined symbol: __intel_sse2_strrchr","title":"Load the GNU compiler"},{"location":"Software_Guides/Installing_Software/#install-your-own-packages-in-the-same-virtualenv","text":"This will use our central virtualenv and the packages we have already installed. # for Python 2 pip install --user <python2pkg> # for Python 3 pip3 install --user <python3pkg> These will install into .python2local or .python3local in your home directory. If your own installed Python packages get into a mess, you can delete (or rename) the whole .python3local and start again.","title":"Install your own packages in the same virtualenv"},{"location":"Software_Guides/Installing_Software/#using-your-own-virtualenv","text":"If you need different packages that are not compatible with the centrally installed versions (eg. what you are trying to install depends on a different version of something we have already installed) then you can create a new virtualenv and only packages you are installing yourself will be in it. # create the new virtualenv, with any name you want virtualenv <DIR> # activate it source <DIR>/bin/activate You only need to create it the first time. Your bash prompt will change to show you that a different virtualenv is active.","title":"Using your own virtualenv"},{"location":"Software_Guides/Installing_Software/#installing-via-setuppy","text":"If you need to install by downloading a package and using setup.py , you can use the --user flag and as long as one of our python module bundles are loaded, it will install into the same .python2local or .python3local as pip does and your packages will be found automatically. python setup.py install --user If you want to install to a different directory in your space to keep this package separate, you can use --prefix instead. You'll need to add that location to your $PYTHONPATH and $PATH as well so it can be found. Some install methods won't create the prefix directory you requested for you automatically, so you would need to create it yourself first. This type of install makes it easier for you to only have this package in your paths when you want to use it, which is helpful if it conflicts with something else. # add location to PYTHONPATH so Python can find it export PYTHONPATH=/home/username/your/path/lib/python3.7/site-packages:$PYTHONPATH # if necessary, create lib/pythonx.x/site-packages in your desired install location mkdir -p /home/username/your/path/lib/python3.7/site-packages # do the install python setup.py install --prefix=/home/username/your/path It will tend to tell you at install time if you need to change or create the $PYTHONPATH directory. To use this package, you'll need to add it to your paths in your jobscript or .bashrc . Check that the PATH is where your Python executables were installed. export PYTHONPATH=/home/username/your/path/lib/python3.7/site-packages:$PYTHONPATH export PATH=/home/username/your/path/bin:$PATH It is very important that you keep the :$PYTHONPATH or :$PATH at the end of these - you are putting your location at the front of the existing contents of the path. If you leave them out, then only your package location will be found and nothing else.","title":"Installing via setup.py"},{"location":"Software_Guides/Installing_Software/#troubleshooting-remove-your-pip-cache","text":"If you built something and it went wrong, and are trying to reinstall it with pip and keep getting errors that you think you should have fixed, you may still be using a previous cached version. The cache is in .cache/pip in your home directory, and you can delete it. You can prevent caching entirely by installing using pip3 install --user --no-cache-dir <python3pkg>","title":"Troubleshooting: remove your pip cache"},{"location":"Software_Guides/Installing_Software/#troubleshooting-python-script-executable-paths","text":"If you have an executable Python script (eg. something you run using pyutility and not python pyutility.py ) that begins like this: 1 #!/usr/bin/python2.6 and fails because that Python doesn't exist in that location or isn't the one that has the additional packages installed, then you should change it so it uses the first Python found in your environment instead, which will be the one from the Python module you've loaded. 1 #!/usr/bin/env python","title":"Troubleshooting: Python script executable paths"},{"location":"Software_Guides/Matlab/","text":"MATLAB is a numerical computing environment and proprietary programming language developed by MathWorks. Our MATLAB installs include all the toolboxes included in UCL's Total Academic Headcount-Campus licence plus the Matlab Distributed Computing Server. We also have the NAG toolbox for Matlab available. You can submit single node multi-threaded MATLAB jobs or single node jobs which use the Parallel Computing Toolbox and Matlab's Distributed Computing Server. Please note that these currently will not work across multiple nodes, so all types of MATLAB jobs can only use one node. You can submit jobs to Myriad and Legion from MATLAB running on your own desktop or laptop. Setup \u00a7 You need to load MATLAB once from a login node before you can submit any jobs. This allows it to set up your ~/.matlab directory as a symbolic link to ~/Scratch/.matlab so that the compute nodes can write to it. # on a login node module load xorg-utils/X11R7.7 module load matlab/full/r2018b/9.5 You can run module avail matlab to see all the available installed versions. Single node multi-threaded batch jobs \u00a7 This is the simplest way to start using MATLAB on the cluster. You will need a .m file containing the MATLAB commands you want to carry out. Here is an example jobscript which you would submit using the qsub command, after you have loaded the MATLAB module once on a login node as mentioned in Setup : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #!/bin/bash -l # Batch script to run a multi-threaded MATLAB job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per core. #$ -l mem=1G # Request 15 gigabytes of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Request a number of threads (which will use that number of cores). # On Myriad you can set the number of threads to a maximum of 36. #$ -pe smp 36 # Request one MATLAB licence - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l matlab=1 # Set the name of the job. #$ -N Matlab_multiThreadedJob1 # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID. # This directory must already exist. #$ -wd /home/<your_UCL_id>/Scratch/Matlab_examples # Your work should be done in $TMPDIR cd $TMPDIR module load xorg-utils/X11R7.7 module load matlab/full/r2018b/9.5 # outputs the modules you have loaded module list # Optional: copy your script and any other files into $TMPDIR. # This is only practical if you have a small number of files. # If you do not copy them in, you must always refer to them using a # full path so they can be found, eg. ~/Scratch/Matlab_examples/analyse.m cp ~/Scratch/Matlab_examples/myMatlabJob.m $TMPDIR cp ~/Scratch/Matlab_examples/initialise.m $TMPDIR cp ~/Scratch/Matlab_examples/analyse.m $TMPDIR # These echoes output what you are about to run echo \"\" echo \"Running matlab -nosplash -nodisplay < myMatlabJob.m ...\" echo \"\" matlab -nosplash -nodesktop -nodisplay < myMatlabJob.m # Or if you did not copy your files: # matlab -nosplash -nodesktop -nodisplay < ~/Scratch/Matlab_examples/myMatlabJob.m # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Matlab_examples/files_from_job_${JOB_ID}.tgz $TMPDIR # Make sure you have given enough time for the copy to complete! Alternative syntax: Instead of using Unix input redirection like this: matlab -nosplash -nodesktop -nodisplay < $Matlab_infile you can also do: matlab -nosplash -nodesktop -nodisplay -r $Matlab_infile Run without the JVM to reduce overhead \u00a7 You can give the -nojvm option to tell MATLAB to run without the Java Virtual Machine. This will speed up startup time, possibly execution time, and remove some memory overhead, but will prevent you using any tools that require Java (eg, tools that use the Java API for I/O and networking like URLREAD, or call Java object methods). Run single-threaded \u00a7 Most of the time, MATLAB will create many threads and use them as it wishes. If you know your job is entirely single-threaded, you can force MATLAB to run with only one thread on one core, which will allow you to have more jobs running at once. To request one core only, set #$ -pe smp 1 in your jobscript. Run MATLAB like this: matlab -nosplash -nodesktop -nodisplay -nojvm -singleCompThread < $Matlab_infile The -singleCompThread forces MATLAB to run single-threaded, and the -nojvm tells it to run without the Java Virtual Machine, as above. Using the MATLAB GUI interactively \u00a7 You can run MATLAB interactively for short amounts of time on the login nodes (please do not do this if your work will be resource-intensive). You can also run it interactively in a qrsh session on the compute nodes. Launching with matlab will give you the full graphical user interface - you will need to have logged in to the cluster with X-forwarding on for this to work. Launching with matlab -nodesktop -nodisplay will give you the MATLAB terminal. Submitting jobs using the Distributed Computing Server (DCS) \u00a7 You must have loaded the MATLAB module once from a login node as described in Setup before you can submit any Matlab DCS jobs. MATLAB DCS jobs must be submitted from an interactive or scripted Matlab session which can be running on the cluster login nodes or on your own machine. MATLAB DCS jobs will only work inside a single node on our clusters. On Myriad this means a maximum of 36 workers can be used per job. Importing the cluster profile \u00a7 You need to import the cluster profile into your MATLAB environment and set it as the default before you can submit DCS jobs. This only needs doing once. The imported profile will be saved in your MATLAB settings directory. Importing the profile can be done either by calling MATLAB functions or via the graphical interface. The profile is stored here (for 2018b): /shared/ucl/apps/Matlab/toolbox_local/Legion/R2018b/LegionGraceMyriadProfile_R2018b.settings Import using MATLAB functions \u00a7 Run these functions from a MATLAB session: profile_Legion = parallel.importProfile ('/shared/ucl/apps/Matlab/toolbox_local/Legion/R2018b/LegionGraceMyriadProfile_R2018b.settings'); parallel.defaultClusterProfile ('LegionGraceMyriadProfileR2018b'); Import from MATLAB GUI \u00a7 To import using the graphical interface instead, do this. From the Home tab select the Parallel menu and click Create and Manage Clusters... . The Cluster Profile Manager window will open. Select Import and in the Import Profiles from file window navigate to the LegionGraceMyriadProfile_R2018b.settings file shown above and select Open. Select the resulting LegionGraceMyriadProfileR2018b profile and click Set as Default . The Cluster Profile Manager window should now look like this: In both cases after you exit MATLAB your cluster profile is saved for future use. Environment variables needed for job submission \u00a7 We have set up three Grid Engine environment variables to assist with job submission from within MATLAB. These are needed to pass in job resource parameters that aren't supported by the internal MATLAB job submission mechanism. SGE_CONTEXT : a comma-separated list of variables treated as if added via the -ac option, eg. exclusive SGE_OPT : a comma-separated list of resources treated as if added via the -l option, eg. h_rt=0:10:0,mem=1G,tmpfs=15G SGE_PROJECT : a project treated as if added via the -P option (not normally needed). -ac exclusive prevents anything else running on the same node as your job, even if you aren't using all the cores. This is no longer a necessary option for MATLAB jobs. There are two ways to set these: 1) Before starting your MATLAB session, using the usual Bash method of exporting environment variables: export SGE_CONTEXT=exclusive export SGE_OPT=h_rt=0:15:0,mem=2G,tmpfs=15G export SGE_PROJECT=<your project ID> 2) Inside your MATLAB session, using MATLAB's setenv function: setenv ('SGE_CONTEXT', 'exclusive'); setenv ('SGE_OPT', 'h_rt=0:15:0,mem=2G,tmpfs=15G'); setenv ('SGE_PROJECT', '<your project ID>'); Example: a simple DCS job \u00a7 This submits a job from inside a MATLAB session running on a login node. You need to start MATLAB from a directory in Scratch - jobs will inherit this as their working directory. This is an example where you have only one MATLAB source file. 1) Change to the directory in Scratch you want the job to run from and set the SGE environment variables. cd ~/Scratch/Matlab_examples export SGE_OPT=h_rt=0:10:0,mem=1G,tmpfs=15G 2) Either start the MATLAB GUI: matlab or start a MATLAB terminal session: matlab -nodesktop -nodisplay 3) Inside MATLAB, create a cluster object using the cluster profile: c = parcluster ('LegionGraceMyriadProfileR2018b'); 4) Use your cluster object to create a job object of the type you need. For this example the job is a parallel job with communication between MATLAB workers of type \"Single Program Multiple Data\": myJob = createCommunicatingJob (c, 'Type', 'SPMD'); 5) Set the number of workers: num_workers = 36; 6) Tell the job the files needed to be made available to each worker - in this example there is only one file: myJob.AttachedFiles = {'colsum.m'}; colsum.m contains the simple magic square example from the MATLAB manual \"Parallel Computing Toolbox User's Guide\". 7) Set the minimum and maximum number of workers for the job (we are asking for an exact number here by setting them the same): myJob.NumWorkersRange = [num_workers, num_workers]; 8) Create a MATLAB task to be executed as part of the job. Here it consists of executing the MATLAB function colsum . The other arguments say that the task returns one parameter and there are no input arguments to the colsum function: task = createTask (myJob, @colsum, 1, {}); 9) Submit the job: submit (myJob); Your job is now submitted to the scheduler and you can see its queue status in qstat as normal. If you were using the MATLAB GUI you can also monitor jobs by selecting Monitor Jobs from the Parallel menu on the Home tab. 10) When the job has completed get the results using: results = fetchOutputs(myJob) You can access the job log from MATLAB using: logMess = getDebugLog (c, myJob); Example: a DCS job with more than one input file \u00a7 This example has several input files. The job type is \"MATLAB Pool\". A \"Pool\" job runs the specified task function with a MATLAB pool available to run the body of parfor loops or spmd blocks and is the default job type. This example was kindly supplied to assist in testing MATLAB by colleagues from CoMPLEX. The first part of creating the job is the same as the above example apart from the longer runtime and larger amount of memory per core: 1) Change into a directory in Scratch, set the SGE variables and launch MATLAB: cd ~/Scratch/Matlab_examples export SGE_OPT=h_rt=1:0:0,mem=2G,tmpfs=15G matlab to launch the GUI or: matlab -nodesktop -nodisplay to start a terminal session. c = parcluster ('LegionGraceMyriadProfileR2018b'); 2) Using our cluster object create a job object of type \"Pool\": myJob2 = createCommunicatingJob (c, 'Type', 'Pool'); 3) Set the number of workers and another variable used by the example: num_workers = 8; simulation_duration_ms = 1000; 4) Tell the job all the input files needed to be made available to each worker as a cell array: myJob2.AttachedFiles = { 'AssemblyFiniteDifferencesMatrix.m' 'AssemblyFiniteDifferencesRightHandSide.m' 'CellModelsComputeIonicCurrents.m' 'CellModelsGetVoltage.m' 'CellModelsInitialise.m' 'CellModelsSetVoltage.m' 'GetStimuliForTimeStep.m' 'SubmitMonodomainJob.m' 'RegressionTest.m' 'RunAndVisualiseMonodomainSimulation.m' 'SolveLinearSystem.m' 'luo_rudy_1991_iionic.m' 'luo_rudy_1991_time_deriv.m'}; 5) Set the minimum and maximum number of workers for the job: myJob2.NumWorkersRange = [num_workers, num_workers]; 6) Create a MATLAB task to be executed as part of the job. For this example it will consist of executing the MATLAB function RunAndVisualiseMonodomainSimulation . The rest of the arguments indicate that the task returns three parameters and there are five input arguments to the function. These are passed as a cell array: task = createTask (myJob2, @RunAndVisualiseMonodomainSimulation, 3, {5000, simulation_duration_ms, 1.4, 1.4, false}); 7) Submit the job: submit (myJob2); As before use fetchOutputs to collect the results. If you closed your session, you can get your results by: c = parcluster ('LegionGraceMyriadProfileR2018b'); # get a cluster object jobs = findJob(c) # get a list of jobs submitted to that cluster job = jobs(3); # pick a particular job results = fetchOutputs(job) You can get other information: diary(job) will give you the job diary, and load(job) will load the workspace. Further reading \u00a7 There is a lot more information about using the MATLAB Distributed Computing Server in the MATLAB manual: Parallel Computing Toolbox User\u2019s Guide . Submitting MATLAB jobs from your workstation/laptop \u00a7 You can submit MATLAB jobs to Myriad and Legion from MATLAB sessions running on your own desktop workstation or laptop systems provided they are running the same version of MATLAB and your computer is within the UCL firewall. With MATLAB R2018b you can currently submit jobs to Myriad and Legion. Older versions (R2016b and R2018a) can be used to submit jobs to Legion only, if you still have an account. Prerequisites \u00a7 You must already have an account on the clusters! Have MATLAB R2018b installed on your local workstation/laptop. The local version must match the version running jobs. MATLAB R2018b can be downloaded from the UCL Software Database . Your local workstation/laptop installation of MATLAB must include the Parallel Computing toolbox. This is included in the UCL TAH MATLAB license and may be installed automatically. If your local workstation/laptop is not directly connected to the UCL network (at home for example), you need to have the UCL VPN client installed and running on it. Remote setup \u00a7 1) On the cluster you are using (Myriad or Legion) create a directory to hold remotely submitted job details. For example: mkdir ~/Scratch/Matlab_remote_jobs This directory needs to be in your Scratch directory as compute nodes need to be able to write to it. You should not use this directory for anything else. 2) On your local workstation/laptop create a directory to hold information about jobs that have been submitted to the cluster. Again you should not use this directory for anything else. 3) Download the support files for remote submission to Myriad and Legion for R2018b or support files for remote submission to Legion only for R2018a . Make sure you download the correct one for your version of MATLAB! 4) This step MUST be done while Matlab is shut down. Unzip the archive into MATLAB's local toolbox directory. Default locations for the local toolbox directory are: Linux: The default local toolbox location is /usr/local/MATLAB/R2018b/toolbox/local for R2018b. Navigate to this directory and use unzip -x archive_name . Mac OS X: The default local toolbox location is /Applications/MATLAB_R2018b.app/toolbox/local for R2018b. In order to view or change the contents of an application package, open /Applications in a Finder window. Then right-click the application and select \"View Package Contents.\" Then navigate to the appropriate directory. Note: if you don't have access to /Applications/MATLAB_R2018b.app/toolbox/local , you can unzip the support files into ~/Documents/MATLAB/ instead. Windows: The default local toolbox location is C:\\Program Files\\MATLAB\\R2018b\\toolbox\\local for R2018b. Extract the archive here. You can unzip the support files into Documents\\MATLAB\\ instead. 5) Download the parallelProfileMyriad function or the parallelProfileLegion function to your local workstation/laptop. It will need to be unzipped. These functions create a cluster profile for Myriad or Legion. 6) Start MATLAB, navigate to where you saved the parallelProfileMyriad.m or parallelProfileLegion.m files and run the function by typing: parallelProfileMyriad or: parallelProfileLegion at your MATLAB prompt (in your MATLAB Command Window if running the MATLAB GUI) and answer the questions. Submitting a job to the cluster \u00a7 1) You need to set the Grid Engine support environment variables on your local computer. Eg. in your MATLAB session set: setenv ('SGE_CONTEXT', 'exclusive'); # optional setenv ('SGE_OPT', 'h_rt=0:15:0,mem=2G,tmpfs=15G'); setenv ('SGE_PROJECT', '<your project ID>'); # optional 2) In your MATLAB session create a cluster object using the cluster profile created by the parallelProfile... functions. For Myriad: c = parcluster ('myriad_R2018b'); For Legion the profile is called legion_R2018b . 3) You can now create and submit jobs in a similar way to that shown in the DCS examples above starting from step 4 in the simple DCS job example or step 2 in the DCS job with multiple input files example . Viewing your results \u00a7 After submitting your job remotely from your desktop, you can close MATLAB and come back later. To see your jobs: Click \"Parallel > Monitor jobs\" This will bring up the job monitor where you can see the status of your jobs and whether they are finished. MATLAB numbers the jobs sequentially. Right-click on a job and choose \"fetch outputs\". This is what will be executed (for job4 on Myriad): myCluster = parcluster('myriad_R2018b'); job4 = myCluster.findJob('ID',4); job4_output = fetchOutputs(job4); The Workspace will show the available data and you can view your results. The data is fetched from the Matlab_remote_jobs directory you created on Myriad (or Legion) in Remote setup step 1, so that will also have files and directories in it called job1, job2 and so on. If you have already fetched the data, you can view the results straight away by selecting that job. If you need to reload everything, you can right-click on the job and the option will be to load variables instead. Writing intermediate results \u00a7 If you want to explicitly write out intermediate results, you need to provide a full path to somewhere in Scratch otherwise MATLAB will try to write them in your home, which isn't writable by the compute nodes. Troubleshooting remote jobs \u00a7 If you get a message like this when retrieving your outputs then something has gone wrong in your job: Task with ID xxx returned 0 outputs but 1 were expected You need to retrieve the debug log to find out what happened. Example: myCluster = parcluster('myriad_R2018b'); job4 = myCluster.findJob('ID',4); jobLog = getDebugLog (myCluster, job4); jobLog There will be a lot of output. Look for lines related to errors happening in your own code. Running MATLAB on GPUs \u00a7 This uses MATLAB's Mandelbrot Set GPU example . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/bin/bash -l # Batch script to run a GPU MATLAB job on Myriad. # Request 15 minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:15:0 # Request 2 gigabytes of RAM per core. #$ -l mem=2G # Request 15 gigabytes of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Request 1 GPU #$ -l gpu=1 # Request 1 CPU core. (Max on Myriad is 36) #$ -pe smp 1 # Request one MATLAB licence - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l matlab=1 # Set the name of the job. #$ -N Matlab_GPU_Job1 # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID. # This directory must already exist. #$ -wd /home/<your_UCL_id>/Scratch/Matlab_examples # Your work should be done in $TMPDIR cd $TMPDIR # Optional: Copy your script and any other files into $TMPDIR. # If not, you must always refer to them using a full path. cp /home/ccaabaa/Software/Matlab/Mandelbrot_GPU.m $TMPDIR module unload compilers mpi module load compilers/gnu/4.9.2 module load xorg-utils/X11R7.7 module load matlab/full/r2018b/9.5 module list # These echoes output what you are about to run echo \"\" echo \"Running matlab -nosplash -nodisplay < Mandelbrot_GPU.m ...\" echo \"\" matlab -nosplash -nodesktop -nodisplay < Mandelbrot_GPU.m # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Matlab_examples/files_from_job_${JOB_ID}.tgz $TMPDIR # Make sure you have given enough time for the copy to complete!","title":"MATLAB"},{"location":"Software_Guides/Matlab/#setup","text":"You need to load MATLAB once from a login node before you can submit any jobs. This allows it to set up your ~/.matlab directory as a symbolic link to ~/Scratch/.matlab so that the compute nodes can write to it. # on a login node module load xorg-utils/X11R7.7 module load matlab/full/r2018b/9.5 You can run module avail matlab to see all the available installed versions.","title":"Setup"},{"location":"Software_Guides/Matlab/#single-node-multi-threaded-batch-jobs","text":"This is the simplest way to start using MATLAB on the cluster. You will need a .m file containing the MATLAB commands you want to carry out. Here is an example jobscript which you would submit using the qsub command, after you have loaded the MATLAB module once on a login node as mentioned in Setup : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #!/bin/bash -l # Batch script to run a multi-threaded MATLAB job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per core. #$ -l mem=1G # Request 15 gigabytes of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Request a number of threads (which will use that number of cores). # On Myriad you can set the number of threads to a maximum of 36. #$ -pe smp 36 # Request one MATLAB licence - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l matlab=1 # Set the name of the job. #$ -N Matlab_multiThreadedJob1 # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID. # This directory must already exist. #$ -wd /home/<your_UCL_id>/Scratch/Matlab_examples # Your work should be done in $TMPDIR cd $TMPDIR module load xorg-utils/X11R7.7 module load matlab/full/r2018b/9.5 # outputs the modules you have loaded module list # Optional: copy your script and any other files into $TMPDIR. # This is only practical if you have a small number of files. # If you do not copy them in, you must always refer to them using a # full path so they can be found, eg. ~/Scratch/Matlab_examples/analyse.m cp ~/Scratch/Matlab_examples/myMatlabJob.m $TMPDIR cp ~/Scratch/Matlab_examples/initialise.m $TMPDIR cp ~/Scratch/Matlab_examples/analyse.m $TMPDIR # These echoes output what you are about to run echo \"\" echo \"Running matlab -nosplash -nodisplay < myMatlabJob.m ...\" echo \"\" matlab -nosplash -nodesktop -nodisplay < myMatlabJob.m # Or if you did not copy your files: # matlab -nosplash -nodesktop -nodisplay < ~/Scratch/Matlab_examples/myMatlabJob.m # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Matlab_examples/files_from_job_${JOB_ID}.tgz $TMPDIR # Make sure you have given enough time for the copy to complete! Alternative syntax: Instead of using Unix input redirection like this: matlab -nosplash -nodesktop -nodisplay < $Matlab_infile you can also do: matlab -nosplash -nodesktop -nodisplay -r $Matlab_infile","title":"Single node multi-threaded batch jobs"},{"location":"Software_Guides/Matlab/#run-without-the-jvm-to-reduce-overhead","text":"You can give the -nojvm option to tell MATLAB to run without the Java Virtual Machine. This will speed up startup time, possibly execution time, and remove some memory overhead, but will prevent you using any tools that require Java (eg, tools that use the Java API for I/O and networking like URLREAD, or call Java object methods).","title":"Run without the JVM to reduce overhead"},{"location":"Software_Guides/Matlab/#run-single-threaded","text":"Most of the time, MATLAB will create many threads and use them as it wishes. If you know your job is entirely single-threaded, you can force MATLAB to run with only one thread on one core, which will allow you to have more jobs running at once. To request one core only, set #$ -pe smp 1 in your jobscript. Run MATLAB like this: matlab -nosplash -nodesktop -nodisplay -nojvm -singleCompThread < $Matlab_infile The -singleCompThread forces MATLAB to run single-threaded, and the -nojvm tells it to run without the Java Virtual Machine, as above.","title":"Run single-threaded"},{"location":"Software_Guides/Matlab/#using-the-matlab-gui-interactively","text":"You can run MATLAB interactively for short amounts of time on the login nodes (please do not do this if your work will be resource-intensive). You can also run it interactively in a qrsh session on the compute nodes. Launching with matlab will give you the full graphical user interface - you will need to have logged in to the cluster with X-forwarding on for this to work. Launching with matlab -nodesktop -nodisplay will give you the MATLAB terminal.","title":"Using the MATLAB GUI interactively"},{"location":"Software_Guides/Matlab/#submitting-jobs-using-the-distributed-computing-server-dcs","text":"You must have loaded the MATLAB module once from a login node as described in Setup before you can submit any Matlab DCS jobs. MATLAB DCS jobs must be submitted from an interactive or scripted Matlab session which can be running on the cluster login nodes or on your own machine. MATLAB DCS jobs will only work inside a single node on our clusters. On Myriad this means a maximum of 36 workers can be used per job.","title":"Submitting jobs using the Distributed Computing Server (DCS)"},{"location":"Software_Guides/Matlab/#importing-the-cluster-profile","text":"You need to import the cluster profile into your MATLAB environment and set it as the default before you can submit DCS jobs. This only needs doing once. The imported profile will be saved in your MATLAB settings directory. Importing the profile can be done either by calling MATLAB functions or via the graphical interface. The profile is stored here (for 2018b): /shared/ucl/apps/Matlab/toolbox_local/Legion/R2018b/LegionGraceMyriadProfile_R2018b.settings","title":"Importing the cluster profile"},{"location":"Software_Guides/Matlab/#import-using-matlab-functions","text":"Run these functions from a MATLAB session: profile_Legion = parallel.importProfile ('/shared/ucl/apps/Matlab/toolbox_local/Legion/R2018b/LegionGraceMyriadProfile_R2018b.settings'); parallel.defaultClusterProfile ('LegionGraceMyriadProfileR2018b');","title":"Import using MATLAB functions"},{"location":"Software_Guides/Matlab/#import-from-matlab-gui","text":"To import using the graphical interface instead, do this. From the Home tab select the Parallel menu and click Create and Manage Clusters... . The Cluster Profile Manager window will open. Select Import and in the Import Profiles from file window navigate to the LegionGraceMyriadProfile_R2018b.settings file shown above and select Open. Select the resulting LegionGraceMyriadProfileR2018b profile and click Set as Default . The Cluster Profile Manager window should now look like this: In both cases after you exit MATLAB your cluster profile is saved for future use.","title":"Import from MATLAB GUI"},{"location":"Software_Guides/Matlab/#environment-variables-needed-for-job-submission","text":"We have set up three Grid Engine environment variables to assist with job submission from within MATLAB. These are needed to pass in job resource parameters that aren't supported by the internal MATLAB job submission mechanism. SGE_CONTEXT : a comma-separated list of variables treated as if added via the -ac option, eg. exclusive SGE_OPT : a comma-separated list of resources treated as if added via the -l option, eg. h_rt=0:10:0,mem=1G,tmpfs=15G SGE_PROJECT : a project treated as if added via the -P option (not normally needed). -ac exclusive prevents anything else running on the same node as your job, even if you aren't using all the cores. This is no longer a necessary option for MATLAB jobs. There are two ways to set these: 1) Before starting your MATLAB session, using the usual Bash method of exporting environment variables: export SGE_CONTEXT=exclusive export SGE_OPT=h_rt=0:15:0,mem=2G,tmpfs=15G export SGE_PROJECT=<your project ID> 2) Inside your MATLAB session, using MATLAB's setenv function: setenv ('SGE_CONTEXT', 'exclusive'); setenv ('SGE_OPT', 'h_rt=0:15:0,mem=2G,tmpfs=15G'); setenv ('SGE_PROJECT', '<your project ID>');","title":"Environment variables needed for job submission"},{"location":"Software_Guides/Matlab/#example-a-simple-dcs-job","text":"This submits a job from inside a MATLAB session running on a login node. You need to start MATLAB from a directory in Scratch - jobs will inherit this as their working directory. This is an example where you have only one MATLAB source file. 1) Change to the directory in Scratch you want the job to run from and set the SGE environment variables. cd ~/Scratch/Matlab_examples export SGE_OPT=h_rt=0:10:0,mem=1G,tmpfs=15G 2) Either start the MATLAB GUI: matlab or start a MATLAB terminal session: matlab -nodesktop -nodisplay 3) Inside MATLAB, create a cluster object using the cluster profile: c = parcluster ('LegionGraceMyriadProfileR2018b'); 4) Use your cluster object to create a job object of the type you need. For this example the job is a parallel job with communication between MATLAB workers of type \"Single Program Multiple Data\": myJob = createCommunicatingJob (c, 'Type', 'SPMD'); 5) Set the number of workers: num_workers = 36; 6) Tell the job the files needed to be made available to each worker - in this example there is only one file: myJob.AttachedFiles = {'colsum.m'}; colsum.m contains the simple magic square example from the MATLAB manual \"Parallel Computing Toolbox User's Guide\". 7) Set the minimum and maximum number of workers for the job (we are asking for an exact number here by setting them the same): myJob.NumWorkersRange = [num_workers, num_workers]; 8) Create a MATLAB task to be executed as part of the job. Here it consists of executing the MATLAB function colsum . The other arguments say that the task returns one parameter and there are no input arguments to the colsum function: task = createTask (myJob, @colsum, 1, {}); 9) Submit the job: submit (myJob); Your job is now submitted to the scheduler and you can see its queue status in qstat as normal. If you were using the MATLAB GUI you can also monitor jobs by selecting Monitor Jobs from the Parallel menu on the Home tab. 10) When the job has completed get the results using: results = fetchOutputs(myJob) You can access the job log from MATLAB using: logMess = getDebugLog (c, myJob);","title":"Example: a simple DCS job"},{"location":"Software_Guides/Matlab/#example-a-dcs-job-with-more-than-one-input-file","text":"This example has several input files. The job type is \"MATLAB Pool\". A \"Pool\" job runs the specified task function with a MATLAB pool available to run the body of parfor loops or spmd blocks and is the default job type. This example was kindly supplied to assist in testing MATLAB by colleagues from CoMPLEX. The first part of creating the job is the same as the above example apart from the longer runtime and larger amount of memory per core: 1) Change into a directory in Scratch, set the SGE variables and launch MATLAB: cd ~/Scratch/Matlab_examples export SGE_OPT=h_rt=1:0:0,mem=2G,tmpfs=15G matlab to launch the GUI or: matlab -nodesktop -nodisplay to start a terminal session. c = parcluster ('LegionGraceMyriadProfileR2018b'); 2) Using our cluster object create a job object of type \"Pool\": myJob2 = createCommunicatingJob (c, 'Type', 'Pool'); 3) Set the number of workers and another variable used by the example: num_workers = 8; simulation_duration_ms = 1000; 4) Tell the job all the input files needed to be made available to each worker as a cell array: myJob2.AttachedFiles = { 'AssemblyFiniteDifferencesMatrix.m' 'AssemblyFiniteDifferencesRightHandSide.m' 'CellModelsComputeIonicCurrents.m' 'CellModelsGetVoltage.m' 'CellModelsInitialise.m' 'CellModelsSetVoltage.m' 'GetStimuliForTimeStep.m' 'SubmitMonodomainJob.m' 'RegressionTest.m' 'RunAndVisualiseMonodomainSimulation.m' 'SolveLinearSystem.m' 'luo_rudy_1991_iionic.m' 'luo_rudy_1991_time_deriv.m'}; 5) Set the minimum and maximum number of workers for the job: myJob2.NumWorkersRange = [num_workers, num_workers]; 6) Create a MATLAB task to be executed as part of the job. For this example it will consist of executing the MATLAB function RunAndVisualiseMonodomainSimulation . The rest of the arguments indicate that the task returns three parameters and there are five input arguments to the function. These are passed as a cell array: task = createTask (myJob2, @RunAndVisualiseMonodomainSimulation, 3, {5000, simulation_duration_ms, 1.4, 1.4, false}); 7) Submit the job: submit (myJob2); As before use fetchOutputs to collect the results. If you closed your session, you can get your results by: c = parcluster ('LegionGraceMyriadProfileR2018b'); # get a cluster object jobs = findJob(c) # get a list of jobs submitted to that cluster job = jobs(3); # pick a particular job results = fetchOutputs(job) You can get other information: diary(job) will give you the job diary, and load(job) will load the workspace.","title":"Example: a DCS job with more than one input file"},{"location":"Software_Guides/Matlab/#further-reading","text":"There is a lot more information about using the MATLAB Distributed Computing Server in the MATLAB manual: Parallel Computing Toolbox User\u2019s Guide .","title":"Further reading"},{"location":"Software_Guides/Matlab/#submitting-matlab-jobs-from-your-workstationlaptop","text":"You can submit MATLAB jobs to Myriad and Legion from MATLAB sessions running on your own desktop workstation or laptop systems provided they are running the same version of MATLAB and your computer is within the UCL firewall. With MATLAB R2018b you can currently submit jobs to Myriad and Legion. Older versions (R2016b and R2018a) can be used to submit jobs to Legion only, if you still have an account.","title":"Submitting MATLAB jobs from your workstation/laptop"},{"location":"Software_Guides/Matlab/#prerequisites","text":"You must already have an account on the clusters! Have MATLAB R2018b installed on your local workstation/laptop. The local version must match the version running jobs. MATLAB R2018b can be downloaded from the UCL Software Database . Your local workstation/laptop installation of MATLAB must include the Parallel Computing toolbox. This is included in the UCL TAH MATLAB license and may be installed automatically. If your local workstation/laptop is not directly connected to the UCL network (at home for example), you need to have the UCL VPN client installed and running on it.","title":"Prerequisites"},{"location":"Software_Guides/Matlab/#remote-setup","text":"1) On the cluster you are using (Myriad or Legion) create a directory to hold remotely submitted job details. For example: mkdir ~/Scratch/Matlab_remote_jobs This directory needs to be in your Scratch directory as compute nodes need to be able to write to it. You should not use this directory for anything else. 2) On your local workstation/laptop create a directory to hold information about jobs that have been submitted to the cluster. Again you should not use this directory for anything else. 3) Download the support files for remote submission to Myriad and Legion for R2018b or support files for remote submission to Legion only for R2018a . Make sure you download the correct one for your version of MATLAB! 4) This step MUST be done while Matlab is shut down. Unzip the archive into MATLAB's local toolbox directory. Default locations for the local toolbox directory are: Linux: The default local toolbox location is /usr/local/MATLAB/R2018b/toolbox/local for R2018b. Navigate to this directory and use unzip -x archive_name . Mac OS X: The default local toolbox location is /Applications/MATLAB_R2018b.app/toolbox/local for R2018b. In order to view or change the contents of an application package, open /Applications in a Finder window. Then right-click the application and select \"View Package Contents.\" Then navigate to the appropriate directory. Note: if you don't have access to /Applications/MATLAB_R2018b.app/toolbox/local , you can unzip the support files into ~/Documents/MATLAB/ instead. Windows: The default local toolbox location is C:\\Program Files\\MATLAB\\R2018b\\toolbox\\local for R2018b. Extract the archive here. You can unzip the support files into Documents\\MATLAB\\ instead. 5) Download the parallelProfileMyriad function or the parallelProfileLegion function to your local workstation/laptop. It will need to be unzipped. These functions create a cluster profile for Myriad or Legion. 6) Start MATLAB, navigate to where you saved the parallelProfileMyriad.m or parallelProfileLegion.m files and run the function by typing: parallelProfileMyriad or: parallelProfileLegion at your MATLAB prompt (in your MATLAB Command Window if running the MATLAB GUI) and answer the questions.","title":"Remote setup"},{"location":"Software_Guides/Matlab/#submitting-a-job-to-the-cluster","text":"1) You need to set the Grid Engine support environment variables on your local computer. Eg. in your MATLAB session set: setenv ('SGE_CONTEXT', 'exclusive'); # optional setenv ('SGE_OPT', 'h_rt=0:15:0,mem=2G,tmpfs=15G'); setenv ('SGE_PROJECT', '<your project ID>'); # optional 2) In your MATLAB session create a cluster object using the cluster profile created by the parallelProfile... functions. For Myriad: c = parcluster ('myriad_R2018b'); For Legion the profile is called legion_R2018b . 3) You can now create and submit jobs in a similar way to that shown in the DCS examples above starting from step 4 in the simple DCS job example or step 2 in the DCS job with multiple input files example .","title":"Submitting a job to the cluster"},{"location":"Software_Guides/Matlab/#viewing-your-results","text":"After submitting your job remotely from your desktop, you can close MATLAB and come back later. To see your jobs: Click \"Parallel > Monitor jobs\" This will bring up the job monitor where you can see the status of your jobs and whether they are finished. MATLAB numbers the jobs sequentially. Right-click on a job and choose \"fetch outputs\". This is what will be executed (for job4 on Myriad): myCluster = parcluster('myriad_R2018b'); job4 = myCluster.findJob('ID',4); job4_output = fetchOutputs(job4); The Workspace will show the available data and you can view your results. The data is fetched from the Matlab_remote_jobs directory you created on Myriad (or Legion) in Remote setup step 1, so that will also have files and directories in it called job1, job2 and so on. If you have already fetched the data, you can view the results straight away by selecting that job. If you need to reload everything, you can right-click on the job and the option will be to load variables instead.","title":"Viewing your results"},{"location":"Software_Guides/Matlab/#writing-intermediate-results","text":"If you want to explicitly write out intermediate results, you need to provide a full path to somewhere in Scratch otherwise MATLAB will try to write them in your home, which isn't writable by the compute nodes.","title":"Writing intermediate results"},{"location":"Software_Guides/Matlab/#troubleshooting-remote-jobs","text":"If you get a message like this when retrieving your outputs then something has gone wrong in your job: Task with ID xxx returned 0 outputs but 1 were expected You need to retrieve the debug log to find out what happened. Example: myCluster = parcluster('myriad_R2018b'); job4 = myCluster.findJob('ID',4); jobLog = getDebugLog (myCluster, job4); jobLog There will be a lot of output. Look for lines related to errors happening in your own code.","title":"Troubleshooting remote jobs"},{"location":"Software_Guides/Matlab/#running-matlab-on-gpus","text":"This uses MATLAB's Mandelbrot Set GPU example . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/bin/bash -l # Batch script to run a GPU MATLAB job on Myriad. # Request 15 minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:15:0 # Request 2 gigabytes of RAM per core. #$ -l mem=2G # Request 15 gigabytes of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Request 1 GPU #$ -l gpu=1 # Request 1 CPU core. (Max on Myriad is 36) #$ -pe smp 1 # Request one MATLAB licence - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l matlab=1 # Set the name of the job. #$ -N Matlab_GPU_Job1 # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID. # This directory must already exist. #$ -wd /home/<your_UCL_id>/Scratch/Matlab_examples # Your work should be done in $TMPDIR cd $TMPDIR # Optional: Copy your script and any other files into $TMPDIR. # If not, you must always refer to them using a full path. cp /home/ccaabaa/Software/Matlab/Mandelbrot_GPU.m $TMPDIR module unload compilers mpi module load compilers/gnu/4.9.2 module load xorg-utils/X11R7.7 module load matlab/full/r2018b/9.5 module list # These echoes output what you are about to run echo \"\" echo \"Running matlab -nosplash -nodisplay < Mandelbrot_GPU.m ...\" echo \"\" matlab -nosplash -nodesktop -nodisplay < Mandelbrot_GPU.m # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Matlab_examples/files_from_job_${JOB_ID}.tgz $TMPDIR # Make sure you have given enough time for the copy to complete!","title":"Running MATLAB on GPUs"},{"location":"Software_Guides/Other_Software/","text":"We maintain a large software stack that is available across all our clusters (licenses permitting). We use environment modules to let you manage which specific versions of software packages you are using. General use of environment modules \u00a7 We have a default set of modules that everyone has loaded when they log in: these include the current default compiler and MPI, some utili ties to make your life easier and some text editors. Summary of module commands \u00a7 module avail # shows available modules module whatis # shows available modules with brief explanations module list # shows your currently loaded modules module load <module> # load this module module unload <module> # unload this module module purge # unload all modules module show <module> # Shows what the module requires and what it sets up module help <module> # Shows a longer text description for the software Find out if a software package is installed and load it \u00a7 Generically, the way you find out if a piece of software is installed is to run module avail packagename This gives you a list of all the modules we have that match the name you searched for. You can then type module show packagename and it will show you the other software dependencies this module has: these have to be loaded first. It also shows where the software is installed and what environment variables it sets up. Once you have found the modules you want to load, it is good practice to refer to them using their full name, including the version. If you use the short form ( package rather than package/5.1.2/gnu-4.9.2 ) then a matching module will be loaded, but if we install a different version, your jobs may begin using the new one and you would not know which version created your results. Different software versions may not be compatible or may have different default settings, so this is undesirable. You may need to unload current modules in order to load some requirements (eg different compiler, different MPI). This example switches from Intel compiler and MPI modules to GNU ones. module unload compilers mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/3.1.4/gnu-4.9.2 You can use the short name when unloading things because there is usually only one match in your current modules. The last part of a module name usually tells you what compiler it was built with. There may be a GNU compiler version and an Intel compiler version of the same software available. Once the module is loaded, you should have all the usual executables in your path, and can use its commands. You load modules in exactly the same way inside a jobscript. Notes on how to run specific packages \u00a7 The packages below have slightly complex commands needed to run them, or different settings needed on our clusters. These are examples of what should be added to your jobscripts. Change the module load command to the version you want to load and check that the dependencies are the same. The top of a jobscript should contain your resource requests . See also examples of full jobscripts . ABAQUS \u00a7 ABAQUS is a commercial software suite for finite element analysis and computer-aided engineering. You must be authorised by the Mech Eng Department before you can be added to the group controlling access to ABAQUS (legabq). A serial interactive analysis can be run on the compute nodes (via a qrsh session) like this: abaqus interactive job=myJobSerial input=myInputFile.inp A parallel job can be run like this (fill in your own username): module load abaqus/2017 INPUT_FILE=/home/<username>/ABAQUS/heattransfermanifold_cavity_parallel.inp ABAQUS_ARGS= ABAQUS_PARALLELSCRATCH=/home/<username>/Scratch/Abaqus/parallelscratch # creates a parallel scratch dir and a new working dir for this job mkdir -p $ABAQUS_PARALLELSCRATCH mkdir -p $JOB_NAME.$JOB_ID cd $JOB_NAME.$JOB_ID cp $INPUT_FILE . INPUT=$(basename $INPUT_FILE) abaqus interactive cpus=$NSLOTS mp_mode=mpi job=$INPUT.$JOB_ID input=$INPUT \\ scratch=$ABAQUS_PARALLELSCRATCH $ABAQUS_ARGS BEAST \u00a7 BEAST is an application for Bayesian MCMC analysis of molecular sequences orientated towards rooted, time-measured phylogenies inferred using strict or relaxed molecular clock models. Note that FigTree and Tracer are available as standalone modules. The addons DISSECT, MODEL_SELECTION, and SNAPP are installed for BEAST. cd $TMPDIR module load java/1.8.0_45 module load beast/2.3.0 beast -threads $OMP_NUM_THREADS ~/Scratch/BEAST/gopher.xml # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/BEAST/files_from_job_$JOB_ID.tar.gz $TMPDIR Bowtie \u00a7 Bowtie 1 and 2 are tools for aligning sequencing reads to their reference sequences. Bowtie 1 and 2 are available. For reads longer than about 50 bp Bowtie 2 is generally faster, more sensitive, and uses less memory than Bowtie 1. For relatively short reads (e.g. less than 50 bp) Bowtie 1 is sometimes faster and/or more sensitive. For further differences, see How is Bowtie 2 different from Bowtie 1? . Bowtie sets $BT1_HOME and Bowtie2 sets $BT2_HOME . You can have both modules loaded at once. cd $TMPDIR module load bowtie2/2.2.5 # Run Bowtie2 example from getting started guide: # http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#getting-started-with-bowtie-2-lambda-phage-example bowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus bowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Bowtie2_output/files_from_job_$JOB_ID.tgz $TMPDIR CASTEP \u00a7 CASTEP is a full-featured materials modelling code based on a first-principles quantum mechanical description of electrons and nuclei. module load castep/17.21/intel-2017 # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun castep.mpi input If you have access to the source code and wish to build your own copy, it has been suggested that compiling with these options (on Grace) gave a build that ran about 10% faster than the default compilation options: make COMMS_ARCH=mpi SUBARCH=mpi FFT=mkl MATHLIBS=mkl10 BUILD=fast Do check for numerical accuracy with any optimisations you carry out. Cctools \u00a7 Provides the Parrot connector to CVMFS, the CernVM File System. By default, the cctools module sets the following: export PARROT_CVMFS_REPO=<default-repositories> export PARROT_ALLOW_SWITCHING_CVMFS_REPOSITORIES=yes export HTTP_PROXY=DIRECT; export PARROT_HTTP_PROXY=DIRECT; Example usage - will list the contents of the repository then exit: module load cctools/7.0.11/gnu-4.9.2 parrot_run bash ls /cvmfs/alice.cern.ch exit That will create the cache in /tmp/parrot.xxxxx on the login nodes when run interactively. To use in a job, you will want to put the cache somewhere in your space that the compute nodes can access. You can set the cache to be in your Scratch, or to $TMPDIR on the nodes if it just needs to exist for the duration of that job. export PARROT_CVMFS_ALIEN_CACHE=</path/to/cache> CFD-ACE \u00a7 CFD-ACE+ is a commercial computational fluid dynamics solver developed by ESI Group. It solves the conservation equations of mass, momentum, energy, chemical species and other scalar transport equations using the finite volume method. These equations enable coupled simulations of fluid, thermal, chemical, biological, electrical and mechanical phenomena. The license is owned by the Department of Mechanical Engineering who must give permission for users to be added to the group lgcfdace . module load cfd-ace/2018.0 CFD-SOLVER -model 3Dstepchannel_060414.DTF -num $NSLOTS -wd `pwd` \\ -hosts $TMPDIR/machines -rsh=ssh -decomp -metis -sim 1 -platformmpi -job COMSOL \u00a7 COMSOL Multiphysics is a cross-platform finite element analysis, solver and multiphysics simulation software. Electrical Engineering have a group license for version 52 and must give permission for users to be added to the group legcomsl . Chemical Engineering have a Departmental License for version 53 and members of that department may be added to the group lgcomsol . # Run a parallel COMSOL job # Versions 52 and 52a have this additional module prerequisite module load xulrunner/3.6.28/gnu-4.9.2 # pick the version to load module load comsol/53a # Parallel multinode options: # $NHOSTS gets the number of nodes the job is running on and # $TMPDIR/machines is the machinefile that tells it which nodes. # These are automatically set up in a \"-pe mpi\" job environment. comsol -nn $NHOSTS -clustersimple batch -f $TMPDIR/machines -inputfile micromixer_batch.mph \\ -outputfile micromixer_batch_output_${JOB_ID}.mph # On Myriad you need to specify the fabric: comsol batch -f $TMPDIR/machines -np $NSLOTS -mpifabrics shm:tcp \\ -inputfile micromixer_batch.mph -outputfile micromixer_batch_output_${JOB_ID}.mph CP2K \u00a7 CP2K performs atomistic and molecular simulations. module unload compilers mpi module load mpi/openmpi/3.0.0/gnu-4.9.2 module load cp2k/5.1/ompi/gnu-4.9.2 # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun cp2k.popt < input.in > output.out For CP2K 4.1 there is also a Chemistry department version with submission script generator. To access it: module load chemistry-modules module load submission-scripts The command submitters will then list the submitters available. You can then run cp2k.submit which will ask you questions in order to create a suitable jobscript. The cp2k.submit submitter takes up to 6 arguments, and any omitted will be asked for interactively: cp2k.submit \u00abinput_file\u00bb \u00abcores\u00bb \u00abversion\u00bb \u00abmaximum_run_time\u00bb \u00abmemory_per_core\u00bb \u00abjob_name\u00bb So, for example: cp2k.submit water.inp 8 4.1 2:00:00 4G mywatermolecule would request a job running CP2K 4.1 with the input file water.inp , on 8 cores, with a maximum runtime of 2 hours, with 4 gigabytes of memory per core, and a job name of mywatermolecule . CRYSTAL \u00a7 CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. CRYSTAL is commercial software which is available free of charge to UK academics. You must obtain a license from Crystal Solutions: How to get CRYSTAL - Academic UK license . You need to create an account and then request to be upgraded to Academic UK. Access to CRYSTAL is enabled by being a member of the reserved application group legcryst . For proof of access we accept emails from CRYSTAL saying your account has been upgraded to \"Academic UK\", or a screenshot of your account page showing you have the full download available rather than just the demo version. module unload mpi module load mpi/openmpi/2.1.2/intel-2017 module load crystal17/v1.0.1 # 9. Create a directory for this job and copy the input file into it. mkdir test00 cd test00 cp ~/Scratch/Crystal17/test_cases/inputs/test00.d12 INPUT # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. # The CRYSTAL module sets $CRYxx_EXEDIR and $VERSION environment variables. gerun $CRY17_EXEDIR/$VERSION/Pcrystal # Similarly, for Pproperties the command would be gerun $CRY17_EXEDIR/$VERSION/Pproperties FreeSurfer \u00a7 FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data. Freesurfer can use threads to run on multiple cores in one node: request the number with -pe smp in the resource-request part of your jobscript. #$ -pe smp 4 module load xorg-utils/X11R7.7 module load freesurfer/6.0.0 export SUBJECTS_DIR=~/Scratch/FreeSurfer_examples/subjects # -openmp $NSLOTS runs with the number of threads you requested recon-all -openmp $NSLOTS -i sample-001.nii.gz -s bert -all GAMESS \u00a7 The General Atomic and Molecular Electronic Structure System (GAMESS) is a general ab initio quantum chemistry package. The GAMESS module should be loaded once from a login node before submitting a job - this creates the ~/Scratch/gamess directory for you which is used as USERSCR to write some scratch files during the job. If you don't want to keep these files and would prefer them to be written to $TMPDIR instead, you can put export $GAMESS_USERSCR=$TMPDIR in your jobscript after the module load command. module unload compilers mpi module load compilers/intel/2015/update2 module load mpi/intel/2015/update3/intel module load gamess/5Dec2014_R1/intel-2015-update2 # Optional: set where the USERSCR files go. # By default, the module sets it to ~/Scratch/gamess export $GAMESS_USERSCR=$TMPDIR rungms exam01.inp 00 $NSLOTS $(ppn) GATK \u00a7 The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Version 4 of GATK is BSD-licensed so does not require a group to control access to the software. Version 3 of GATK requires you to agree to the GATK license before we can add you to the leggatk group which gives you access: you can do this by downloading GATK 3 from The Broad Institute GATK download page , reading the license, and telling us you agree to it. You may need to create a gatkforums account before you can download. GATK 3 uses Java 1.7 (the system Java) so you do not need to load a Java module. GATK 4 uses 1.8 so you need to load java/1.8.0_92 first. Load the version you want, then to run GATK you should either prefix the .jar you want to run with $GATKPATH : java -Xmx2g -jar $GATKPATH/GenomeAnalysisTK.jar OPTION1=value1 OPTION2=value2... Or we provide wrappers, so you can run it one of these ways instead: GenomeAnalysisTK OPTION1=value1 OPTION2=value2... gatk OPTION1=value1 OPTION2=value2... GROMACS \u00a7 We have many versions of GROMACS installed, some built with Plumed. The module name will indicate this. Which executable you should run depends on the problem you wish to solve. For both single and double precision version builds, serial binaries and an MPI binary for mdrun ( mdrun_mpi for newer versions, gmx_mpi for Plumed and some older versions) are provided. Double precision binaries have a _d suffix (so gmx_d , mdrun_mpi_d , gmx_mpi_d etc). # Example for gromacs/2019.3/intel-2018 module unload compilers mpi module load compilers/intel/2018/update3 module load mpi/intel/2018/update3/intel module load gromacs/2019.3/intel-2018 # Run GROMACS - replace with mdrun command line suitable for your job! gerun mdrun_mpi -v -stepout 10000 # Plumed example for gromacs/2019.3/plumed/intel-2018 module unload compilers mpi module load compilers/intel/2018/update3 module load mpi/intel/2018/update3/intel module load libmatheval module load flex module load plumed/2.5.2/intel-2018 module load gromacs/2019.3/plumed/intel-2018 # Run GROMACS - replace with mdrun command line suitable for your job! gerun gmx_mpi -v -stepout 10000 Passing in options to GROMACS non-interactively \u00a7 Some GROMACS executables like trjconv normally take interactive input. You can't do this in a jobscript, so you need to pass in the input you would normally type in. There are several ways of doing this, mentioned at GROMACS Documentation - Using Commands in Scripts . The simplest is to echo the input in and keep your gmx options as they would normally be. If the inputs you would normally type were 3 and 3, then you can do this: echo 3 3 | gmx whatevercommand -options Checkpoint and restart \u00a7 GROMACS has built-in checkpoint and restart ability, so you can use this if your runs will not complete in the maximum 48hr wallclock time. Have a look at the GROMACS manual for full details, as there are more options than mentioned here. You can tell GROMACS to write a checkpoint file when it is approaching the maximum wallclock time available, and then exit. In this case, we had asked for 48hrs wallclock. This tells GROMACS to start from the last checkpoint if there is one, and write a new checkpoint just before it reaches 47 hrs runtime. gerun mdrun_mpi -cpi -maxh 47 <options> The next job you submit with the same script will carry on from the checkpoint the last job wrote. You could use job dependencies to submit two identical jobs at the same time and have one dependent on the other, so it won't start until the first finishes - have a look at man qsub for the -hold_jid option. You can also write checkpoints at given intervals: # Write checkpoints every 120 mins, start from checkpoint if there is one. gerun mdrun_mpi -cpi -cpt 120 <options> Hammock \u00a7 Hammock is a tool for peptide sequence clustering. It is able to cluster extremely large amounts of short peptide sequences into groups sharing sequence motifs. Typical Hammock applications are NGS-based experiments using large combinatorial peptide libraries, e.g. Phage display. Hammock has to be installed in your own space to function, so we provide a hammock module that contains the main dependencies and creates a quick-install alias: # on the login nodes module unload compilers module load hammock/1.0.5 do-hammock-install This will install Hammock 1.0.5 in your home, edit settings.prop to use clustal-omega and hmmer from our modules and tell it to write temporary files in your Scratch directory (in the form Hammock_temp_time ). # in your jobscript module unload compilers module load hammock/1.0.5 # This copies the MUSI example that comes with Hammock into your working # directory and runs it. The module sets $HAMMOCKPATH for you. # You must set the output directory to somewhere in Scratch with -d. # Below makes a different outputdir per job so multiple runs don't overwrite files. cp $HAMMOCKPATH/../examples/MUSI/musi.fa . outputdir=~/Scratch/hammock-examples/musi_$JOB_ID mkdir -p $outputdir echo \"Running java -jar $HAMMOCKPATH/Hammock.jar full -i musi.fa -d $outputdir\" java -jar $HAMMOCKPATH/Hammock.jar full -i musi.fa -d $outputdir HOPSPACK \u00a7 HOPSPACK (Hybrid Optimization Parallel Search PACKage) solves derivative-free optimization problems using an open source, C++ software framework. We have versions of HOPSPACK built using the GNU compiler and OpenMPI, and the Intel compiler and MPI. This example shows the GNU version. Serial and parallel versions are available, HOPSPACK_main_mpi and HOPSPACK_main_serial . module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.8.4/gnu-4.9.2 module load atlas/3.10.2/gnu-4.9.2 module load hopspack/2.0.2/gnu-4.9.2 # Add the examples directory we are using to our path. # Replace this with the path to your own executables. export PATH=$PATH:~/Scratch/examples/1-var-bnds-only/ # Run parallel HOPSPACK. # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun HOPSPACK_main_mpi ~/Scratch/examples/1-var-bnds-only/example1_params.txt > example1_output.txt IDL \u00a7 IDL is a complete environment and language for the analysis and visualisation of scientific and other technical data. It can be used for everything from quick interactive data exploration to building complex applications. Single-threaded jobscript: cd $TMPDIR module load idl/8.4.1 # Copy IDL source files to $TMPDIR cp ~/Scratch/IDL/fib.pro $TMPDIR cp ~/Scratch/IDL/run1.pro $TMPDIR idl -queue -e @run1.pro # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/IDL_output/files_from_job_$JOB_ID.tgz $TMPDIR Parallel jobscript: cd $TMPDIR module load idl/8.1 # this sets the IDL thread pool: do not change this export IDL_CPU_TPOOL_NTHREADS=$OMP_NUM_THREADS # Copy IDL source files to $TMPDIR cp ~/Scratch/IDL/fib.pro $TMPDIR cp ~/Scratch/IDL/run2mp.pro $TMPDIR idl -queue -e @run2mp.pro # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/IDL_output/files_from_job_$JOB_ID.tgz $TMPDIR JAGS \u00a7 JAGS (Just Another Gibbs Sampler) is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation not wholly unlike BUGS. Use this to use JAGS in standalone command line mode: module unload compilers mpi module load compilers/gnu/4.9.2 module load openblas/0.2.14/gnu-4.9.2 module load jags/4.2.0/gnu.4.9.2-openblas We have also added JAGS support to r/recommended using the rjags and R2jags R packages. LAMMPS \u00a7 LAMMPS is an open source parallel molecular dynamics code which exhibits good scaling in a wide range of environments. The LAMMPS binaries are called lmp_$cluster and all have an lmp_default symlink which can be used. LAMMPS-8Dec15 and later were built with additional packages kspace , replica , rigid , and class2 . The versions from lammps-16Mar18-basic_install onwards (not lammps/16Mar18/intel-2017 ) have most of the included packages built. There are also userintel and gpu versions from this point. We do not install the LAMMPS user packages as part of our central install, but you can build your own version with the ones that you want in your space. module unload compilers mpi module load compilers/intel/2018 module load mpi/intel/2018 module load lammps/16Mar18/basic/intel-2018 # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun $(which lmp_default) -in inputfile MEME Suite \u00a7 MEME Suite: Motif-based sequence analysis tools. This install is for the command-line tools and connects to their website for further analysis. module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.8.4/gnu-4.9.2 module load perl/5.22.0 module load python2/recommended module load ghostscript/9.16/gnu-4.9.2 module load meme/4.10.1_4 miRDeep2 \u00a7 Discovering known and novel miRNAs from deep sequencing data, miRDeep2 is a completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs. The tool reports known and hundreds of novel microRNAs with high accuracy in seven species representing the major animal clades. module load squid/1.9g module load randfold/2.0 module load perl/5.22.0 module load bowtie/1.1.2 module load python/2.7.9 module load viennarna/2.1.9 module load mirdeep/2.0.0.7 MISO/misopy \u00a7 MISO (Mixture of Isoforms) is a probabilistic framework that quantitates the expression level of alternatively spliced genes from RNA-Seq data, and identifies differentially regulated isoforms or exons across samples. misopy is available as part of the python2/recommended bundle. MISO can run multithreaded on one node, or can submit multiple independent single-core jobs at once using the --use-cluster option. If you want to use MISO's ability to create and submit jobs itself, you need a MISO settings file like the one shown below. You give your job options as arguments to the qsub command in the cluster_command line. Settings files can be used with the --settings-filename=SETTINGS_FILENAME option. You will also need to put your module unload and load commands in your .bashrc if using MISO's own job submission, because you are no longer including them in a jobscript. Example miso_settings.txt . Multithreaded jobs will use num_processors . num_processors is ignored if --use-cluster is specified: [data] filter_results = True min_event_reads = 20 [cluster] cluster_command = \"qsub -l h_rt=00:10:00 -l mem=1GB -wd ~/Scratch\" [sampler] burn_in = 500 lag = 10 num_iters = 5000 num_chains = 6 num_processors = 4 MOLPRO \u00a7 Molpro is a complete system of ab initio programs for molecular electronic structure calculations. Molpro 2015.1.3 was provided as binary only and supports communication over Ethernet and not Infiniband - use this one on single-node jobs primarily. Molpro 2015.1.5 was built from source with the Intel compilers and Intel MPI. module load molpro/2015.1.5/intel-2015-update2 # Example files available in /shared/ucl/apps/molpro/2015.1.5/intel-2015-update2/molprop_2015_1_linux_x86_64_i8/examples/ # You need to set the wavefunction directory to somewhere in Scratch with -W. # $SGE_O_WORKDIR is what your job specified with -wd. # $NSLOTS will use the number of cores you requested with -pe mpi. echo \"Running molpro -n $NSLOTS -W $SGE_O_WORKDIR h2o_scf.com\" molpro -n $NSLOTS -W $SGE_O_WORKDIR h2o_scf.com On Myriad, if you get this error, please use the binary 2015.1.3 install. libi40iw-i40iw_ucreate_qp: failed to create QP, unsupported QP type: 0x4 MRtrix \u00a7 MRtrix provides a set of tools to perform diffusion-weighted MRI white matter tractography in the presence of crossing fibres. module load python3/recommended module load qt/4.8.6/gnu-4.9.2 module load eigen/3.2.5/gnu-4.9.2 module load fftw/3.3.6-pl2/gnu-4.9.2 module load mrtrix/3.0rc3/gnu-4.9.2/nogui You must load these modules once from a login node before submitting a job. It copies a .mrtrix.conf to your home directory the first time you run this module from a login node, which sets: Analyse.LeftToRight: false NumberOfThreads: 4 You need to alter NumberOfThreads to what you are using in your job script before you submit a job. The MRtrix GUI tools are unavailable: mrview and shview in MRtrix 3 cannot be run over a remote X11 connection so are not usable on our clusters. To use these tools you will need a local install on your own computer. MuTect \u00a7 MuTect is a tool developed at the Broad Institute for the reliable and accurate identification of somatic point mutations in next generation sequencing data of cancer genomes. It is built on top of the GenomeAnalysisToolkit (GATK), which is also developed at the Broad Institute, so it uses the same command-line conventions and (almost all) the same input and output file formats. MuTect requires you to agree to the GATK license before we can add you to the lgmutect group which gives you access: you can do this by downloading MuTect from The Broad Institute CGA page . You may need to create a gatkforums account before you can download. MuTect is currently not compatible with Java 1.8, so you need to use the system Java 1.7. Set up your modules as follows: module load mutect/1.1.7 Then to run MuTect, you should either prefix the .jar you want to run with $MUTECTPATH : java -Xmx2g -jar $MUTECTPATH/mutect-1.1.7.jar OPTION1=value1 OPTION2=value2... Or we provide wrappers, so you can run it this way instead: mutect OPTION1=value1 OPTION2=value2... NONMEM \u00a7 NONMEM\u00ae is a nonlinear mixed effects modelling tool used in population pharmacokinetic / pharmacodynamic analysis. We have one build that uses the GNU compiler and ATLAS and an Intel build using MKL. Both use Intel MPI. This example uses the Intel build. jobDir=example1_parallel_$JOB_ID mkdir $jobDir # Copy control and datafiles to jobDir cp /shared/ucl/apps/NONMEM/examples/foce_parallel.ctl $jobDir cp /shared/ucl/apps/NONMEM/examples/example1b.csv $jobDir cd $jobDir module unload compilers mpi module load compilers/intel/2015/update2 module load mpi/intel/2015/update3/intel module load nonmem/7.3.0/intel-2015-update2 # Create parafile for job using $TMPDIR/machines parafile.sh $TMPDIR/machines > example1.pnm nmfe73 foce_parallel.ctl example1.res -parafile=example1.pnm -background -maxlim=1 > example1.log NWChem \u00a7 NWChem applies theoretical techniques to predict the structure, properties, and reactivity of chemical and biological species ranging in size from tens to millions of atoms. You should load the NWChem module you wish to use once from a login node, as it will create a symlinked .nwchemrc in your home. module load python/2.7.12 module load nwchem/6.8-47-gdf6c956/intel-2017 # $NSLOTS will get the number of processes you asked for with -pe mpi. mpirun -np $NSLOTS -machinefile $TMPDIR/machines nwchem hpcvl_sample.nw If your run terminates with an error saying ARMCI supports block process mapping only then you are probably trying to use round-robin MPI process placement, which ARMCI does not like. gerun uses round-robin for Intel MPI by default as it works better in most cases. Use mpirun instead of gerun : mpirun -np $NSLOTS -machinefile $TMPDIR/machines nwchem input.nw If you get an error complaining about $NWCHEM_NWPW_LIBRARY similar to this: warning:::::::::::::: from_compile NWCHEM_NWPW_LIBRARY is: < /dev/shm/tmp.VB3DpmjULc/nwchem-6.6/src/nwpw/libraryps/> but file does not exist or you do not have access to it ! ------------------------------------------------------------------------ nwpwlibfile: no nwpw library found 0 then your ~/.nwchemrc symlink is likely pointing to a different version that you used previously. Deleting the symlink and loading the module you want to use will recreate it correctly. Picard \u00a7 Picard comprises Java-based command-line utilities that manipulate SAM files, and a Java API (SAM-JDK) for creating new programs that read and write SAM files. Both SAM text format and SAM binary (BAM) format are supported. Picard requires a Java 1.8 module to be loaded. module load java/1.8.0_92 module load picard-tools/2.18.9 To run Picard you can prefix the .jar you want to run with $PICARDPATH and give the full command, or we have wrappers: java -Xmx2g -jar $PICARDPATH/picard.jar PicardCommand TMP_DIR=$TMPDIR OPTION1=value1 OPTION2=value2... The wrappers allow you to run commands like this - in this case our wrapper sets TMP_DIR for you as well: PicardCommand OPTION1=value1 OPTION2=value2... Temporary files: by default, Picard writes temporary files into /tmp rather than into $TMPDIR . These are not cleaned up after your job ends, and means future runs can fail because /tmp is full (and requesting more tmpfs in your job doesn't make it larger). If you run Picard with the full java -jar command then give Picard the TMP_DIR=$TMPDIR option as our example above does to get it to write in the correct place. Quantum Espresso \u00a7 Quantum Espresso is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modelling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. module load xorg-utils module load quantum-espresso/6.1-impi/intel2017 # Set the path here to where ever you keep your pseudopotentials. export ESPRESSO_PSEUDO=$HOME/qe-psp # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun pw.x -in input.in >output.out Repast HPC \u00a7 Repast for High Performance Computing (Repast HPC) is a next generation agent-based modelling system intended for large-scale distributed computing platforms. It implements the core Repast Simphony concepts (e.g. contexts and projections), modifying them to work in a parallel distributed environment. module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load hdf/5-1.8.15/gnu-4.9.2 module load netcdf/4.3.3.1/gnu-4.9.2 module load netcdf-fortran/4.4.1/gnu-4.9.2 module load mpi/openmpi/1.8.4/gnu-4.9.2 module load python/2.7.9 module load boost/1_54_0/mpi/gnu-4.9.2 module load netcdf-c++/4.2/gnu-4.9.2 module load repast-hpc/2.1/gnu-4.9.2 The module sets the environment variables $REPAST_HPC_INCLUDE , $REPAST_HPC_LIB_DIR and $REPAST_HPC_LIB . ROOT \u00a7 ROOT provides a set of OO frameworks for handling, analysing, and visualising large amounts of data. Included are specialised storage methods, methods for histograming, curve fitting, function evaluation, minimization etc. ROOT includes a built-in CINT C++ interpreter. module unload compilers mpi module load compilers/gnu/4.9.2 module load fftw/3.3.4/gnu-4.9.2 module load gsl/1.16/gnu-4.9.2 module load root/6.04.00/gnu-4.9.2 # run root in batch mode root -b -q myMacro.C > myMacro.out SAS \u00a7 SAS is a statistics package providing a wide range of tools for data management, analysis and presentation. cd $TMPDIR module load sas/9.4/64 # copy all your input files into $TMPDIR cp ~/Scratch/sas_input/example1/* $TMPDIR sas example1.in # tar up all contents of $TMPDIR back into your space tar cvzf $HOME/Scratch/SAS_output/files_from_job_$JOB_ID.tgz $TMPDIR StarCCM+ \u00a7 StarCCM+ is a commercial CFD package that handles fluid flows, heat transfer, stress simulations, and other common applications of such. Before running any StarCCM+ jobs on the clusters you must load the StarCCM+ module on a login node. This is so the module can set up two symbolic links in your home directory to directories created in your Scratch area so that user settings etc can be written by running jobs. module load star-ccm+/13.06.012 Here is the jobscript example. # Request one license per core - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l ccmpsuite=1 module load star-ccm+/13.06.012 starccm+ -np $NSLOTS -machinefile $TMPDIR/machines -rsh ssh -batch my_input.sim StarCD \u00a7 StarCD is a commercial package for modelling and simulating combustion and engine dynamics. You must request access to the group controlling StarCD access (legstarc) to use it. The license is owned by the Department of Mechanical Engineering who will need to approve your access request. # Request one license per core - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l starsuite=1 module load star-cd/4.28.050 # run star without its tracker process as this causes multicore jobs to die early star -notracker StarCD uses IBM Platform MPI by default. You can also run StarCD simulations using Intel MPI by changing the command line to: star -notracker -mpi=intel Simulations run using Intel MPI may run faster than they do when using IBM Platform MPI. Stata/MP \u00a7 Stata is a statistics, data management, and graphics system. Stata/MP is the version of the package that runs on multiple cores. We have a sixteen user license of Stata/MP. Our license supports Stata running on up to four cores per job. # Select 4 OpenMP threads (the most possible) #$ -pe smp 4 cd $TMPDIR module load stata/15 # copy files to $TMPDIR cp myfile.do $TMPDIR stata-mp -b do myfile.do # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Stata_output/files_from_job_$JOB_ID.tar.gz $TMPDIR Torch \u00a7 Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first. We provide a torch-deps module that contains the main Torch dependencies and creates a quick-install alias, do-torch-install . This uses Torch's installation script to git clone the current distribution and install LuaJIT, LuaRocks and Torch in ~/torch . module unload compilers mpi module load torch-deps do-torch-install You should load these same modules in your jobscript when using the version of torch this installs. Turbomole \u00a7 Turbomole is an ab initio computational chemistry program that implements various quantum chemistry methods. Turbomole has a Chemistry-wide license. Reserved application group legtmole for Chemistry users only. There are scripts you can use to generate Turbomole jobs for you: /shared/ucl/apps/turbomole/turbomole-mpi.submit /shared/ucl/apps/turbomole/turbomole-smp.submit They will ask you which version you want to use, how much memory, how many cores etc and set up and submit the job for you. Use the first for MPI jobs and the second for single-node shared memory threaded jobs. VarScan \u00a7 VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. module load java/1.8.0_45 module load varscan/2.3.9 Then to run VarScan, you should either prefix the .jar you want to run with $VARSCANPATH: java -Xmx2g -jar $VARSCANPATH/VarScan.v2.3.9.jar OPTION1=value1 OPTION2=value2... Or we provide wrappers, so you can run it this way instead: varscan OPTION1=value1 OPTION2=value2... VASP \u00a7 The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP is licensed software. To gain access, you need to email us the name and email of the main VASP license holder (plus the license number if you have it). We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group, and remove you when VASP tell us you no longer have access. The VASP executables for current versions are named like this: * vasp_gam - optimised for gamma point calculations only * vasp_std - standard version * vasp_ncl - for non-collinear magnetic structure and/or spin-orbit coupling calculations module unload compilers mpi module load compilers/intel/2017/update1 module load mpi/intel/2017/update1/intel # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun vasp_std > vasp_output.$JOB_ID Note: although you can run VASP using the default Intel 2018 compiler this can lead to numerical errors in some types of simulation. In those cases we recommend switching to the specific compiler and MPI version used to build that install (mentioned at the end of the module name). We do this in the example above. Building your own VASP: You may also install your own copy of VASP in your home if you have access to the source, and we provide a simple VASP individual install script (tested with VASP 5.4.4, no patches). You need to download the VASP source code into your home directory and then you can run the script following the instructions at the top. XMDS \u00a7 XMDS allows the fast and easy solution of sets of ordinary, partial and stochastic differential equations, using a variety of efficient numerical algorithms. You will need to load the modules on a login node and run xmds2-setup to set up XMDS. module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/intel/2015/update3/gnu-4.9.2 module load python2/recommended module load fftw/3.3.4-impi/gnu-4.9.2 module load hdf/5-1.8.15/gnu-4.9.2 module load xmds/2.2.2 # run this on a login node to set up XMDS xmds2-setup You can also build the current developmental version from SVN in your space by running create-svn-xmds-inst . Note: the SVN version on 28 Oct 2015 was failing unit test test_nonlocal_access_multiple_components .","title":"Other Software"},{"location":"Software_Guides/Other_Software/#general-use-of-environment-modules","text":"We have a default set of modules that everyone has loaded when they log in: these include the current default compiler and MPI, some utili ties to make your life easier and some text editors.","title":"General use of environment modules"},{"location":"Software_Guides/Other_Software/#summary-of-module-commands","text":"module avail # shows available modules module whatis # shows available modules with brief explanations module list # shows your currently loaded modules module load <module> # load this module module unload <module> # unload this module module purge # unload all modules module show <module> # Shows what the module requires and what it sets up module help <module> # Shows a longer text description for the software","title":"Summary of module commands"},{"location":"Software_Guides/Other_Software/#find-out-if-a-software-package-is-installed-and-load-it","text":"Generically, the way you find out if a piece of software is installed is to run module avail packagename This gives you a list of all the modules we have that match the name you searched for. You can then type module show packagename and it will show you the other software dependencies this module has: these have to be loaded first. It also shows where the software is installed and what environment variables it sets up. Once you have found the modules you want to load, it is good practice to refer to them using their full name, including the version. If you use the short form ( package rather than package/5.1.2/gnu-4.9.2 ) then a matching module will be loaded, but if we install a different version, your jobs may begin using the new one and you would not know which version created your results. Different software versions may not be compatible or may have different default settings, so this is undesirable. You may need to unload current modules in order to load some requirements (eg different compiler, different MPI). This example switches from Intel compiler and MPI modules to GNU ones. module unload compilers mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/3.1.4/gnu-4.9.2 You can use the short name when unloading things because there is usually only one match in your current modules. The last part of a module name usually tells you what compiler it was built with. There may be a GNU compiler version and an Intel compiler version of the same software available. Once the module is loaded, you should have all the usual executables in your path, and can use its commands. You load modules in exactly the same way inside a jobscript.","title":"Find out if a software package is installed and load it"},{"location":"Software_Guides/Other_Software/#notes-on-how-to-run-specific-packages","text":"The packages below have slightly complex commands needed to run them, or different settings needed on our clusters. These are examples of what should be added to your jobscripts. Change the module load command to the version you want to load and check that the dependencies are the same. The top of a jobscript should contain your resource requests . See also examples of full jobscripts .","title":"Notes on how to run specific packages"},{"location":"Software_Guides/Other_Software/#abaqus","text":"ABAQUS is a commercial software suite for finite element analysis and computer-aided engineering. You must be authorised by the Mech Eng Department before you can be added to the group controlling access to ABAQUS (legabq). A serial interactive analysis can be run on the compute nodes (via a qrsh session) like this: abaqus interactive job=myJobSerial input=myInputFile.inp A parallel job can be run like this (fill in your own username): module load abaqus/2017 INPUT_FILE=/home/<username>/ABAQUS/heattransfermanifold_cavity_parallel.inp ABAQUS_ARGS= ABAQUS_PARALLELSCRATCH=/home/<username>/Scratch/Abaqus/parallelscratch # creates a parallel scratch dir and a new working dir for this job mkdir -p $ABAQUS_PARALLELSCRATCH mkdir -p $JOB_NAME.$JOB_ID cd $JOB_NAME.$JOB_ID cp $INPUT_FILE . INPUT=$(basename $INPUT_FILE) abaqus interactive cpus=$NSLOTS mp_mode=mpi job=$INPUT.$JOB_ID input=$INPUT \\ scratch=$ABAQUS_PARALLELSCRATCH $ABAQUS_ARGS","title":"ABAQUS"},{"location":"Software_Guides/Other_Software/#beast","text":"BEAST is an application for Bayesian MCMC analysis of molecular sequences orientated towards rooted, time-measured phylogenies inferred using strict or relaxed molecular clock models. Note that FigTree and Tracer are available as standalone modules. The addons DISSECT, MODEL_SELECTION, and SNAPP are installed for BEAST. cd $TMPDIR module load java/1.8.0_45 module load beast/2.3.0 beast -threads $OMP_NUM_THREADS ~/Scratch/BEAST/gopher.xml # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/BEAST/files_from_job_$JOB_ID.tar.gz $TMPDIR","title":"BEAST"},{"location":"Software_Guides/Other_Software/#bowtie","text":"Bowtie 1 and 2 are tools for aligning sequencing reads to their reference sequences. Bowtie 1 and 2 are available. For reads longer than about 50 bp Bowtie 2 is generally faster, more sensitive, and uses less memory than Bowtie 1. For relatively short reads (e.g. less than 50 bp) Bowtie 1 is sometimes faster and/or more sensitive. For further differences, see How is Bowtie 2 different from Bowtie 1? . Bowtie sets $BT1_HOME and Bowtie2 sets $BT2_HOME . You can have both modules loaded at once. cd $TMPDIR module load bowtie2/2.2.5 # Run Bowtie2 example from getting started guide: # http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#getting-started-with-bowtie-2-lambda-phage-example bowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus bowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Bowtie2_output/files_from_job_$JOB_ID.tgz $TMPDIR","title":"Bowtie"},{"location":"Software_Guides/Other_Software/#castep","text":"CASTEP is a full-featured materials modelling code based on a first-principles quantum mechanical description of electrons and nuclei. module load castep/17.21/intel-2017 # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun castep.mpi input If you have access to the source code and wish to build your own copy, it has been suggested that compiling with these options (on Grace) gave a build that ran about 10% faster than the default compilation options: make COMMS_ARCH=mpi SUBARCH=mpi FFT=mkl MATHLIBS=mkl10 BUILD=fast Do check for numerical accuracy with any optimisations you carry out.","title":"CASTEP"},{"location":"Software_Guides/Other_Software/#cctools","text":"Provides the Parrot connector to CVMFS, the CernVM File System. By default, the cctools module sets the following: export PARROT_CVMFS_REPO=<default-repositories> export PARROT_ALLOW_SWITCHING_CVMFS_REPOSITORIES=yes export HTTP_PROXY=DIRECT; export PARROT_HTTP_PROXY=DIRECT; Example usage - will list the contents of the repository then exit: module load cctools/7.0.11/gnu-4.9.2 parrot_run bash ls /cvmfs/alice.cern.ch exit That will create the cache in /tmp/parrot.xxxxx on the login nodes when run interactively. To use in a job, you will want to put the cache somewhere in your space that the compute nodes can access. You can set the cache to be in your Scratch, or to $TMPDIR on the nodes if it just needs to exist for the duration of that job. export PARROT_CVMFS_ALIEN_CACHE=</path/to/cache>","title":"Cctools"},{"location":"Software_Guides/Other_Software/#cfd-ace","text":"CFD-ACE+ is a commercial computational fluid dynamics solver developed by ESI Group. It solves the conservation equations of mass, momentum, energy, chemical species and other scalar transport equations using the finite volume method. These equations enable coupled simulations of fluid, thermal, chemical, biological, electrical and mechanical phenomena. The license is owned by the Department of Mechanical Engineering who must give permission for users to be added to the group lgcfdace . module load cfd-ace/2018.0 CFD-SOLVER -model 3Dstepchannel_060414.DTF -num $NSLOTS -wd `pwd` \\ -hosts $TMPDIR/machines -rsh=ssh -decomp -metis -sim 1 -platformmpi -job","title":"CFD-ACE"},{"location":"Software_Guides/Other_Software/#comsol","text":"COMSOL Multiphysics is a cross-platform finite element analysis, solver and multiphysics simulation software. Electrical Engineering have a group license for version 52 and must give permission for users to be added to the group legcomsl . Chemical Engineering have a Departmental License for version 53 and members of that department may be added to the group lgcomsol . # Run a parallel COMSOL job # Versions 52 and 52a have this additional module prerequisite module load xulrunner/3.6.28/gnu-4.9.2 # pick the version to load module load comsol/53a # Parallel multinode options: # $NHOSTS gets the number of nodes the job is running on and # $TMPDIR/machines is the machinefile that tells it which nodes. # These are automatically set up in a \"-pe mpi\" job environment. comsol -nn $NHOSTS -clustersimple batch -f $TMPDIR/machines -inputfile micromixer_batch.mph \\ -outputfile micromixer_batch_output_${JOB_ID}.mph # On Myriad you need to specify the fabric: comsol batch -f $TMPDIR/machines -np $NSLOTS -mpifabrics shm:tcp \\ -inputfile micromixer_batch.mph -outputfile micromixer_batch_output_${JOB_ID}.mph","title":"COMSOL"},{"location":"Software_Guides/Other_Software/#cp2k","text":"CP2K performs atomistic and molecular simulations. module unload compilers mpi module load mpi/openmpi/3.0.0/gnu-4.9.2 module load cp2k/5.1/ompi/gnu-4.9.2 # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun cp2k.popt < input.in > output.out For CP2K 4.1 there is also a Chemistry department version with submission script generator. To access it: module load chemistry-modules module load submission-scripts The command submitters will then list the submitters available. You can then run cp2k.submit which will ask you questions in order to create a suitable jobscript. The cp2k.submit submitter takes up to 6 arguments, and any omitted will be asked for interactively: cp2k.submit \u00abinput_file\u00bb \u00abcores\u00bb \u00abversion\u00bb \u00abmaximum_run_time\u00bb \u00abmemory_per_core\u00bb \u00abjob_name\u00bb So, for example: cp2k.submit water.inp 8 4.1 2:00:00 4G mywatermolecule would request a job running CP2K 4.1 with the input file water.inp , on 8 cores, with a maximum runtime of 2 hours, with 4 gigabytes of memory per core, and a job name of mywatermolecule .","title":"CP2K"},{"location":"Software_Guides/Other_Software/#crystal","text":"CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. CRYSTAL is commercial software which is available free of charge to UK academics. You must obtain a license from Crystal Solutions: How to get CRYSTAL - Academic UK license . You need to create an account and then request to be upgraded to Academic UK. Access to CRYSTAL is enabled by being a member of the reserved application group legcryst . For proof of access we accept emails from CRYSTAL saying your account has been upgraded to \"Academic UK\", or a screenshot of your account page showing you have the full download available rather than just the demo version. module unload mpi module load mpi/openmpi/2.1.2/intel-2017 module load crystal17/v1.0.1 # 9. Create a directory for this job and copy the input file into it. mkdir test00 cd test00 cp ~/Scratch/Crystal17/test_cases/inputs/test00.d12 INPUT # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. # The CRYSTAL module sets $CRYxx_EXEDIR and $VERSION environment variables. gerun $CRY17_EXEDIR/$VERSION/Pcrystal # Similarly, for Pproperties the command would be gerun $CRY17_EXEDIR/$VERSION/Pproperties","title":"CRYSTAL"},{"location":"Software_Guides/Other_Software/#freesurfer","text":"FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data. Freesurfer can use threads to run on multiple cores in one node: request the number with -pe smp in the resource-request part of your jobscript. #$ -pe smp 4 module load xorg-utils/X11R7.7 module load freesurfer/6.0.0 export SUBJECTS_DIR=~/Scratch/FreeSurfer_examples/subjects # -openmp $NSLOTS runs with the number of threads you requested recon-all -openmp $NSLOTS -i sample-001.nii.gz -s bert -all","title":"FreeSurfer"},{"location":"Software_Guides/Other_Software/#gamess","text":"The General Atomic and Molecular Electronic Structure System (GAMESS) is a general ab initio quantum chemistry package. The GAMESS module should be loaded once from a login node before submitting a job - this creates the ~/Scratch/gamess directory for you which is used as USERSCR to write some scratch files during the job. If you don't want to keep these files and would prefer them to be written to $TMPDIR instead, you can put export $GAMESS_USERSCR=$TMPDIR in your jobscript after the module load command. module unload compilers mpi module load compilers/intel/2015/update2 module load mpi/intel/2015/update3/intel module load gamess/5Dec2014_R1/intel-2015-update2 # Optional: set where the USERSCR files go. # By default, the module sets it to ~/Scratch/gamess export $GAMESS_USERSCR=$TMPDIR rungms exam01.inp 00 $NSLOTS $(ppn)","title":"GAMESS"},{"location":"Software_Guides/Other_Software/#gatk","text":"The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Version 4 of GATK is BSD-licensed so does not require a group to control access to the software. Version 3 of GATK requires you to agree to the GATK license before we can add you to the leggatk group which gives you access: you can do this by downloading GATK 3 from The Broad Institute GATK download page , reading the license, and telling us you agree to it. You may need to create a gatkforums account before you can download. GATK 3 uses Java 1.7 (the system Java) so you do not need to load a Java module. GATK 4 uses 1.8 so you need to load java/1.8.0_92 first. Load the version you want, then to run GATK you should either prefix the .jar you want to run with $GATKPATH : java -Xmx2g -jar $GATKPATH/GenomeAnalysisTK.jar OPTION1=value1 OPTION2=value2... Or we provide wrappers, so you can run it one of these ways instead: GenomeAnalysisTK OPTION1=value1 OPTION2=value2... gatk OPTION1=value1 OPTION2=value2...","title":"GATK"},{"location":"Software_Guides/Other_Software/#gromacs","text":"We have many versions of GROMACS installed, some built with Plumed. The module name will indicate this. Which executable you should run depends on the problem you wish to solve. For both single and double precision version builds, serial binaries and an MPI binary for mdrun ( mdrun_mpi for newer versions, gmx_mpi for Plumed and some older versions) are provided. Double precision binaries have a _d suffix (so gmx_d , mdrun_mpi_d , gmx_mpi_d etc). # Example for gromacs/2019.3/intel-2018 module unload compilers mpi module load compilers/intel/2018/update3 module load mpi/intel/2018/update3/intel module load gromacs/2019.3/intel-2018 # Run GROMACS - replace with mdrun command line suitable for your job! gerun mdrun_mpi -v -stepout 10000 # Plumed example for gromacs/2019.3/plumed/intel-2018 module unload compilers mpi module load compilers/intel/2018/update3 module load mpi/intel/2018/update3/intel module load libmatheval module load flex module load plumed/2.5.2/intel-2018 module load gromacs/2019.3/plumed/intel-2018 # Run GROMACS - replace with mdrun command line suitable for your job! gerun gmx_mpi -v -stepout 10000","title":"GROMACS"},{"location":"Software_Guides/Other_Software/#passing-in-options-to-gromacs-non-interactively","text":"Some GROMACS executables like trjconv normally take interactive input. You can't do this in a jobscript, so you need to pass in the input you would normally type in. There are several ways of doing this, mentioned at GROMACS Documentation - Using Commands in Scripts . The simplest is to echo the input in and keep your gmx options as they would normally be. If the inputs you would normally type were 3 and 3, then you can do this: echo 3 3 | gmx whatevercommand -options","title":"Passing in options to GROMACS non-interactively"},{"location":"Software_Guides/Other_Software/#checkpoint-and-restart","text":"GROMACS has built-in checkpoint and restart ability, so you can use this if your runs will not complete in the maximum 48hr wallclock time. Have a look at the GROMACS manual for full details, as there are more options than mentioned here. You can tell GROMACS to write a checkpoint file when it is approaching the maximum wallclock time available, and then exit. In this case, we had asked for 48hrs wallclock. This tells GROMACS to start from the last checkpoint if there is one, and write a new checkpoint just before it reaches 47 hrs runtime. gerun mdrun_mpi -cpi -maxh 47 <options> The next job you submit with the same script will carry on from the checkpoint the last job wrote. You could use job dependencies to submit two identical jobs at the same time and have one dependent on the other, so it won't start until the first finishes - have a look at man qsub for the -hold_jid option. You can also write checkpoints at given intervals: # Write checkpoints every 120 mins, start from checkpoint if there is one. gerun mdrun_mpi -cpi -cpt 120 <options>","title":"Checkpoint and restart"},{"location":"Software_Guides/Other_Software/#hammock","text":"Hammock is a tool for peptide sequence clustering. It is able to cluster extremely large amounts of short peptide sequences into groups sharing sequence motifs. Typical Hammock applications are NGS-based experiments using large combinatorial peptide libraries, e.g. Phage display. Hammock has to be installed in your own space to function, so we provide a hammock module that contains the main dependencies and creates a quick-install alias: # on the login nodes module unload compilers module load hammock/1.0.5 do-hammock-install This will install Hammock 1.0.5 in your home, edit settings.prop to use clustal-omega and hmmer from our modules and tell it to write temporary files in your Scratch directory (in the form Hammock_temp_time ). # in your jobscript module unload compilers module load hammock/1.0.5 # This copies the MUSI example that comes with Hammock into your working # directory and runs it. The module sets $HAMMOCKPATH for you. # You must set the output directory to somewhere in Scratch with -d. # Below makes a different outputdir per job so multiple runs don't overwrite files. cp $HAMMOCKPATH/../examples/MUSI/musi.fa . outputdir=~/Scratch/hammock-examples/musi_$JOB_ID mkdir -p $outputdir echo \"Running java -jar $HAMMOCKPATH/Hammock.jar full -i musi.fa -d $outputdir\" java -jar $HAMMOCKPATH/Hammock.jar full -i musi.fa -d $outputdir","title":"Hammock"},{"location":"Software_Guides/Other_Software/#hopspack","text":"HOPSPACK (Hybrid Optimization Parallel Search PACKage) solves derivative-free optimization problems using an open source, C++ software framework. We have versions of HOPSPACK built using the GNU compiler and OpenMPI, and the Intel compiler and MPI. This example shows the GNU version. Serial and parallel versions are available, HOPSPACK_main_mpi and HOPSPACK_main_serial . module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.8.4/gnu-4.9.2 module load atlas/3.10.2/gnu-4.9.2 module load hopspack/2.0.2/gnu-4.9.2 # Add the examples directory we are using to our path. # Replace this with the path to your own executables. export PATH=$PATH:~/Scratch/examples/1-var-bnds-only/ # Run parallel HOPSPACK. # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun HOPSPACK_main_mpi ~/Scratch/examples/1-var-bnds-only/example1_params.txt > example1_output.txt","title":"HOPSPACK"},{"location":"Software_Guides/Other_Software/#idl","text":"IDL is a complete environment and language for the analysis and visualisation of scientific and other technical data. It can be used for everything from quick interactive data exploration to building complex applications. Single-threaded jobscript: cd $TMPDIR module load idl/8.4.1 # Copy IDL source files to $TMPDIR cp ~/Scratch/IDL/fib.pro $TMPDIR cp ~/Scratch/IDL/run1.pro $TMPDIR idl -queue -e @run1.pro # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/IDL_output/files_from_job_$JOB_ID.tgz $TMPDIR Parallel jobscript: cd $TMPDIR module load idl/8.1 # this sets the IDL thread pool: do not change this export IDL_CPU_TPOOL_NTHREADS=$OMP_NUM_THREADS # Copy IDL source files to $TMPDIR cp ~/Scratch/IDL/fib.pro $TMPDIR cp ~/Scratch/IDL/run2mp.pro $TMPDIR idl -queue -e @run2mp.pro # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/IDL_output/files_from_job_$JOB_ID.tgz $TMPDIR","title":"IDL"},{"location":"Software_Guides/Other_Software/#jags","text":"JAGS (Just Another Gibbs Sampler) is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation not wholly unlike BUGS. Use this to use JAGS in standalone command line mode: module unload compilers mpi module load compilers/gnu/4.9.2 module load openblas/0.2.14/gnu-4.9.2 module load jags/4.2.0/gnu.4.9.2-openblas We have also added JAGS support to r/recommended using the rjags and R2jags R packages.","title":"JAGS"},{"location":"Software_Guides/Other_Software/#lammps","text":"LAMMPS is an open source parallel molecular dynamics code which exhibits good scaling in a wide range of environments. The LAMMPS binaries are called lmp_$cluster and all have an lmp_default symlink which can be used. LAMMPS-8Dec15 and later were built with additional packages kspace , replica , rigid , and class2 . The versions from lammps-16Mar18-basic_install onwards (not lammps/16Mar18/intel-2017 ) have most of the included packages built. There are also userintel and gpu versions from this point. We do not install the LAMMPS user packages as part of our central install, but you can build your own version with the ones that you want in your space. module unload compilers mpi module load compilers/intel/2018 module load mpi/intel/2018 module load lammps/16Mar18/basic/intel-2018 # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun $(which lmp_default) -in inputfile","title":"LAMMPS"},{"location":"Software_Guides/Other_Software/#meme-suite","text":"MEME Suite: Motif-based sequence analysis tools. This install is for the command-line tools and connects to their website for further analysis. module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.8.4/gnu-4.9.2 module load perl/5.22.0 module load python2/recommended module load ghostscript/9.16/gnu-4.9.2 module load meme/4.10.1_4","title":"MEME Suite"},{"location":"Software_Guides/Other_Software/#mirdeep2","text":"Discovering known and novel miRNAs from deep sequencing data, miRDeep2 is a completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs. The tool reports known and hundreds of novel microRNAs with high accuracy in seven species representing the major animal clades. module load squid/1.9g module load randfold/2.0 module load perl/5.22.0 module load bowtie/1.1.2 module load python/2.7.9 module load viennarna/2.1.9 module load mirdeep/2.0.0.7","title":"miRDeep2"},{"location":"Software_Guides/Other_Software/#misomisopy","text":"MISO (Mixture of Isoforms) is a probabilistic framework that quantitates the expression level of alternatively spliced genes from RNA-Seq data, and identifies differentially regulated isoforms or exons across samples. misopy is available as part of the python2/recommended bundle. MISO can run multithreaded on one node, or can submit multiple independent single-core jobs at once using the --use-cluster option. If you want to use MISO's ability to create and submit jobs itself, you need a MISO settings file like the one shown below. You give your job options as arguments to the qsub command in the cluster_command line. Settings files can be used with the --settings-filename=SETTINGS_FILENAME option. You will also need to put your module unload and load commands in your .bashrc if using MISO's own job submission, because you are no longer including them in a jobscript. Example miso_settings.txt . Multithreaded jobs will use num_processors . num_processors is ignored if --use-cluster is specified: [data] filter_results = True min_event_reads = 20 [cluster] cluster_command = \"qsub -l h_rt=00:10:00 -l mem=1GB -wd ~/Scratch\" [sampler] burn_in = 500 lag = 10 num_iters = 5000 num_chains = 6 num_processors = 4","title":"MISO/misopy"},{"location":"Software_Guides/Other_Software/#molpro","text":"Molpro is a complete system of ab initio programs for molecular electronic structure calculations. Molpro 2015.1.3 was provided as binary only and supports communication over Ethernet and not Infiniband - use this one on single-node jobs primarily. Molpro 2015.1.5 was built from source with the Intel compilers and Intel MPI. module load molpro/2015.1.5/intel-2015-update2 # Example files available in /shared/ucl/apps/molpro/2015.1.5/intel-2015-update2/molprop_2015_1_linux_x86_64_i8/examples/ # You need to set the wavefunction directory to somewhere in Scratch with -W. # $SGE_O_WORKDIR is what your job specified with -wd. # $NSLOTS will use the number of cores you requested with -pe mpi. echo \"Running molpro -n $NSLOTS -W $SGE_O_WORKDIR h2o_scf.com\" molpro -n $NSLOTS -W $SGE_O_WORKDIR h2o_scf.com On Myriad, if you get this error, please use the binary 2015.1.3 install. libi40iw-i40iw_ucreate_qp: failed to create QP, unsupported QP type: 0x4","title":"MOLPRO"},{"location":"Software_Guides/Other_Software/#mrtrix","text":"MRtrix provides a set of tools to perform diffusion-weighted MRI white matter tractography in the presence of crossing fibres. module load python3/recommended module load qt/4.8.6/gnu-4.9.2 module load eigen/3.2.5/gnu-4.9.2 module load fftw/3.3.6-pl2/gnu-4.9.2 module load mrtrix/3.0rc3/gnu-4.9.2/nogui You must load these modules once from a login node before submitting a job. It copies a .mrtrix.conf to your home directory the first time you run this module from a login node, which sets: Analyse.LeftToRight: false NumberOfThreads: 4 You need to alter NumberOfThreads to what you are using in your job script before you submit a job. The MRtrix GUI tools are unavailable: mrview and shview in MRtrix 3 cannot be run over a remote X11 connection so are not usable on our clusters. To use these tools you will need a local install on your own computer.","title":"MRtrix"},{"location":"Software_Guides/Other_Software/#mutect","text":"MuTect is a tool developed at the Broad Institute for the reliable and accurate identification of somatic point mutations in next generation sequencing data of cancer genomes. It is built on top of the GenomeAnalysisToolkit (GATK), which is also developed at the Broad Institute, so it uses the same command-line conventions and (almost all) the same input and output file formats. MuTect requires you to agree to the GATK license before we can add you to the lgmutect group which gives you access: you can do this by downloading MuTect from The Broad Institute CGA page . You may need to create a gatkforums account before you can download. MuTect is currently not compatible with Java 1.8, so you need to use the system Java 1.7. Set up your modules as follows: module load mutect/1.1.7 Then to run MuTect, you should either prefix the .jar you want to run with $MUTECTPATH : java -Xmx2g -jar $MUTECTPATH/mutect-1.1.7.jar OPTION1=value1 OPTION2=value2... Or we provide wrappers, so you can run it this way instead: mutect OPTION1=value1 OPTION2=value2...","title":"MuTect"},{"location":"Software_Guides/Other_Software/#nonmem","text":"NONMEM\u00ae is a nonlinear mixed effects modelling tool used in population pharmacokinetic / pharmacodynamic analysis. We have one build that uses the GNU compiler and ATLAS and an Intel build using MKL. Both use Intel MPI. This example uses the Intel build. jobDir=example1_parallel_$JOB_ID mkdir $jobDir # Copy control and datafiles to jobDir cp /shared/ucl/apps/NONMEM/examples/foce_parallel.ctl $jobDir cp /shared/ucl/apps/NONMEM/examples/example1b.csv $jobDir cd $jobDir module unload compilers mpi module load compilers/intel/2015/update2 module load mpi/intel/2015/update3/intel module load nonmem/7.3.0/intel-2015-update2 # Create parafile for job using $TMPDIR/machines parafile.sh $TMPDIR/machines > example1.pnm nmfe73 foce_parallel.ctl example1.res -parafile=example1.pnm -background -maxlim=1 > example1.log","title":"NONMEM"},{"location":"Software_Guides/Other_Software/#nwchem","text":"NWChem applies theoretical techniques to predict the structure, properties, and reactivity of chemical and biological species ranging in size from tens to millions of atoms. You should load the NWChem module you wish to use once from a login node, as it will create a symlinked .nwchemrc in your home. module load python/2.7.12 module load nwchem/6.8-47-gdf6c956/intel-2017 # $NSLOTS will get the number of processes you asked for with -pe mpi. mpirun -np $NSLOTS -machinefile $TMPDIR/machines nwchem hpcvl_sample.nw If your run terminates with an error saying ARMCI supports block process mapping only then you are probably trying to use round-robin MPI process placement, which ARMCI does not like. gerun uses round-robin for Intel MPI by default as it works better in most cases. Use mpirun instead of gerun : mpirun -np $NSLOTS -machinefile $TMPDIR/machines nwchem input.nw If you get an error complaining about $NWCHEM_NWPW_LIBRARY similar to this: warning:::::::::::::: from_compile NWCHEM_NWPW_LIBRARY is: < /dev/shm/tmp.VB3DpmjULc/nwchem-6.6/src/nwpw/libraryps/> but file does not exist or you do not have access to it ! ------------------------------------------------------------------------ nwpwlibfile: no nwpw library found 0 then your ~/.nwchemrc symlink is likely pointing to a different version that you used previously. Deleting the symlink and loading the module you want to use will recreate it correctly.","title":"NWChem"},{"location":"Software_Guides/Other_Software/#picard","text":"Picard comprises Java-based command-line utilities that manipulate SAM files, and a Java API (SAM-JDK) for creating new programs that read and write SAM files. Both SAM text format and SAM binary (BAM) format are supported. Picard requires a Java 1.8 module to be loaded. module load java/1.8.0_92 module load picard-tools/2.18.9 To run Picard you can prefix the .jar you want to run with $PICARDPATH and give the full command, or we have wrappers: java -Xmx2g -jar $PICARDPATH/picard.jar PicardCommand TMP_DIR=$TMPDIR OPTION1=value1 OPTION2=value2... The wrappers allow you to run commands like this - in this case our wrapper sets TMP_DIR for you as well: PicardCommand OPTION1=value1 OPTION2=value2... Temporary files: by default, Picard writes temporary files into /tmp rather than into $TMPDIR . These are not cleaned up after your job ends, and means future runs can fail because /tmp is full (and requesting more tmpfs in your job doesn't make it larger). If you run Picard with the full java -jar command then give Picard the TMP_DIR=$TMPDIR option as our example above does to get it to write in the correct place.","title":"Picard"},{"location":"Software_Guides/Other_Software/#quantum-espresso","text":"Quantum Espresso is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modelling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. module load xorg-utils module load quantum-espresso/6.1-impi/intel2017 # Set the path here to where ever you keep your pseudopotentials. export ESPRESSO_PSEUDO=$HOME/qe-psp # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun pw.x -in input.in >output.out","title":"Quantum Espresso"},{"location":"Software_Guides/Other_Software/#repast-hpc","text":"Repast for High Performance Computing (Repast HPC) is a next generation agent-based modelling system intended for large-scale distributed computing platforms. It implements the core Repast Simphony concepts (e.g. contexts and projections), modifying them to work in a parallel distributed environment. module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load hdf/5-1.8.15/gnu-4.9.2 module load netcdf/4.3.3.1/gnu-4.9.2 module load netcdf-fortran/4.4.1/gnu-4.9.2 module load mpi/openmpi/1.8.4/gnu-4.9.2 module load python/2.7.9 module load boost/1_54_0/mpi/gnu-4.9.2 module load netcdf-c++/4.2/gnu-4.9.2 module load repast-hpc/2.1/gnu-4.9.2 The module sets the environment variables $REPAST_HPC_INCLUDE , $REPAST_HPC_LIB_DIR and $REPAST_HPC_LIB .","title":"Repast HPC"},{"location":"Software_Guides/Other_Software/#root","text":"ROOT provides a set of OO frameworks for handling, analysing, and visualising large amounts of data. Included are specialised storage methods, methods for histograming, curve fitting, function evaluation, minimization etc. ROOT includes a built-in CINT C++ interpreter. module unload compilers mpi module load compilers/gnu/4.9.2 module load fftw/3.3.4/gnu-4.9.2 module load gsl/1.16/gnu-4.9.2 module load root/6.04.00/gnu-4.9.2 # run root in batch mode root -b -q myMacro.C > myMacro.out","title":"ROOT"},{"location":"Software_Guides/Other_Software/#sas","text":"SAS is a statistics package providing a wide range of tools for data management, analysis and presentation. cd $TMPDIR module load sas/9.4/64 # copy all your input files into $TMPDIR cp ~/Scratch/sas_input/example1/* $TMPDIR sas example1.in # tar up all contents of $TMPDIR back into your space tar cvzf $HOME/Scratch/SAS_output/files_from_job_$JOB_ID.tgz $TMPDIR","title":"SAS"},{"location":"Software_Guides/Other_Software/#starccm","text":"StarCCM+ is a commercial CFD package that handles fluid flows, heat transfer, stress simulations, and other common applications of such. Before running any StarCCM+ jobs on the clusters you must load the StarCCM+ module on a login node. This is so the module can set up two symbolic links in your home directory to directories created in your Scratch area so that user settings etc can be written by running jobs. module load star-ccm+/13.06.012 Here is the jobscript example. # Request one license per core - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l ccmpsuite=1 module load star-ccm+/13.06.012 starccm+ -np $NSLOTS -machinefile $TMPDIR/machines -rsh ssh -batch my_input.sim","title":"StarCCM+"},{"location":"Software_Guides/Other_Software/#starcd","text":"StarCD is a commercial package for modelling and simulating combustion and engine dynamics. You must request access to the group controlling StarCD access (legstarc) to use it. The license is owned by the Department of Mechanical Engineering who will need to approve your access request. # Request one license per core - makes sure your job doesn't start # running until sufficient licenses are free. #$ -l starsuite=1 module load star-cd/4.28.050 # run star without its tracker process as this causes multicore jobs to die early star -notracker StarCD uses IBM Platform MPI by default. You can also run StarCD simulations using Intel MPI by changing the command line to: star -notracker -mpi=intel Simulations run using Intel MPI may run faster than they do when using IBM Platform MPI.","title":"StarCD"},{"location":"Software_Guides/Other_Software/#statamp","text":"Stata is a statistics, data management, and graphics system. Stata/MP is the version of the package that runs on multiple cores. We have a sixteen user license of Stata/MP. Our license supports Stata running on up to four cores per job. # Select 4 OpenMP threads (the most possible) #$ -pe smp 4 cd $TMPDIR module load stata/15 # copy files to $TMPDIR cp myfile.do $TMPDIR stata-mp -b do myfile.do # tar up all contents of $TMPDIR back into your space tar zcvf $HOME/Scratch/Stata_output/files_from_job_$JOB_ID.tar.gz $TMPDIR","title":"Stata/MP"},{"location":"Software_Guides/Other_Software/#torch","text":"Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first. We provide a torch-deps module that contains the main Torch dependencies and creates a quick-install alias, do-torch-install . This uses Torch's installation script to git clone the current distribution and install LuaJIT, LuaRocks and Torch in ~/torch . module unload compilers mpi module load torch-deps do-torch-install You should load these same modules in your jobscript when using the version of torch this installs.","title":"Torch"},{"location":"Software_Guides/Other_Software/#turbomole","text":"Turbomole is an ab initio computational chemistry program that implements various quantum chemistry methods. Turbomole has a Chemistry-wide license. Reserved application group legtmole for Chemistry users only. There are scripts you can use to generate Turbomole jobs for you: /shared/ucl/apps/turbomole/turbomole-mpi.submit /shared/ucl/apps/turbomole/turbomole-smp.submit They will ask you which version you want to use, how much memory, how many cores etc and set up and submit the job for you. Use the first for MPI jobs and the second for single-node shared memory threaded jobs.","title":"Turbomole"},{"location":"Software_Guides/Other_Software/#varscan","text":"VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. module load java/1.8.0_45 module load varscan/2.3.9 Then to run VarScan, you should either prefix the .jar you want to run with $VARSCANPATH: java -Xmx2g -jar $VARSCANPATH/VarScan.v2.3.9.jar OPTION1=value1 OPTION2=value2... Or we provide wrappers, so you can run it this way instead: varscan OPTION1=value1 OPTION2=value2...","title":"VarScan"},{"location":"Software_Guides/Other_Software/#vasp","text":"The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP is licensed software. To gain access, you need to email us the name and email of the main VASP license holder (plus the license number if you have it). We will then ask VASP if we can add you, and on confirmation can do so. We will add you to the legvasp reserved application group, and remove you when VASP tell us you no longer have access. The VASP executables for current versions are named like this: * vasp_gam - optimised for gamma point calculations only * vasp_std - standard version * vasp_ncl - for non-collinear magnetic structure and/or spin-orbit coupling calculations module unload compilers mpi module load compilers/intel/2017/update1 module load mpi/intel/2017/update1/intel # Gerun is our mpirun wrapper which sets the machinefile and number of # processes to the amount you requested with -pe mpi. gerun vasp_std > vasp_output.$JOB_ID Note: although you can run VASP using the default Intel 2018 compiler this can lead to numerical errors in some types of simulation. In those cases we recommend switching to the specific compiler and MPI version used to build that install (mentioned at the end of the module name). We do this in the example above. Building your own VASP: You may also install your own copy of VASP in your home if you have access to the source, and we provide a simple VASP individual install script (tested with VASP 5.4.4, no patches). You need to download the VASP source code into your home directory and then you can run the script following the instructions at the top.","title":"VASP"},{"location":"Software_Guides/Other_Software/#xmds","text":"XMDS allows the fast and easy solution of sets of ordinary, partial and stochastic differential equations, using a variety of efficient numerical algorithms. You will need to load the modules on a login node and run xmds2-setup to set up XMDS. module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/intel/2015/update3/gnu-4.9.2 module load python2/recommended module load fftw/3.3.4-impi/gnu-4.9.2 module load hdf/5-1.8.15/gnu-4.9.2 module load xmds/2.2.2 # run this on a login node to set up XMDS xmds2-setup You can also build the current developmental version from SVN in your space by running create-svn-xmds-inst . Note: the SVN version on 28 Oct 2015 was failing unit test test_nonlocal_access_multiple_components .","title":"XMDS"},{"location":"Software_Guides/R/","text":"Type module avail r to see the currently available versions of R. The current version will always also exist as r/recommended - this is a module bundle and loading it will also load its many dependencies. module show r/recommended shows you exactly which versions loading this module will give you. R can be run on a single core or multithreaded using many cores (some commands can run threaded automatically, otherwise you may wish to look at R's parallel package). Rmpi and snow allow multi-node parallel jobs using MPI to be run. List of additional R packages shows you what packages are installed and available for the current R version. Setup \u00a7 Before you can use R interactively, you need to load the R module using: module unload compilers mpi module load r/recommended Example serial jobscript \u00a7 This script runs R using only one core. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/bin/bash -l # Example jobscript to run a single core R job # Request ten minutes of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM. Change this to suit your requirements. #$ -l mem=1G # Set the name of the job. You can change this if you wish. #$ -N R_job_1 # Set the working directory to somewhere in your scratch space. This is # necessary because the compute nodes cannot write to your $HOME # NOTE: this directory must exist. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/R_output # Your work must be done in $TMPDIR (serial jobs particularly) cd $TMPDIR # Load the R module and run your R program module unload compilers module unload mpi module load r/recommended R --no-save < /home/username/Scratch/myR_job.R > myR_job.out # Preferably, tar-up (archive) all output files to transfer them back # to your space. This will include the R_output file above. tar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR # Make sure you have given enough time for the copy to complete! You will need to change the -wd /home/<your_UCL_id>/Scratch/R_output location and the location of your R input file, called myR_job.R here. myR_job.out is the file we are redirecting the output into. The output file is saved in the tar archive produced by the last command in the runscript and will be in $HOME/Scratch/R_output . If your jobscript is called run-R.sh then your job submission command would be: qsub run-R.sh Example shared memory threaded parallel job \u00a7 This script uses multiple cores on the same node. It cannot run across multiple nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash -l # Example jobscript to run an OpenMP threaded R job across multiple cores on one node. # This may be using the foreach packages foreach(...) %dopar% for example. # Request ten minutes of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per core. Change this to suit your requirements. #$ -l mem=1G # Set the name of the job. You can change this if you wish. #$ -N R_jobMC_2 # Select 12 threads. The number of threads here must equal the number of worker # processes in the registerDoMC call in your R program. #$ -pe smp 12 # Set the working directory to somewhere in your scratch space. This is # necessary because the compute nodes cannot write to your $HOME # NOTE: this directory must exist. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/R_output # Your work must be done in $TMPDIR cd $TMPDIR # Load the R module and run your R program module unload compilers module unload mpi module load r/recommended R --no-save < /home/username/Scratch/myR_job.R > myR_job.out # Preferably, tar-up (archive) all output files to transfer them back # to your space. This will include the R_output file above. tar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR # Make sure you have given enough time for the copy to complete! You will need to change the -wd /home/<your_UCL_id>/Scratch/R_output location and the location of your R input file, called myR_job.R here. myR_job.out is the file we are redirecting the output into. The output file is saved in the tar archive produced by the last command in the runscript and will be in $HOME/Scratch/R_output . If your jobscript is called run-R.sh then your job submission command would be: qsub run-R.sh Example multi-node parallel job using Rmpi and snow \u00a7 This script uses Rmpi and snow to allow it to run across multiple nodes using MPI. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/bin/bash -l # Example jobscript to run an R MPI parallel job # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process. #$ -l mem=1G # Request 15 gigabytes of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N snow_monte_carlo # Select the MPI parallel environment with 32 processes #$ -pe mpi 32 # Set the working directory to somewhere in your scratch space. This is # necessary because the compute nodes cannot write to your $HOME # NOTE: this directory must exist. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/R_output # Load the R module module unload compilers mpi module load r/recommended # Copy example files in to the working directory (not necessary if already there) cp ~/R/Examples/snow_example.R . cp ~/R/Examples/monte_carlo.R . # Run our MPI job. GERun is our wrapper for mpirun, which launches MPI jobs gerun RMPISNOW < snow_example.R > snow.out.${JOB_ID} The output file is saved in $HOME/Scratch/R_examples/snow/snow.out.${JOB_ID} . If your jobscript is called run-R-snow.sh then your job submission command would be: qsub run-R-snow.sh Example R script using Rmpi and snow \u00a7 This R script has been written to use Rmpi and snow and can be used with the above jobscript. It is snow_example.R above. #Load the snow and random number package. library(snow) library(Rmpi) # This example uses the already installed LEcuyers RNG library(rlecuyer) library(rlecuyer) # Set up our input/output source('./monte_carlo.R') sink('./monte_carlo_output.txt') # Get a reference to our snow cluster that has been set up by the RMPISNOW # script. cl <- getMPIcluster () # Display info about each process in the cluster print(clusterCall(cl, function() Sys.info())) # Load the random number package on each R process clusterEvalQ (cl, library (rlecuyer)) # Generate a seed for the pseudorandom number generator, unique to each # processor in the cluster. #Uncomment below line for default (unchanging) random number seed. #clusterSetupRNG(cl, type = 'RNGstream') #The lines below set up a time-based random number seed. Note that #this only demonstrates the virtues of changing the seed; no guarantee #is made that this seed is at all useful. Comment out if you uncomment #the above line. s <- sum(strtoi(charToRaw(date()), base = 32)) clusterSetupRNGstream(cl, seed=rep(s,6)) #Choose which of the following blocks best fit your own needs. # BLOCK 1 # Set up the input to our Monte Carlo function. # Input is identical across the batch, only RNG seed has changed. # For this example, both clusters will roll one die. nrolls <- 2 print(\"Roll the dice once...\") output <- clusterCall(cl, monte_carlo, nrolls) output print(\"Roll the dice again...\") output <- clusterCall(cl, monte_carlo, nrolls) output # Output should show the results of two rolls of a six-sided die. #BLOCK 2 # Input is different for each processor print(\"Second example: coin flip plus 3 dice\") input <- array(1:2) # Set up array of inputs, with each entry input[1] <- 1 # corresponding to one processor. input[2] <- 3 parameters <- array(1:2) # Set up inputs that will be used by each cluster. parameters[1] <- 2 # These will be passed to monte_carlo as its parameters[2] <- 6 # second argument. output <- clusterApply(cl, input, monte_carlo, parameters) # Output should show the results of a coin flip and the roll of three # six-sided die. # Output the output. output inputStrings <- array(1:2) inputStrings[1] <- 'abc' inputStrings[2] <- 'def' output <- clusterApply(cl, inputStrings, paste, 'foo') output #clusterEvalQ(cl, sinkWorkerOutput(\"snow_monte_carlo.out\")) # Clean up the cluster and release the relevant resources. stopCluster(cl) sink() mpi.quit() This example is based on SHARCNET's Using R and MPI . Using your own local R packages \u00a7 First you need to tell R where to install your local package to and where to look for local packages, using the R library path. Set your R library path \u00a7 There are several ways to modify your R library path so you can pick up local packages that you have installed in your own space. The easiest way is to add them to the R_LIBS environment variable (insert the correct path): export R_LIBS=/your/local/R/library/path:$R_LIBS Setting that in your terminal will let you install them from inside R and should be put in your jobscript (or your .bashrc ) when you submit a job using those libraries. This appends your directory to $R_LIBS rather than overwriting it so the system libraries can still be found. You can also change the library path for a session from within R: .libPaths(c('~/MyRlibs',.libPaths())) This puts your local directory at the beginning of R's search path, and means that install.packages() will automatically put packages there and the library() function will find libraries in your local directory. Install an R package \u00a7 To install, after setting your library path: From inside R, you can do install.packages('package_name', repos=\"http://cran.r-project.org\") Or if you have downloaded the tar file, you can do R CMD INSTALL -l /home/username/your_R_libs_directory package.tar.gz If you want to keep some libraries separate, you can have multiple colon-separated paths in your $R_LIBS and specify which one you want to install into with R CMD INSTALL . BioConductor \u00a7 If you are installing extra packages for BioConductor, check that you are using the same version that the R module you have loaded is using. Eg. you can find the BioConductor 3.6 package downloads here .","title":"R"},{"location":"Software_Guides/R/#setup","text":"Before you can use R interactively, you need to load the R module using: module unload compilers mpi module load r/recommended","title":"Setup"},{"location":"Software_Guides/R/#example-serial-jobscript","text":"This script runs R using only one core. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/bin/bash -l # Example jobscript to run a single core R job # Request ten minutes of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM. Change this to suit your requirements. #$ -l mem=1G # Set the name of the job. You can change this if you wish. #$ -N R_job_1 # Set the working directory to somewhere in your scratch space. This is # necessary because the compute nodes cannot write to your $HOME # NOTE: this directory must exist. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/R_output # Your work must be done in $TMPDIR (serial jobs particularly) cd $TMPDIR # Load the R module and run your R program module unload compilers module unload mpi module load r/recommended R --no-save < /home/username/Scratch/myR_job.R > myR_job.out # Preferably, tar-up (archive) all output files to transfer them back # to your space. This will include the R_output file above. tar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR # Make sure you have given enough time for the copy to complete! You will need to change the -wd /home/<your_UCL_id>/Scratch/R_output location and the location of your R input file, called myR_job.R here. myR_job.out is the file we are redirecting the output into. The output file is saved in the tar archive produced by the last command in the runscript and will be in $HOME/Scratch/R_output . If your jobscript is called run-R.sh then your job submission command would be: qsub run-R.sh","title":"Example serial jobscript"},{"location":"Software_Guides/R/#example-shared-memory-threaded-parallel-job","text":"This script uses multiple cores on the same node. It cannot run across multiple nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash -l # Example jobscript to run an OpenMP threaded R job across multiple cores on one node. # This may be using the foreach packages foreach(...) %dopar% for example. # Request ten minutes of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per core. Change this to suit your requirements. #$ -l mem=1G # Set the name of the job. You can change this if you wish. #$ -N R_jobMC_2 # Select 12 threads. The number of threads here must equal the number of worker # processes in the registerDoMC call in your R program. #$ -pe smp 12 # Set the working directory to somewhere in your scratch space. This is # necessary because the compute nodes cannot write to your $HOME # NOTE: this directory must exist. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/R_output # Your work must be done in $TMPDIR cd $TMPDIR # Load the R module and run your R program module unload compilers module unload mpi module load r/recommended R --no-save < /home/username/Scratch/myR_job.R > myR_job.out # Preferably, tar-up (archive) all output files to transfer them back # to your space. This will include the R_output file above. tar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR # Make sure you have given enough time for the copy to complete! You will need to change the -wd /home/<your_UCL_id>/Scratch/R_output location and the location of your R input file, called myR_job.R here. myR_job.out is the file we are redirecting the output into. The output file is saved in the tar archive produced by the last command in the runscript and will be in $HOME/Scratch/R_output . If your jobscript is called run-R.sh then your job submission command would be: qsub run-R.sh","title":"Example shared memory threaded parallel job"},{"location":"Software_Guides/R/#example-multi-node-parallel-job-using-rmpi-and-snow","text":"This script uses Rmpi and snow to allow it to run across multiple nodes using MPI. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/bin/bash -l # Example jobscript to run an R MPI parallel job # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process. #$ -l mem=1G # Request 15 gigabytes of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N snow_monte_carlo # Select the MPI parallel environment with 32 processes #$ -pe mpi 32 # Set the working directory to somewhere in your scratch space. This is # necessary because the compute nodes cannot write to your $HOME # NOTE: this directory must exist. # Replace \"<your_UCL_id>\" with your UCL user ID #$ -wd /home/<your_UCL_id>/Scratch/R_output # Load the R module module unload compilers mpi module load r/recommended # Copy example files in to the working directory (not necessary if already there) cp ~/R/Examples/snow_example.R . cp ~/R/Examples/monte_carlo.R . # Run our MPI job. GERun is our wrapper for mpirun, which launches MPI jobs gerun RMPISNOW < snow_example.R > snow.out.${JOB_ID} The output file is saved in $HOME/Scratch/R_examples/snow/snow.out.${JOB_ID} . If your jobscript is called run-R-snow.sh then your job submission command would be: qsub run-R-snow.sh","title":"Example multi-node parallel job using Rmpi and snow"},{"location":"Software_Guides/R/#example-r-script-using-rmpi-and-snow","text":"This R script has been written to use Rmpi and snow and can be used with the above jobscript. It is snow_example.R above. #Load the snow and random number package. library(snow) library(Rmpi) # This example uses the already installed LEcuyers RNG library(rlecuyer) library(rlecuyer) # Set up our input/output source('./monte_carlo.R') sink('./monte_carlo_output.txt') # Get a reference to our snow cluster that has been set up by the RMPISNOW # script. cl <- getMPIcluster () # Display info about each process in the cluster print(clusterCall(cl, function() Sys.info())) # Load the random number package on each R process clusterEvalQ (cl, library (rlecuyer)) # Generate a seed for the pseudorandom number generator, unique to each # processor in the cluster. #Uncomment below line for default (unchanging) random number seed. #clusterSetupRNG(cl, type = 'RNGstream') #The lines below set up a time-based random number seed. Note that #this only demonstrates the virtues of changing the seed; no guarantee #is made that this seed is at all useful. Comment out if you uncomment #the above line. s <- sum(strtoi(charToRaw(date()), base = 32)) clusterSetupRNGstream(cl, seed=rep(s,6)) #Choose which of the following blocks best fit your own needs. # BLOCK 1 # Set up the input to our Monte Carlo function. # Input is identical across the batch, only RNG seed has changed. # For this example, both clusters will roll one die. nrolls <- 2 print(\"Roll the dice once...\") output <- clusterCall(cl, monte_carlo, nrolls) output print(\"Roll the dice again...\") output <- clusterCall(cl, monte_carlo, nrolls) output # Output should show the results of two rolls of a six-sided die. #BLOCK 2 # Input is different for each processor print(\"Second example: coin flip plus 3 dice\") input <- array(1:2) # Set up array of inputs, with each entry input[1] <- 1 # corresponding to one processor. input[2] <- 3 parameters <- array(1:2) # Set up inputs that will be used by each cluster. parameters[1] <- 2 # These will be passed to monte_carlo as its parameters[2] <- 6 # second argument. output <- clusterApply(cl, input, monte_carlo, parameters) # Output should show the results of a coin flip and the roll of three # six-sided die. # Output the output. output inputStrings <- array(1:2) inputStrings[1] <- 'abc' inputStrings[2] <- 'def' output <- clusterApply(cl, inputStrings, paste, 'foo') output #clusterEvalQ(cl, sinkWorkerOutput(\"snow_monte_carlo.out\")) # Clean up the cluster and release the relevant resources. stopCluster(cl) sink() mpi.quit() This example is based on SHARCNET's Using R and MPI .","title":"Example R script using Rmpi and snow"},{"location":"Software_Guides/R/#using-your-own-local-r-packages","text":"First you need to tell R where to install your local package to and where to look for local packages, using the R library path.","title":"Using your own local R packages"},{"location":"Software_Guides/R/#set-your-r-library-path","text":"There are several ways to modify your R library path so you can pick up local packages that you have installed in your own space. The easiest way is to add them to the R_LIBS environment variable (insert the correct path): export R_LIBS=/your/local/R/library/path:$R_LIBS Setting that in your terminal will let you install them from inside R and should be put in your jobscript (or your .bashrc ) when you submit a job using those libraries. This appends your directory to $R_LIBS rather than overwriting it so the system libraries can still be found. You can also change the library path for a session from within R: .libPaths(c('~/MyRlibs',.libPaths())) This puts your local directory at the beginning of R's search path, and means that install.packages() will automatically put packages there and the library() function will find libraries in your local directory.","title":"Set your R library path"},{"location":"Software_Guides/R/#install-an-r-package","text":"To install, after setting your library path: From inside R, you can do install.packages('package_name', repos=\"http://cran.r-project.org\") Or if you have downloaded the tar file, you can do R CMD INSTALL -l /home/username/your_R_libs_directory package.tar.gz If you want to keep some libraries separate, you can have multiple colon-separated paths in your $R_LIBS and specify which one you want to install into with R CMD INSTALL .","title":"Install an R package"},{"location":"Software_Guides/R/#bioconductor","text":"If you are installing extra packages for BioConductor, check that you are using the same version that the R module you have loaded is using. Eg. you can find the BioConductor 3.6 package downloads here .","title":"BioConductor"},{"location":"Software_Guides/Singularity/","text":"Singularity is installed on all our clusters. You can use containers you have downloaded in your space. Run singularity --version to see which version we currently have installed. Set up cache locations and bind directories \u00a7 The cache directories should be set to somewhere in your space so they don't fill up /tmp on the login nodes. The bindpath specifies what directories are made available inside the container - only your home is bound by default so you need to add Scratch. Replace uccaxxx with your own username. # Set all the Singularity cache dirs to Scratch export SINGULARITY_CACHEDIR=/home/uccaxxx/Scratch/.singularity/ export SINGULARITY_TMPDIR=/home/uccaxxx/Scratch/.singularity/tmp export SINGULARITY_LOCALCACHEDIR=/home/uccaxxx/Scratch/.singularity/localcache export SINGULARITY_PULLFOLDER=/home/uccaxxx/Scratch/.singularity/pull # Bind your Scratch directory so it is accessible from inside the container export SINGULARITY_BINDPATH=/scratch/scratch/uccaxxx Different subdirectories are being set for each cache so you can tell which files came from where. You probably want to add those export statements to your .bashrc under # User specific aliases and functions so those environment variables are always set when you log in. For more information on these options, have a look at the Singularity documentation: Singularity user guide Singularity Bind Paths and Mounts Singularity Build Environment","title":"Singularity"},{"location":"Software_Guides/Singularity/#set-up-cache-locations-and-bind-directories","text":"The cache directories should be set to somewhere in your space so they don't fill up /tmp on the login nodes. The bindpath specifies what directories are made available inside the container - only your home is bound by default so you need to add Scratch. Replace uccaxxx with your own username. # Set all the Singularity cache dirs to Scratch export SINGULARITY_CACHEDIR=/home/uccaxxx/Scratch/.singularity/ export SINGULARITY_TMPDIR=/home/uccaxxx/Scratch/.singularity/tmp export SINGULARITY_LOCALCACHEDIR=/home/uccaxxx/Scratch/.singularity/localcache export SINGULARITY_PULLFOLDER=/home/uccaxxx/Scratch/.singularity/pull # Bind your Scratch directory so it is accessible from inside the container export SINGULARITY_BINDPATH=/scratch/scratch/uccaxxx Different subdirectories are being set for each cache so you can tell which files came from where. You probably want to add those export statements to your .bashrc under # User specific aliases and functions so those environment variables are always set when you log in. For more information on these options, have a look at the Singularity documentation: Singularity user guide Singularity Bind Paths and Mounts Singularity Build Environment","title":"Set up cache locations and bind directories"},{"location":"Supplementary/GPU_Clusters/","text":"UCL users may be able to access the following GPU clusters. National GPU clusters \u00a7 There are two nationally-accessible EPSRC Tier 2 HPC centres with GPUs. Access is generally managed through calls to an EPSRC Resource Allocation Panel Tier 2 RAP calls Filterable page for all open EPSRC calls There may also be pump-priming/proof of concept access available. General information about machines with external access is available at HPC-UK . CSD3 \u00a7 Suitable for workloads spanning multiple compute nodes using GPUs and MPI NVIDIA Tesla P100 Access to CSD3 JADE \u00a7 NVIDIA DGX-1 (Tesla P100) Access to JADE","title":"GPU clusters"},{"location":"Supplementary/GPU_Clusters/#national-gpu-clusters","text":"There are two nationally-accessible EPSRC Tier 2 HPC centres with GPUs. Access is generally managed through calls to an EPSRC Resource Allocation Panel Tier 2 RAP calls Filterable page for all open EPSRC calls There may also be pump-priming/proof of concept access available. General information about machines with external access is available at HPC-UK .","title":"National GPU clusters"},{"location":"Supplementary/GPU_Clusters/#csd3","text":"Suitable for workloads spanning multiple compute nodes using GPUs and MPI NVIDIA Tesla P100 Access to CSD3","title":"CSD3"},{"location":"Supplementary/GPU_Clusters/#jade","text":"NVIDIA DGX-1 (Tesla P100) Access to JADE","title":"JADE"},{"location":"Supplementary/GPU_Nodes/","text":"GPU Nodes \u00a7 Node Types \u00a7 You can view the hardware specifications for GPU node types in Myriad . There are several types of GPU nodes available in Myriad. Available modules \u00a7 You can see the available CUDA modules by typing `module avail cuda` Sample CUDA code \u00a7 There are samples in some CUDA install locations, e.g. /shared/ucl/apps/cuda/7.5.18/gnu-4.9.2/samples /shared/ucl/apps/cuda/8.0.61/gnu-4.9.2/samples which are further documented by NVIDIA here . In general, you should look at their CUDA docs: http://docs.nvidia.com/cuda/ Sample jobscripts \u00a7 You can see sample jobscripts here . Use this in your script to request up to 2 GPUs. `#$ -l gpu=2` Load GCC and the relevant CUDA module. module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18/gnu-4.9.2 Running the sample code \u00a7 To get started, here's how you would compile one of the CUDA samples and run it in an interactive session on a GPU node. You can compile CUDA code on the login nodes like this (which do not have GPUs) if they do not require all the CUDA libraries to be present at compile time. If they do, you'll get an error saying it cannot link the CUDA libraries, and ERROR: CUDA could not be found on your system and you will need tro do your compiling on the GPU node as well. 1. Load the cuda module `module unload compilers mpi` `module load compilers/gnu/4.9.2` `module load cuda/7.5.18/gnu-4.9.2` 2. Copy the samples directory to somewhere in your home (or to Scratch if you're building on the GPU node or are going to want a job to write anything in the same directory). cp -r /shared/ucl/apps/cuda/7.5.18/gnu-4.9.2/NVIDIA_CUDA-7.5_Samples/ ~/cuda 3. Choose an example: eigenvalues in this case, and build using the provided makefile - if you have a look at it you can see it is using nvcc and g++. cd NVIDIA_CUDA-7.5_Samples/6_Advanced/eigenvalues/ make 4. Request an interactive job with a GPU and wait to be given access to the node. You will see your prompt change to indicate that you are on a different node than the login node once your qrsh request has been scheduled, and you can then continue. Load the cuda module on the node and run the program. qrsh -l mem=1G,h_rt=0:30:0,gpu=1 -now no # wait for interactive job to start module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18 cd ~/cuda/NVIDIA_CUDA-7.5_Samples/6_Advanced/eigenvalues/ ./eigenvalues 5. Your output should look something like this: Starting eigenvalues GPU Device 0: \"Tesla M2070\" with compute capability 2.0 Matrix size: 2048 x 2048 Precision: 0.000010 Iterations to be timed: 100 Result filename: 'eigenvalues.dat' Gerschgorin interval: -2.894310 / 2.923303 Average time step 1: 26.739325 ms Average time step 2, one intervals: 9.031162 ms Average time step 2, mult intervals: 0.004330 ms Average time TOTAL: 35.806992 ms Test Succeeded! Building your own code \u00a7 As above, if the code you are trying to compile needs to link against libcuda, it must also be built on a GPU node because only the GPU nodes have the correct libraries. The NVIDIA examples don't require this, but things like Tensorflow do. Tensorflow \u00a7 Tensorflow is installed: type module avail tensorflow to see the available versions. Modules to load for the non-MKL GPU version: module unload compilers mpi module load compilers/gnu/4.9.2 module load python3/3.7 module load cuda/10.0.130/gnu-4.9.2 module load cudnn/7.4.2.24/cuda-10.0 module load tensorflow/2.0.0/gpu-py37 Using MPI and GPUs \u00a7 It is possible to run MPI programs that use GPUs but only within a single node, so you can request up to 2 GPUs and 36 cores on Myriad. Looking for more GPUs? \u00a7 GPU clusters available to UCL users .","title":"GPU nodes"},{"location":"Supplementary/GPU_Nodes/#gpu-nodes","text":"","title":"GPU Nodes"},{"location":"Supplementary/GPU_Nodes/#node-types","text":"You can view the hardware specifications for GPU node types in Myriad . There are several types of GPU nodes available in Myriad.","title":"Node Types"},{"location":"Supplementary/GPU_Nodes/#available-modules","text":"You can see the available CUDA modules by typing `module avail cuda`","title":"Available modules"},{"location":"Supplementary/GPU_Nodes/#sample-cuda-code","text":"There are samples in some CUDA install locations, e.g. /shared/ucl/apps/cuda/7.5.18/gnu-4.9.2/samples /shared/ucl/apps/cuda/8.0.61/gnu-4.9.2/samples which are further documented by NVIDIA here . In general, you should look at their CUDA docs: http://docs.nvidia.com/cuda/","title":"Sample CUDA code"},{"location":"Supplementary/GPU_Nodes/#sample-jobscripts","text":"You can see sample jobscripts here . Use this in your script to request up to 2 GPUs. `#$ -l gpu=2` Load GCC and the relevant CUDA module. module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18/gnu-4.9.2","title":"Sample jobscripts"},{"location":"Supplementary/GPU_Nodes/#running-the-sample-code","text":"To get started, here's how you would compile one of the CUDA samples and run it in an interactive session on a GPU node. You can compile CUDA code on the login nodes like this (which do not have GPUs) if they do not require all the CUDA libraries to be present at compile time. If they do, you'll get an error saying it cannot link the CUDA libraries, and ERROR: CUDA could not be found on your system and you will need tro do your compiling on the GPU node as well. 1. Load the cuda module `module unload compilers mpi` `module load compilers/gnu/4.9.2` `module load cuda/7.5.18/gnu-4.9.2` 2. Copy the samples directory to somewhere in your home (or to Scratch if you're building on the GPU node or are going to want a job to write anything in the same directory). cp -r /shared/ucl/apps/cuda/7.5.18/gnu-4.9.2/NVIDIA_CUDA-7.5_Samples/ ~/cuda 3. Choose an example: eigenvalues in this case, and build using the provided makefile - if you have a look at it you can see it is using nvcc and g++. cd NVIDIA_CUDA-7.5_Samples/6_Advanced/eigenvalues/ make 4. Request an interactive job with a GPU and wait to be given access to the node. You will see your prompt change to indicate that you are on a different node than the login node once your qrsh request has been scheduled, and you can then continue. Load the cuda module on the node and run the program. qrsh -l mem=1G,h_rt=0:30:0,gpu=1 -now no # wait for interactive job to start module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18 cd ~/cuda/NVIDIA_CUDA-7.5_Samples/6_Advanced/eigenvalues/ ./eigenvalues 5. Your output should look something like this: Starting eigenvalues GPU Device 0: \"Tesla M2070\" with compute capability 2.0 Matrix size: 2048 x 2048 Precision: 0.000010 Iterations to be timed: 100 Result filename: 'eigenvalues.dat' Gerschgorin interval: -2.894310 / 2.923303 Average time step 1: 26.739325 ms Average time step 2, one intervals: 9.031162 ms Average time step 2, mult intervals: 0.004330 ms Average time TOTAL: 35.806992 ms Test Succeeded!","title":"Running the sample code"},{"location":"Supplementary/GPU_Nodes/#building-your-own-code","text":"As above, if the code you are trying to compile needs to link against libcuda, it must also be built on a GPU node because only the GPU nodes have the correct libraries. The NVIDIA examples don't require this, but things like Tensorflow do.","title":"Building your own code"},{"location":"Supplementary/GPU_Nodes/#tensorflow","text":"Tensorflow is installed: type module avail tensorflow to see the available versions. Modules to load for the non-MKL GPU version: module unload compilers mpi module load compilers/gnu/4.9.2 module load python3/3.7 module load cuda/10.0.130/gnu-4.9.2 module load cudnn/7.4.2.24/cuda-10.0 module load tensorflow/2.0.0/gpu-py37","title":"Tensorflow"},{"location":"Supplementary/GPU_Nodes/#using-mpi-and-gpus","text":"It is possible to run MPI programs that use GPUs but only within a single node, so you can request up to 2 GPUs and 36 cores on Myriad.","title":"Using MPI and GPUs"},{"location":"Supplementary/GPU_Nodes/#looking-for-more-gpus","text":"GPU clusters available to UCL users .","title":"Looking for more GPUs?"},{"location":"Supplementary/Hostkeys/","text":"These are the current hostkey fingerprints for our clusters. Myriad \u00a7 ECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA RSA key fingerprint is 29:a7:45:04:83:86:ec:95:fa:25:dc:7a:f4:93:78:c1 Kathleen \u00a7 ECDSA key fingerprint is SHA256:rCKAb0yOWXK8+GClKy/pdbwrUbrGMvFkMciZLVcbaTA RSA key fingerprint is 5a:cf:95:a2:e4:05:8a:36:46:dc:65:0a:f2:8b:ab:e1 Thomas \u00a7 ECDSA key fingerprint is SHA256:r48udIRDfBEIJG+jiIJFs/56ZayaKUdusFd+JQ3jsO4 RSA key fingerprint is SHA256:AZ88UVU3BfZkSBOsMw5VKgbDi47o3dpEabPlIB9GtcM Michael \u00a7 ECDSA key fingerprint is SHA256:3PMLXp6ny0dECycvx4D7+t0sNgsSsLvSO5QUYmzkbhs RSA key fingerprint is 85:31:4b:cf:1a:ec:64:e4:b2:98:28:4a:46:b2:c1:90","title":"Hostkeys"},{"location":"Supplementary/Hostkeys/#myriad","text":"ECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA RSA key fingerprint is 29:a7:45:04:83:86:ec:95:fa:25:dc:7a:f4:93:78:c1","title":"Myriad"},{"location":"Supplementary/Hostkeys/#kathleen","text":"ECDSA key fingerprint is SHA256:rCKAb0yOWXK8+GClKy/pdbwrUbrGMvFkMciZLVcbaTA RSA key fingerprint is 5a:cf:95:a2:e4:05:8a:36:46:dc:65:0a:f2:8b:ab:e1","title":"Kathleen"},{"location":"Supplementary/Hostkeys/#thomas","text":"ECDSA key fingerprint is SHA256:r48udIRDfBEIJG+jiIJFs/56ZayaKUdusFd+JQ3jsO4 RSA key fingerprint is SHA256:AZ88UVU3BfZkSBOsMw5VKgbDi47o3dpEabPlIB9GtcM","title":"Thomas"},{"location":"Supplementary/Hostkeys/#michael","text":"ECDSA key fingerprint is SHA256:3PMLXp6ny0dECycvx4D7+t0sNgsSsLvSO5QUYmzkbhs RSA key fingerprint is 85:31:4b:cf:1a:ec:64:e4:b2:98:28:4a:46:b2:c1:90","title":"Michael"},{"location":"Supplementary/Points_of_Contact/","text":"This page contains tools and information for the nominated Points of Contact. Other system-specific information is at Thomas , Michael or Young . These commands can all be run as thomas-command or michael-command or young-command : they run the same thing and the different names are for convenience. Displaying user information \u00a7 thomas-show , michael-show or young-show is a tool that enables you to find a lot of information about users. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. At the top level, --user shows all information for one user, in multiple tables. --contacts shows all points of contact - useful for getting the IDs, and --institutes is the same. --allusers will show everyone's basic info. --getmmm will show the most recently used mmm username. thomas-show -h usage: thomas-show [-h] [--user username] [--contacts] [--institutes] [--allusers] [--getmmm] {recentusers,getusers,whois} ... Show data from the Thomas database. Use [positional argument -h] for more help. positional arguments: {recentusers,getusers,whois} recentusers Show the n newest users (5 by default) getusers Show all users with this project, institute, contact whois Search for users matching the given requirements optional arguments: -h, --help show this help message and exit --user username Show all current info for this user --contacts Show all allowed values for contact --institutes Show all allowed values for institute --allusers Show all current users --getmmm Show the highest mmm username used Show recent users \u00a7 thomas-show recentusers shows you the most recently-added N users, default 5. thomas-show recentusers -h usage: thomas-show recentusers [-h] [-n N] optional arguments: -h, --help show this help message and exit -n N Show users with a given project, institute, contact \u00a7 thomas-show getusers or will search for exact matches to the given project, institute, contact combination. thomas-show getusers -h usage: thomas-show getusers [-h] [-p PROJECT] [-i INST_ID] [-c POC_ID] optional arguments: -h, --help show this help message and exit -p PROJECT, --project PROJECT Project name -i INST_ID, --institute INST_ID Institute ID -c POC_ID, --contact POC_ID Point of Contact ID Search for users based on partial information \u00a7 thomas-show whois can be used to search for partial matches to username, name, email fragments, including all of those in combination. thomas-show whois -h usage: thomas-show whois [-h] [-u USERNAME] [-e EMAIL] [-n GIVEN_NAME] [-s SURNAME] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user contains -e EMAIL, --email EMAIL Email address of user contains -n GIVEN_NAME, --name GIVEN_NAME Given name of user contains -s SURNAME, --surname SURNAME Surname of user contains Adding user information and new projects \u00a7 thomas-add will add information to the database. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. Please note that all options have a --debug flag that will allow you to see the query generated without committing the changes to the database - double-check that the information you are adding is correct. thomas-add -h usage: thomas-add [-h] {user,project,projectuser,poc,institute} ... Add data to the Thomas database. Use [positional argument -h] for more help. positional arguments: {user,project,projectuser,poc,institute} csv Add all users from the provided CSV file user Adding a new user with their initial project project Adding a new project projectuser Adding a new user-project-contact relationship poc Adding a new Point of Contact institute Adding a new institute/consortium optional arguments: -h, --help show this help message and exit Add a new user \u00a7 thomas-add user or allows you to add a new user, with their initial project and point of contact. This does not create their account, but does email us with everything we need in order to create it. If you run this, you do not need to email us separately. The project specified must exist. The user will be allocated the next free mmmxxxx username - you should only specify username yourself if they are an existing UCL user, or on Young if they previously had a Thomas or Michael account you should give them the same username. You can get your poc_id by looking at thomas-show --contacts . thomas-add user -h usage: thomas-add user [-h] -u USERNAME -n GIVEN_NAME [-s SURNAME] -e EMAIL_ADDRESS -k \"SSH_KEY\" -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user -n GIVEN_NAME, --name GIVEN_NAME Given name of user -s SURNAME, --surname SURNAME Surname of user (optional) -e EMAIL_ADDRESS, --email EMAIL_ADDRESS Institutional email address of user -k \"SSH_KEY\", --key \"SSH_KEY\" User's public ssh key (quotes necessary) -p PROJECT_ID, --project PROJECT_ID Initial project the user belongs to -c POC_ID, --contact POC_ID Short ID of the user's Point of Contact --verbose Show SQL queries that are being submitted --debug Show SQL query submitted without committing the change SSH key formats \u00a7 It will verify the provided ssh key by default. Note that it has to be in the form ssh-xxx keystartshere . If someone has sent in a key which has line breaks and header items, make it into this format by adding the key type to the start and removing the line breaks from the key body. This key: ---- BEGIN SSH2 PUBLIC KEY ---- Comment: \"comment goes here\" AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K 7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+ 9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW 7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJW OeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV 3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== ---- END SSH2 PUBLIC KEY ---- should be converted into ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJWOeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== Other types of keys (ed25519 etc) will say what they are in the first line, and you should change the ssh-rsa appropriately. The guide linked at Creating an ssh key in Windows also shows where users can get the second format out of PuTTY. Add new users in bulk from a CSV file \u00a7 young-add csv allows you to add users in bulk using a CSV file of specific format and headers. The CSV is comma-separated with a header line of email,given_name,surname,username,project_ID,ssh_key You can leave username empty for it to allocate them a new username, but if they have an existing mmm username you should fill it in. It may be useful to show users with a given institute on Thomas if you are migrating users from one service to another. You can download a CSV template here . Replace the example data. young-add csv will try to automatically get your Point of Contact ID based on your username. If it can't, or if you have more than one, it will give you a list to choose from. (All users in one CSV upload will be added using the same Point of Contact ID). The project you are adding the user to must already exist. The SSH key must be formatted as shown in SSH key formats . Add a new project \u00a7 thomas-add project or will create a new project, associated with an institution. It will not show in Gold until it also has a user in it. A project ID should begin with your institute ID, followed by an underscore and a project name. thomas-add project -h usage: thomas-add project [-h] -p PROJECT_ID -i INST_ID [--debug] optional arguments: -h, --help show this help message and exit -p PROJECT_ID, --project PROJECT_ID A new unique project ID -i INST_ID, --institute INST_ID Institute ID this project belongs to --debug Show SQL query submitted without committing the change Add a new project/user pairing \u00a7 thomas-add projectuser will add an existing user to an existing project. Creating a new user for an existing project also creates this relationship. After a new project-user relationship is added, a cron job will pick that up within 15 minutes and create that project for that user in Gold, with no allocation. thomas-add projectuser -h usage: thomas-add projectuser [-h] -u USERNAME -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME An existing UCL username -p PROJECT_ID, --project PROJECT_ID An existing project ID -c POC_ID, --contact POC_ID An existing Point of Contact ID --debug Show SQL query submitted without committing the change Gold resource allocation \u00a7 We are currently using Gold to manage allocations. Thomas and Michael share one Gold database, so all the projects exist on both, but they are only active on the correct cluster. Young has its own database. Reporting from Gold \u00a7 There are wrapper scripts for a number of Gold commands (these exist in the userscripts module, loaded by default). These are all set to report in cpu-hours with the -h flag, as that is our main unit. If you wish to change anything about the wrappers, they live in /shared/ucl/apps/cluster-scripts/ so you can take a copy and add your preferred options. They all have a --man option to see the man pages for that command. Here are some basic useful options and what they do. They can all be given more options for more specific searches. gusage -p project_name [-s start_time] # Show the Gold usage per user in this project, in the given timeframe if specified. gbalance # Show the balance for every project, split into total, reserved and available. glsuser # Shows all the users in Gold. glsproject # Shows all the projects and which users are in them. glsres # Show all the current reservatioms, inc user and project. The Name column is the SGE job ID. gstatement # Produce a reporting statement showing beginning and end balances, credits and debits. # Less useful commands glstxn # Show all Gold transactions. Filter or it will take forever to run. glsalloc # Show all the allocations. These can be run by any user. The date format is YYYY-MM-DD. Eg. gstatement -p PROJECT -s 2017-08-01 will show all credits and debits for the given project since the given date, saying which user and job ID each charge was associated with. Transferring Gold \u00a7 As the point of contact, you can transfer Gold from your allocation account into other project accounts. As before, we've put -h in the wrapper so it is always working in cpu-hours. gtransfer --fromProject xxx_allocation --toProject xxx_subproject cpu_hours You can also transfer in the opposite direction, from the subproject back into your allocation account. Note that you are able to transfer your allocation into another institute's projects, but you cannot transfer it back again - only the other institute's point of contact (or rc-support) can give it back, so be careful which project you specify. When two allocations are active \u00a7 There is now an overlap period of a week when two allocations can be active. By default, gtransfer will transfer from active allocations in the order of earliest expiring first. To transfer from the new allocation only, you need to specify the allocation id. gtransfer -i allocation_ID --fromProject xxx_allocation --toProject xxx_subproject cpu_hours glsalloc -p xxx_allocation shows you all allocations that ever existed for your institute, and the first column is the id. Id Account Projects StartTime EndTime Amount Deposited Description --- ------- --------------------- ---------- ---------- ---------- ---------- -------------- 87 38 UKCP_allocation 2017-08-07 2017-11-05 212800.00 3712800.00 97 38 UKCP_allocation 2017-10-30 2018-02-04 3712800.00 3712800.00","title":"MMM Points of Contact"},{"location":"Supplementary/Points_of_Contact/#displaying-user-information","text":"thomas-show , michael-show or young-show is a tool that enables you to find a lot of information about users. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. At the top level, --user shows all information for one user, in multiple tables. --contacts shows all points of contact - useful for getting the IDs, and --institutes is the same. --allusers will show everyone's basic info. --getmmm will show the most recently used mmm username. thomas-show -h usage: thomas-show [-h] [--user username] [--contacts] [--institutes] [--allusers] [--getmmm] {recentusers,getusers,whois} ... Show data from the Thomas database. Use [positional argument -h] for more help. positional arguments: {recentusers,getusers,whois} recentusers Show the n newest users (5 by default) getusers Show all users with this project, institute, contact whois Search for users matching the given requirements optional arguments: -h, --help show this help message and exit --user username Show all current info for this user --contacts Show all allowed values for contact --institutes Show all allowed values for institute --allusers Show all current users --getmmm Show the highest mmm username used","title":"Displaying user information"},{"location":"Supplementary/Points_of_Contact/#show-recent-users","text":"thomas-show recentusers shows you the most recently-added N users, default 5. thomas-show recentusers -h usage: thomas-show recentusers [-h] [-n N] optional arguments: -h, --help show this help message and exit -n N","title":"Show recent users"},{"location":"Supplementary/Points_of_Contact/#show-users-with-a-given-project-institute-contact","text":"thomas-show getusers or will search for exact matches to the given project, institute, contact combination. thomas-show getusers -h usage: thomas-show getusers [-h] [-p PROJECT] [-i INST_ID] [-c POC_ID] optional arguments: -h, --help show this help message and exit -p PROJECT, --project PROJECT Project name -i INST_ID, --institute INST_ID Institute ID -c POC_ID, --contact POC_ID Point of Contact ID","title":"Show users with a given project, institute, contact"},{"location":"Supplementary/Points_of_Contact/#search-for-users-based-on-partial-information","text":"thomas-show whois can be used to search for partial matches to username, name, email fragments, including all of those in combination. thomas-show whois -h usage: thomas-show whois [-h] [-u USERNAME] [-e EMAIL] [-n GIVEN_NAME] [-s SURNAME] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user contains -e EMAIL, --email EMAIL Email address of user contains -n GIVEN_NAME, --name GIVEN_NAME Given name of user contains -s SURNAME, --surname SURNAME Surname of user contains","title":"Search for users based on partial information"},{"location":"Supplementary/Points_of_Contact/#adding-user-information-and-new-projects","text":"thomas-add will add information to the database. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. Please note that all options have a --debug flag that will allow you to see the query generated without committing the changes to the database - double-check that the information you are adding is correct. thomas-add -h usage: thomas-add [-h] {user,project,projectuser,poc,institute} ... Add data to the Thomas database. Use [positional argument -h] for more help. positional arguments: {user,project,projectuser,poc,institute} csv Add all users from the provided CSV file user Adding a new user with their initial project project Adding a new project projectuser Adding a new user-project-contact relationship poc Adding a new Point of Contact institute Adding a new institute/consortium optional arguments: -h, --help show this help message and exit","title":"Adding user information and new projects"},{"location":"Supplementary/Points_of_Contact/#add-a-new-user","text":"thomas-add user or allows you to add a new user, with their initial project and point of contact. This does not create their account, but does email us with everything we need in order to create it. If you run this, you do not need to email us separately. The project specified must exist. The user will be allocated the next free mmmxxxx username - you should only specify username yourself if they are an existing UCL user, or on Young if they previously had a Thomas or Michael account you should give them the same username. You can get your poc_id by looking at thomas-show --contacts . thomas-add user -h usage: thomas-add user [-h] -u USERNAME -n GIVEN_NAME [-s SURNAME] -e EMAIL_ADDRESS -k \"SSH_KEY\" -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user -n GIVEN_NAME, --name GIVEN_NAME Given name of user -s SURNAME, --surname SURNAME Surname of user (optional) -e EMAIL_ADDRESS, --email EMAIL_ADDRESS Institutional email address of user -k \"SSH_KEY\", --key \"SSH_KEY\" User's public ssh key (quotes necessary) -p PROJECT_ID, --project PROJECT_ID Initial project the user belongs to -c POC_ID, --contact POC_ID Short ID of the user's Point of Contact --verbose Show SQL queries that are being submitted --debug Show SQL query submitted without committing the change","title":"Add a new user"},{"location":"Supplementary/Points_of_Contact/#ssh-key-formats","text":"It will verify the provided ssh key by default. Note that it has to be in the form ssh-xxx keystartshere . If someone has sent in a key which has line breaks and header items, make it into this format by adding the key type to the start and removing the line breaks from the key body. This key: ---- BEGIN SSH2 PUBLIC KEY ---- Comment: \"comment goes here\" AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K 7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+ 9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW 7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJW OeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV 3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== ---- END SSH2 PUBLIC KEY ---- should be converted into ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJWOeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== Other types of keys (ed25519 etc) will say what they are in the first line, and you should change the ssh-rsa appropriately. The guide linked at Creating an ssh key in Windows also shows where users can get the second format out of PuTTY.","title":"SSH key formats"},{"location":"Supplementary/Points_of_Contact/#add-new-users-in-bulk-from-a-csv-file","text":"young-add csv allows you to add users in bulk using a CSV file of specific format and headers. The CSV is comma-separated with a header line of email,given_name,surname,username,project_ID,ssh_key You can leave username empty for it to allocate them a new username, but if they have an existing mmm username you should fill it in. It may be useful to show users with a given institute on Thomas if you are migrating users from one service to another. You can download a CSV template here . Replace the example data. young-add csv will try to automatically get your Point of Contact ID based on your username. If it can't, or if you have more than one, it will give you a list to choose from. (All users in one CSV upload will be added using the same Point of Contact ID). The project you are adding the user to must already exist. The SSH key must be formatted as shown in SSH key formats .","title":"Add new users in bulk from a CSV file"},{"location":"Supplementary/Points_of_Contact/#add-a-new-project","text":"thomas-add project or will create a new project, associated with an institution. It will not show in Gold until it also has a user in it. A project ID should begin with your institute ID, followed by an underscore and a project name. thomas-add project -h usage: thomas-add project [-h] -p PROJECT_ID -i INST_ID [--debug] optional arguments: -h, --help show this help message and exit -p PROJECT_ID, --project PROJECT_ID A new unique project ID -i INST_ID, --institute INST_ID Institute ID this project belongs to --debug Show SQL query submitted without committing the change","title":"Add a new project"},{"location":"Supplementary/Points_of_Contact/#add-a-new-projectuser-pairing","text":"thomas-add projectuser will add an existing user to an existing project. Creating a new user for an existing project also creates this relationship. After a new project-user relationship is added, a cron job will pick that up within 15 minutes and create that project for that user in Gold, with no allocation. thomas-add projectuser -h usage: thomas-add projectuser [-h] -u USERNAME -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME An existing UCL username -p PROJECT_ID, --project PROJECT_ID An existing project ID -c POC_ID, --contact POC_ID An existing Point of Contact ID --debug Show SQL query submitted without committing the change","title":"Add a new project/user pairing"},{"location":"Supplementary/Points_of_Contact/#gold-resource-allocation","text":"We are currently using Gold to manage allocations. Thomas and Michael share one Gold database, so all the projects exist on both, but they are only active on the correct cluster. Young has its own database.","title":"Gold resource allocation"},{"location":"Supplementary/Points_of_Contact/#reporting-from-gold","text":"There are wrapper scripts for a number of Gold commands (these exist in the userscripts module, loaded by default). These are all set to report in cpu-hours with the -h flag, as that is our main unit. If you wish to change anything about the wrappers, they live in /shared/ucl/apps/cluster-scripts/ so you can take a copy and add your preferred options. They all have a --man option to see the man pages for that command. Here are some basic useful options and what they do. They can all be given more options for more specific searches. gusage -p project_name [-s start_time] # Show the Gold usage per user in this project, in the given timeframe if specified. gbalance # Show the balance for every project, split into total, reserved and available. glsuser # Shows all the users in Gold. glsproject # Shows all the projects and which users are in them. glsres # Show all the current reservatioms, inc user and project. The Name column is the SGE job ID. gstatement # Produce a reporting statement showing beginning and end balances, credits and debits. # Less useful commands glstxn # Show all Gold transactions. Filter or it will take forever to run. glsalloc # Show all the allocations. These can be run by any user. The date format is YYYY-MM-DD. Eg. gstatement -p PROJECT -s 2017-08-01 will show all credits and debits for the given project since the given date, saying which user and job ID each charge was associated with.","title":"Reporting from Gold"},{"location":"Supplementary/Points_of_Contact/#transferring-gold","text":"As the point of contact, you can transfer Gold from your allocation account into other project accounts. As before, we've put -h in the wrapper so it is always working in cpu-hours. gtransfer --fromProject xxx_allocation --toProject xxx_subproject cpu_hours You can also transfer in the opposite direction, from the subproject back into your allocation account. Note that you are able to transfer your allocation into another institute's projects, but you cannot transfer it back again - only the other institute's point of contact (or rc-support) can give it back, so be careful which project you specify.","title":"Transferring Gold"},{"location":"Supplementary/Points_of_Contact/#when-two-allocations-are-active","text":"There is now an overlap period of a week when two allocations can be active. By default, gtransfer will transfer from active allocations in the order of earliest expiring first. To transfer from the new allocation only, you need to specify the allocation id. gtransfer -i allocation_ID --fromProject xxx_allocation --toProject xxx_subproject cpu_hours glsalloc -p xxx_allocation shows you all allocations that ever existed for your institute, and the first column is the id. Id Account Projects StartTime EndTime Amount Deposited Description --- ------- --------------------- ---------- ---------- ---------- ---------- -------------- 87 38 UKCP_allocation 2017-08-07 2017-11-05 212800.00 3712800.00 97 38 UKCP_allocation 2017-10-30 2018-02-04 3712800.00 3712800.00","title":"When two allocations are active"},{"location":"Wiki_Export/Acknowledging_RC_Systems/","text":"Acknowledging the Use of RC Systems \u00a7 To keep running our services, we depend on being able to demonstrate that they are used in published research. When preparing papers describing work that has used any of our clusters or services, please use the terms below, especially the \" service @UCL\" label, so that we can easily search for them. Legion \u00a7 \"The authors acknowledge the use of the UCL Legion High Performance Computing Facility (Legion@UCL), and associated support services, in the completion of this work.\" Myriad \u00a7 \"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work.\" Grace \u00a7 \"The authors acknowledge the use of the UCL Grace High Performance Computing Facility (Grace@UCL), and associated support services, in the completion of this work.\" Aristotle \u00a7 \"The authors acknowledge the use of the UCL Aristotle Computing Facility (Aristotle@UCL), and associated support services, in the completion of this work.\" Thomas \u00a7 Please find the appropriate wording at Acknowledging the use of Thomas in publications .","title":"Acknowledging the Use of RC Systems"},{"location":"Wiki_Export/Acknowledging_RC_Systems/#acknowledging-the-use-of-rc-systems","text":"To keep running our services, we depend on being able to demonstrate that they are used in published research. When preparing papers describing work that has used any of our clusters or services, please use the terms below, especially the \" service @UCL\" label, so that we can easily search for them.","title":"Acknowledging the Use of RC Systems"},{"location":"Wiki_Export/Acknowledging_RC_Systems/#legion","text":"\"The authors acknowledge the use of the UCL Legion High Performance Computing Facility (Legion@UCL), and associated support services, in the completion of this work.\"","title":"Legion"},{"location":"Wiki_Export/Acknowledging_RC_Systems/#myriad","text":"\"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work.\"","title":"Myriad"},{"location":"Wiki_Export/Acknowledging_RC_Systems/#grace","text":"\"The authors acknowledge the use of the UCL Grace High Performance Computing Facility (Grace@UCL), and associated support services, in the completion of this work.\"","title":"Grace"},{"location":"Wiki_Export/Acknowledging_RC_Systems/#aristotle","text":"\"The authors acknowledge the use of the UCL Aristotle Computing Facility (Aristotle@UCL), and associated support services, in the completion of this work.\"","title":"Aristotle"},{"location":"Wiki_Export/Acknowledging_RC_Systems/#thomas","text":"Please find the appropriate wording at Acknowledging the use of Thomas in publications .","title":"Thomas"},{"location":"Wiki_Export/Additional_Resource_Requests/","text":"Additional Resource Requests \u00a7 We recognise that researchers may sometimes require a higher throughput of work than it is possible to achieve with free \u2018fair share\u2019 usage of Legion , Myriad , and Grace . There a couple of ways of obtaining additional Legion resource beyond this fair share: Make a special request to the CRAG for free access to additional resources \u00a7 Users who wish to request additional resources or reserve resources beyond those provided can complete the additional resource request form in collaboration with your supervisor or the project's principal investigator. This includes requests for increased storage quotas. The completed form should be sent to the Research Computing Platforms team at rc-support@ucl.ac.uk , for technical review. If successful, your case will be presented to the CRAG for consideration at the next meeting of the Group. The CRAG meets monthly, usually on the third Friday of the month, and users will be informed of the Group\u2019s decision as soon as possible after their next meeting. Note that an application to the CRAG for additional resources is only likely to be approved if the impact on other users is not deemed to be significant, or of long duration. Additional resource request form Request hosting of shared datasets \u00a7 We have provision for hosting shared datasets for users on Myriad. These can be datasets that are freely accessible by all users, or ones limited to groups. Hosted datasets: Will not be backed up. Must have a named primary contact. Must be reapplied for every 12 months to make sure they are still current and required. Will have an associated quota. Will be removed when renewal lapses (notice will be given). They are likely to be managed by a role account - access to the role account will be by ssh key. To apply for a hosted dataset, please send this form to rc-support@ucl.ac.uk . Hosted dataset request form Purchase dedicated compute nodes within Legion or Myriad \u00a7 There has previously been a programme allowing researchers to purchase compute nodes to be attached to the Legion cluster. This has been discontinued, as the Myriad cluster replaces the Legion cluster for the majority of users. There are plans to allow the purchase of nodes for Myriad in future but these have not yet been finalised. Further information \u00a7 For further advice or information on future hardware options, please contact rits@ucl.ac.uk .","title":"Additional Resource Requests"},{"location":"Wiki_Export/Additional_Resource_Requests/#additional-resource-requests","text":"We recognise that researchers may sometimes require a higher throughput of work than it is possible to achieve with free \u2018fair share\u2019 usage of Legion , Myriad , and Grace . There a couple of ways of obtaining additional Legion resource beyond this fair share:","title":"Additional Resource Requests"},{"location":"Wiki_Export/Additional_Resource_Requests/#make-a-special-request-to-the-crag-for-free-access-to-additional-resources","text":"Users who wish to request additional resources or reserve resources beyond those provided can complete the additional resource request form in collaboration with your supervisor or the project's principal investigator. This includes requests for increased storage quotas. The completed form should be sent to the Research Computing Platforms team at rc-support@ucl.ac.uk , for technical review. If successful, your case will be presented to the CRAG for consideration at the next meeting of the Group. The CRAG meets monthly, usually on the third Friday of the month, and users will be informed of the Group\u2019s decision as soon as possible after their next meeting. Note that an application to the CRAG for additional resources is only likely to be approved if the impact on other users is not deemed to be significant, or of long duration. Additional resource request form","title":"Make a special request to the CRAG for free access to additional resources"},{"location":"Wiki_Export/Additional_Resource_Requests/#request-hosting-of-shared-datasets","text":"We have provision for hosting shared datasets for users on Myriad. These can be datasets that are freely accessible by all users, or ones limited to groups. Hosted datasets: Will not be backed up. Must have a named primary contact. Must be reapplied for every 12 months to make sure they are still current and required. Will have an associated quota. Will be removed when renewal lapses (notice will be given). They are likely to be managed by a role account - access to the role account will be by ssh key. To apply for a hosted dataset, please send this form to rc-support@ucl.ac.uk . Hosted dataset request form","title":"Request hosting of shared datasets"},{"location":"Wiki_Export/Additional_Resource_Requests/#purchase-dedicated-compute-nodes-within-legion-or-myriad","text":"There has previously been a programme allowing researchers to purchase compute nodes to be attached to the Legion cluster. This has been discontinued, as the Myriad cluster replaces the Legion cluster for the majority of users. There are plans to allow the purchase of nodes for Myriad in future but these have not yet been finalised.","title":"Purchase dedicated compute nodes within Legion or Myriad"},{"location":"Wiki_Export/Additional_Resource_Requests/#further-information","text":"For further advice or information on future hardware options, please contact rits@ucl.ac.uk .","title":"Further information"},{"location":"Wiki_Export/Building_and_Running_Matlab_Programs/","text":"Before you start, here are the caveats: \u00a7 Although full Matlab is now available on Legion, you can still compile Matlab programs on an external machine and then run them on Legion using the Matlab runtime. Your Matlab program must be compiled using a 64bit Linux version of the Matlab compiler; the compiled code is not cross-platform compatible so it cannot be built on OS X and then transferred to Legion. Piping code into the Matlab compiler will not work, and the main routine being executed must be converted into a proper Matlab function. When arguments are passed into compiled Matlab executable, the compiled code does not automatically convert them to the required type (i.e. float or integer) as Matlab does from the command line. In this case the arguments, where necessary, must be converted to numbers using the str2num() function. Because of the way Matlab threads work, you must request exclusive access to Legion nodes when running compiled Matlab programs. Compiling your program: \u00a7 The Matlab code is must be compiled using the mcc tool; this must be initially run as mcc -setup before anything is built. The mcc tool can actually be invoked from the interpreter command prompt and executing help mcc will give you quite a lot of information about how to use the tool, along with examples. All .m files must be built into the compiled code with the first .m referenced in the build line acting as the main entry point for the built code. It may be useful to include data files in the built code which are handled in the build line using the -a <datafile> option. Please remember to make the .m file an actual function and all other dependencies sub-functions, otherwise the compiled code will not execute. Some important mcc options: \u00a7 -m : this is option which runs the macro to generate a C stand-alone application. -R : specify runtime options for the Matlab compiler runtime. Some important runtime options: \u00a7 -nojvm : disables the java virtual machine, which may speed-up certain codes. This option cannot be used if you are planning to have, for example pdf files or any other plots produced as output of your run. -nodisplay : prevents anything being displayed on the screen, can be useful if this happens with the application as this would not work correctly in batch mode. --singleCompThread : use only a single computational thread, otherwise Matlab will try to use more than one thread when the operation being performed supports multi threading. This is an alternative to allocating a whole Legion node to your job. Once the application has been built, there should be an executable named after the prefix of the .m file, generally <app name>.m , and a shell script with the name run\\_<app name>.sh - both these files need to be transferred to Legion. We have installed a runtime environment on Legion here: /shared/ucl/apps/Matlab/R2011a/Runtime7.15/v715/ If you have been given pre-compiled code by someone else, the application may not work as the Matlab runtime version must reasonably match that of the Matlab compiler that was used to build the application. The runtime is freely distributable and can be found in the installation directory of Matlab. The runtime has a GUI install interface and it can be installed at any location in your home directory. For more information, please read your Matlab documentation. Job submission scripts: \u00a7 There are three things that you must take into account: The location of the Matlab compiler runtime needs to be passed to the script used to run the compiled Matlab code as the first argument. The compiler runtime needs a directory (cache) to unpack files to when it is running. By default this directory is in the home folder. This needs to be changed since the home directory is not writable in Legion from the compute nodes. Since the Matlab runs will be single node jobs, the cache location should be in the storage on the compute nodes which is stored in TMPDIR . Use the -ac exclusive SGE option to request exclusive access to a Legion node unless you use the --singleCompThread Matlab option. For example, a multi-threaded serial script should look something like: #!/bin/bash -l # Batch script to run a serial job on Legion under SGE. # Force bash as the executing shell. #$ -S /bin/bash # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM #$ -l mem=1G # Select 12 threads (the most possible on Legion). #$ -l thr=12 # The way Matlab threads work requires Matlab to not share nodes with other # jobs. #$ -ac exclusive # Set the name of the job. #$ -N Matlab_Job_1 # Set the working directory to somewhere in your scratch space. # For example: ##$ -wd /home//Scratch # Alternatively, you can automatically use the current working directory # if you launch your job from anywhere *within ~/Scratch* #$ -cwd # store the MATLAB runtime path in a global environment variable (MCR_HOME) export MCR_HOME = /shared/ucl/apps/Matlab/R2011a/Runtime7.15/v715/ # the path to the Matlab cache is stored in the global variable MCR_CACHE_ROOT export MCR_CACHE_ROOT = $TMPDIR /mcr_cache # make sure the directory in MCR_CACHE_ROOT exists mkdir -p $MCR_CACHE_ROOT # Run the executable, passing the path stored in MCR_HOME as the first argument. # There is no need to pass the content of MCR_CACHE_ROOT as an argument to the # to the run_appname.sh script since it is a variable that the Matlab runtime is aware of. ./run_appname.sh $MCR_HOME [ arguments list ] # Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME /Scratch/files_from_job_ ${ JOB_ID } .tgz $TMPDIR # Make sure you have given enough time for the copy to complete! For any queries and problem reports, please contact rc-support@ucl.ac.uk .","title":"Building and Running Matlab Programs"},{"location":"Wiki_Export/Building_and_Running_Matlab_Programs/#before-you-start-here-are-the-caveats","text":"Although full Matlab is now available on Legion, you can still compile Matlab programs on an external machine and then run them on Legion using the Matlab runtime. Your Matlab program must be compiled using a 64bit Linux version of the Matlab compiler; the compiled code is not cross-platform compatible so it cannot be built on OS X and then transferred to Legion. Piping code into the Matlab compiler will not work, and the main routine being executed must be converted into a proper Matlab function. When arguments are passed into compiled Matlab executable, the compiled code does not automatically convert them to the required type (i.e. float or integer) as Matlab does from the command line. In this case the arguments, where necessary, must be converted to numbers using the str2num() function. Because of the way Matlab threads work, you must request exclusive access to Legion nodes when running compiled Matlab programs.","title":"Before you start, here are the caveats:"},{"location":"Wiki_Export/Building_and_Running_Matlab_Programs/#compiling-your-program","text":"The Matlab code is must be compiled using the mcc tool; this must be initially run as mcc -setup before anything is built. The mcc tool can actually be invoked from the interpreter command prompt and executing help mcc will give you quite a lot of information about how to use the tool, along with examples. All .m files must be built into the compiled code with the first .m referenced in the build line acting as the main entry point for the built code. It may be useful to include data files in the built code which are handled in the build line using the -a <datafile> option. Please remember to make the .m file an actual function and all other dependencies sub-functions, otherwise the compiled code will not execute.","title":"Compiling your program:"},{"location":"Wiki_Export/Building_and_Running_Matlab_Programs/#some-important-mcc-options","text":"-m : this is option which runs the macro to generate a C stand-alone application. -R : specify runtime options for the Matlab compiler runtime.","title":"Some important mcc options:"},{"location":"Wiki_Export/Building_and_Running_Matlab_Programs/#some-important-runtime-options","text":"-nojvm : disables the java virtual machine, which may speed-up certain codes. This option cannot be used if you are planning to have, for example pdf files or any other plots produced as output of your run. -nodisplay : prevents anything being displayed on the screen, can be useful if this happens with the application as this would not work correctly in batch mode. --singleCompThread : use only a single computational thread, otherwise Matlab will try to use more than one thread when the operation being performed supports multi threading. This is an alternative to allocating a whole Legion node to your job. Once the application has been built, there should be an executable named after the prefix of the .m file, generally <app name>.m , and a shell script with the name run\\_<app name>.sh - both these files need to be transferred to Legion. We have installed a runtime environment on Legion here: /shared/ucl/apps/Matlab/R2011a/Runtime7.15/v715/ If you have been given pre-compiled code by someone else, the application may not work as the Matlab runtime version must reasonably match that of the Matlab compiler that was used to build the application. The runtime is freely distributable and can be found in the installation directory of Matlab. The runtime has a GUI install interface and it can be installed at any location in your home directory. For more information, please read your Matlab documentation.","title":"Some important runtime options:"},{"location":"Wiki_Export/Building_and_Running_Matlab_Programs/#job-submission-scripts","text":"There are three things that you must take into account: The location of the Matlab compiler runtime needs to be passed to the script used to run the compiled Matlab code as the first argument. The compiler runtime needs a directory (cache) to unpack files to when it is running. By default this directory is in the home folder. This needs to be changed since the home directory is not writable in Legion from the compute nodes. Since the Matlab runs will be single node jobs, the cache location should be in the storage on the compute nodes which is stored in TMPDIR . Use the -ac exclusive SGE option to request exclusive access to a Legion node unless you use the --singleCompThread Matlab option. For example, a multi-threaded serial script should look something like: #!/bin/bash -l # Batch script to run a serial job on Legion under SGE. # Force bash as the executing shell. #$ -S /bin/bash # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM #$ -l mem=1G # Select 12 threads (the most possible on Legion). #$ -l thr=12 # The way Matlab threads work requires Matlab to not share nodes with other # jobs. #$ -ac exclusive # Set the name of the job. #$ -N Matlab_Job_1 # Set the working directory to somewhere in your scratch space. # For example: ##$ -wd /home//Scratch # Alternatively, you can automatically use the current working directory # if you launch your job from anywhere *within ~/Scratch* #$ -cwd # store the MATLAB runtime path in a global environment variable (MCR_HOME) export MCR_HOME = /shared/ucl/apps/Matlab/R2011a/Runtime7.15/v715/ # the path to the Matlab cache is stored in the global variable MCR_CACHE_ROOT export MCR_CACHE_ROOT = $TMPDIR /mcr_cache # make sure the directory in MCR_CACHE_ROOT exists mkdir -p $MCR_CACHE_ROOT # Run the executable, passing the path stored in MCR_HOME as the first argument. # There is no need to pass the content of MCR_CACHE_ROOT as an argument to the # to the run_appname.sh script since it is a variable that the Matlab runtime is aware of. ./run_appname.sh $MCR_HOME [ arguments list ] # Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME /Scratch/files_from_job_ ${ JOB_ID } .tgz $TMPDIR # Make sure you have given enough time for the copy to complete! For any queries and problem reports, please contact rc-support@ucl.ac.uk .","title":"Job submission scripts:"},{"location":"Wiki_Export/Compiling/","text":"Compiling Your Code \u00a7 Download your code \u00a7 Use wget or curl to download the source code for the software you want to install to your account. There might be binaries available, but they often won't work on our clusters because they were compiled for other machines with other library versions available. Use tar or unzip or similar depending on archive type to uncompress your source code. wget https://www.example.com/program.tar.gz tar -xvf program.tar.gz You won't be able to use a package manager like yum, you'll need to follow the manual installation instructions for a user-space install (not using sudo). Set up your modules \u00a7 Before you start compiling, you need to make sure you have the right compilers, libraries and other tools available for your software. If you haven't changed anything, you will have the default modules loaded. For more information on how to use modules, see RC Systems user environment . Check what the instructions for your software tell you about compiling it. If the website doesn't say much, the source code will hopefully have a README or INSTALL file. You may want to use a different compiler - the default is the Intel compiler. module avail compilers will show you all the compiler modules available. Most Open Source software tends to assume you're using GCC and OpenMPI (if it uses MPI) and is most tested with that combination, so if it doesn't specify you may want to begin there (do check what the newest modules available are): module unload compilers mpi mkl module load compilers/gnu/4.9.2 module load mpi/openmpi/1.10.1/gnu-4.9.2 Available compilers \u00a7 The following compilers are available and supported on Legion: Intel C, C++ and Fortran GNU C, C++ and Fortran We currently have a limited number of licenses for the Intel compilers so only a certain number of users can use them simultaneously. This means that your compilation may fail with an error complaining about not being able to obtain a valid license. If this happens, simply wait for a few minutes and try again. In addition to the supported tools, there are a number of tools installed on Legion which are not supported (for example the PGI compilers) which were installed to build certain supported packages. Users who use the unsupported packages do so at their own risk. Build systems \u00a7 Most software will use some kind of build system to manage how files are compiled and linked and in what order. Here are a few common ones. Automake configure \u00a7 Automake will generate the Makefile for you and hopefully pick up sensible options through configuration. You can give it an install prefix to tell it where to install (or you can build it in place and not use make install at all). ./configure --prefix=/home/username/place/you/want/to/install make # if it has a test suite, good idea to use it make test make install If it has more configuration flags, you can use ./configure --help to view them. Usually configure will create a config.log: you can look in there to find if any tests have failed or things you think should have been picked up haven't. CMake \u00a7 CMake is another build system. It will have a CMakeFile or the instructions will ask you to use cmake or ccmake rather than make. It also generates Makefiles for you. ccmake is a terminal-based interactive interface where you can see what variables are set to and change them, then repeatedly configure until everything is correct, generate the Makefile and quit. cmake is the commandline version. The process tends to go like this: ccmake CMakeLists.txt # press c to configure - will pick up some options # press t to toggle advanced options # keep making changes and configuring until no more errors or changes # press g to generate and exit make # if it has a test suite, good idea to use it make test make install If you need to rerun ccmake and reconfigure, remember to delete the CMakeCache.txt file, or you'll be wondering why your changes haven't taken. Turning on verbose Makefiles in ccmake is also useful if your code didn't compile first time - you'll be able to see what flags the compiler or linker is actually being given when it fails. Make \u00a7 Your code may just come with a Makefile and have no configure, in which case the generic way to compile it is as follows: make targetname There's usually a default target, which make on its own will use. If you need to change any configuration options, you'll need to edit those sections of the Makefile (at the top, where the variables/flags are defined). Here are some typical variables you may want to change in a Makefile. These are what compilers/mpi wrappers to use - these are also defined by the compiler modules, so you can see what they should be. Intel would be icc, icpc, ifort, for example. If it's a program that can be compiled using MPI and only has a variable for CC, then set that to mpicc. CC=gcc CXX=g++ FC=gfortran MPICC=mpicc MPICXX=mpicxx MPIF90=mpif90 CFLAGS and LDFLAGS are flags for the compiler and linker respectively, and there might be LIBS or INCLUDE as well. When linking a library with the name libfoo, use -lfoo . CFLAGS=\"-I/path/to/include\" LDFLAGS=\"-L/path/to/foo/lib -L/path/to/bar/lib\" LDLIBS=\"-lfoo -lbar\" Remember to make clean first if you are recompiling with new options! AVX instructions \u00a7 Note : Legion's current login nodes are of a newer architecture than some of the compute nodes - the login nodes have AVX instructions but the XYZ type nodes do not. This means if you want your code to run on the older nodes, some compiler options cannot be used (e.g. -march=native , -mtune , -xHost ) or your code will segfault. You can either build the code on the login nodes and restrict your jobs to only running on the newer nodes, compile the code with appropriate options for all the nodes, or compile your code inside a job that is running on the XYZ nodes (or during a qrsh session on the same). Intel compilers \u00a7 To tell the Intel compilers to build for SSE4.2 instructions and no AVX, add this to CFLAGS (and CXXFLAGS if relevant): CFLAGS=-axSSE4.2 (Also see icc -help codegen ). GNU compilers \u00a7 To tell GCC to build for SSE4.2 without AVX, add this to CFLAGS (and CXXFLAGS if relevant): CFLAGS=-march=nehalem Restrict node type \u00a7 To restrict a job to newer nodes only, put this in your script: #$ -ac allow=LMNOPQSTU You can also compile one version without AVX and one with, so you can take advantage of the newer nodes when possible. You could use hostname in your jobscript to check what type of node you were on and run the appropriate binary. Hostnames begin with node-x for an X-type node, node-u for a U-type and so on. Test for AVX \u00a7 We have a script that will let you test whether your compiled code is using AVX instructions. If you pass it a directory it will recursively test everything in there. Note that if your code builds multiple kernels and so can choose based on where it runs which instructions to use (like MKL) then this will find AVX instructions but they won't cause your code to segfault. find /home/username/path/ -perm /111 -type f | xargs /shared/ucl/apps/rcops_scripts/hasavx -q BLAS and LAPACK \u00a7 BLAS and LAPACK are provided as part of MKL, OpenBLAS or ATLAS. There are several different OpenBLAS and ATLAS modules on Legion for different compilers. MKL is available in the Intel compiler module. Your code may try to link -lblas -llapack : this isn't the right way to use BLAS and LAPACK with MKL or ATLAS (our OpenBLAS now has symlinks that allow you to do this). MKL on Legion OpenBLAS on Legion ATLAS on Legion Set your PATH and other environment variables \u00a7 After you have installed your software, you'll need to add it to your PATH environment variable so you can run it without having to give the full path to its location. Put this in your ~/.bashrc file so it will set this with every new session you create. Replace username with your username and point to the directory your binary was built in (frequently program/bin ). This adds it to the front of your PATH, so if you install a newer version of something, it will be found before the system one. export PATH=/home/username/location/of/software/binary:$PATH If you built a library that you'll go on to compile other software with, you probably want to also add the lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH, and the include directory to CPATH (add export statements as above). This may mean your configure step will pick your library up correctly without any further effort on your part. To make these changes to your .bashrc take effect in your current session: source ~/.bashrc Python \u00a7 There are python2/recommended and python3/recommended bundles. These use a virtualenv and have pip set up for you. They both have numpy and scipy available. Set compiler module \u00a7 The Python versions on Legion were built with GCC. You can run them with the default Intel compilers loaded because everything depends on the gcc-libs/4.9.2 module. When you are building your own Python packages you should have the GCC compiler module loaded however, to avoid the situation where you build a package with the Intel compiler and then try to run it with GCC, in which case it will be unable to find Intel-specific instructions. module unload compilers module load compilers/gnu/4.9.2 If you get an error like this when trying to run something, recheck what compiler you used. undefined symbol: __intel_sse2_strrchr Install your own packages in the same virtualenv \u00a7 This will use our central virtualenv, which contains a number of packages already installed. pip install --user <python2pkg> pip3 install --user <python3pkg> These will install into .python2local or .python3local directories in your home directory, respectively. To see what is already installed, the Python-shared list shows what is installed for both Python2 and 3, while the Python2 list and Python3 list show what is only installed for one or the other. (There may also be prereqs that aren't listed explicitly - pip will tell you if something is already installed as long as you have the recommended module bundle loaded). Use your own virtualenvs \u00a7 If you need different packages that are not compatible with the central installs, you can create a new virtualenv and only yours will be available. virtualenv <DIR> source <DIR>/bin/activate Your bash prompt will show you that a different virtualenv is active. Installing via setup.py \u00a7 If you need to install using setup.py, you can use the --user flag and as long as one of the python bundles is loaded, it will install into the same .python2local or .python3local as pip and you won't need to add any new paths to your environment. python setup.py install --user You can alternatively use --prefix in which case you will have to set the install prefix to somewhere in your space, and also set PYTHONPATH and PATH to include your install location. Some installs won't create the prefix directory for you, in which case create it first. This is useful if you want to keep this package entirely separate and only in your paths on demand. export PYTHONPATH=/home/username/your/path/lib/python2.7/site-packages:$PYTHONPATH # if necessary, create install path mkdir -p home/username/your/path/lib/python2.7/site-packages python setup.py install --prefix=/home/username/your/path # add these to your .bashrc or jobscript export PYTHONPATH=/home/username/your/path/lib/python2.7/site-packages:$PYTHONPATH export PATH=/home/username/your/path/bin:$PATH Check that the PATH is where your Python executables were installed, and the PYTHONPATH is correct. It will tend to tell you at install time if you need to change or create the PYTHONPATH directory. Python script executable paths \u00a7 If you have an executable python script giving the location of python like this, and it fails because that python doesn't exist in that location or isn't the one that has the additional packages installed: 1 #!/usr/bin/python2.6 You should change it so it uses the first python found in your environment. 1 #!/usr/bin/env python Perl \u00a7 Perl modules will freqently have a Makefile.PL (especially if you download the tar files from CPAN.org yourself). You can install manually as: perl Makefile.PL PREFIX=/home/username/your/perl/location make make install CPAN \u00a7 You can use CPAN to download and install modules locally for you. The first time you run the cpan command, it will create a .cpan directory for you and ask you to give it configuration settings or allow it to set them automatically. You need to tell it where you want your install prefix to be. If it is automatically configured, you need to edit these lines in your .cpan/CPAN/MyConfig.pm , for example if you want it to be in a lib directory in your home (change username to your own username): 'make_install_arg' => q[PREFIX=/home/username/lib] , # other lines in here 'makepl_arg' => q[PREFIX=/home/username/lib] , 'mbuild_install_arg' => q[PREFIX=/home/username/lib] , 'mbuildpl_arg' => q[--install_base /home/username/lib] , It will download and build modules inside .cpan and install them where you specified. Set PERL5LIB paths \u00a7 If you install your own Perl or Perl modules, you will need to append them to your PERL5LIB: export PERL5LIB=/home/username/your/perl/location:$PERL5LIB If you installed with CPAN, you may need to add several paths to this based on the layout it creates inside your nominated Perl directory. Errors when using non-default Perl versions \u00a7 warnings.pm \u00a7 If you are using a version of Perl that is not the default system Perl and get strange errors when trying to run a Perl script, particularly ones about warnings.pm: Search pattern not terminated at /shared/ucl/apps/perl/5.20.0/lib/5.20.0/warnings.pm line 1099 then you need to edit the script so that instead of beginning with #!/usr/bin/perl , it begins with #!/usr/bin/env perl . Otherwise it will try to use the old system Perl libraries with your newer Perl executable, which won't work. libperl.so not found \u00a7 You probably built perl without telling it to build the shared library too. Add -Duseshrplib to your build flags. R \u00a7 There are instructions on installing and using local R packages in Using your own local R packages . Compiling with MPI \u00a7 OpenMPI and Intel MPI are available. Certain programs do not work well with one or the other, so if you are having problems try the other one. Intel MPI is based on MPICH, so if the program you are compiling mentions that, try Intel MPI first. The Intel MPI is threadsafe; some versions of OpenMPI aren't. Note that OpenMPI 1.8.4 had a segv bug in non-blocking collectives that is fixed in OpenMPI 1.10.1. Enabling OpenMP \u00a7 To enable OpenMP with the Intel compilers, you simply need to add -openmp to your compile line. With the GNU compilers you need to add -fopenmp . Problems \u00a7 If you experience problems building your applications, please contact your local IT support in the first instance. We are available at rc-support AT ucl.ac.uk to help you if you still cannot build your app or if you need to report a problem with our software stack.","title":"Compiling Your Code"},{"location":"Wiki_Export/Compiling/#compiling-your-code","text":"","title":"Compiling Your Code"},{"location":"Wiki_Export/Compiling/#download-your-code","text":"Use wget or curl to download the source code for the software you want to install to your account. There might be binaries available, but they often won't work on our clusters because they were compiled for other machines with other library versions available. Use tar or unzip or similar depending on archive type to uncompress your source code. wget https://www.example.com/program.tar.gz tar -xvf program.tar.gz You won't be able to use a package manager like yum, you'll need to follow the manual installation instructions for a user-space install (not using sudo).","title":"Download your code"},{"location":"Wiki_Export/Compiling/#set-up-your-modules","text":"Before you start compiling, you need to make sure you have the right compilers, libraries and other tools available for your software. If you haven't changed anything, you will have the default modules loaded. For more information on how to use modules, see RC Systems user environment . Check what the instructions for your software tell you about compiling it. If the website doesn't say much, the source code will hopefully have a README or INSTALL file. You may want to use a different compiler - the default is the Intel compiler. module avail compilers will show you all the compiler modules available. Most Open Source software tends to assume you're using GCC and OpenMPI (if it uses MPI) and is most tested with that combination, so if it doesn't specify you may want to begin there (do check what the newest modules available are): module unload compilers mpi mkl module load compilers/gnu/4.9.2 module load mpi/openmpi/1.10.1/gnu-4.9.2","title":"Set up your modules"},{"location":"Wiki_Export/Compiling/#available-compilers","text":"The following compilers are available and supported on Legion: Intel C, C++ and Fortran GNU C, C++ and Fortran We currently have a limited number of licenses for the Intel compilers so only a certain number of users can use them simultaneously. This means that your compilation may fail with an error complaining about not being able to obtain a valid license. If this happens, simply wait for a few minutes and try again. In addition to the supported tools, there are a number of tools installed on Legion which are not supported (for example the PGI compilers) which were installed to build certain supported packages. Users who use the unsupported packages do so at their own risk.","title":"Available compilers"},{"location":"Wiki_Export/Compiling/#build-systems","text":"Most software will use some kind of build system to manage how files are compiled and linked and in what order. Here are a few common ones.","title":"Build systems"},{"location":"Wiki_Export/Compiling/#automake-configure","text":"Automake will generate the Makefile for you and hopefully pick up sensible options through configuration. You can give it an install prefix to tell it where to install (or you can build it in place and not use make install at all). ./configure --prefix=/home/username/place/you/want/to/install make # if it has a test suite, good idea to use it make test make install If it has more configuration flags, you can use ./configure --help to view them. Usually configure will create a config.log: you can look in there to find if any tests have failed or things you think should have been picked up haven't.","title":"Automake configure"},{"location":"Wiki_Export/Compiling/#cmake","text":"CMake is another build system. It will have a CMakeFile or the instructions will ask you to use cmake or ccmake rather than make. It also generates Makefiles for you. ccmake is a terminal-based interactive interface where you can see what variables are set to and change them, then repeatedly configure until everything is correct, generate the Makefile and quit. cmake is the commandline version. The process tends to go like this: ccmake CMakeLists.txt # press c to configure - will pick up some options # press t to toggle advanced options # keep making changes and configuring until no more errors or changes # press g to generate and exit make # if it has a test suite, good idea to use it make test make install If you need to rerun ccmake and reconfigure, remember to delete the CMakeCache.txt file, or you'll be wondering why your changes haven't taken. Turning on verbose Makefiles in ccmake is also useful if your code didn't compile first time - you'll be able to see what flags the compiler or linker is actually being given when it fails.","title":"CMake"},{"location":"Wiki_Export/Compiling/#make","text":"Your code may just come with a Makefile and have no configure, in which case the generic way to compile it is as follows: make targetname There's usually a default target, which make on its own will use. If you need to change any configuration options, you'll need to edit those sections of the Makefile (at the top, where the variables/flags are defined). Here are some typical variables you may want to change in a Makefile. These are what compilers/mpi wrappers to use - these are also defined by the compiler modules, so you can see what they should be. Intel would be icc, icpc, ifort, for example. If it's a program that can be compiled using MPI and only has a variable for CC, then set that to mpicc. CC=gcc CXX=g++ FC=gfortran MPICC=mpicc MPICXX=mpicxx MPIF90=mpif90 CFLAGS and LDFLAGS are flags for the compiler and linker respectively, and there might be LIBS or INCLUDE as well. When linking a library with the name libfoo, use -lfoo . CFLAGS=\"-I/path/to/include\" LDFLAGS=\"-L/path/to/foo/lib -L/path/to/bar/lib\" LDLIBS=\"-lfoo -lbar\" Remember to make clean first if you are recompiling with new options!","title":"Make"},{"location":"Wiki_Export/Compiling/#avx-instructions","text":"Note : Legion's current login nodes are of a newer architecture than some of the compute nodes - the login nodes have AVX instructions but the XYZ type nodes do not. This means if you want your code to run on the older nodes, some compiler options cannot be used (e.g. -march=native , -mtune , -xHost ) or your code will segfault. You can either build the code on the login nodes and restrict your jobs to only running on the newer nodes, compile the code with appropriate options for all the nodes, or compile your code inside a job that is running on the XYZ nodes (or during a qrsh session on the same).","title":"AVX instructions"},{"location":"Wiki_Export/Compiling/#intel-compilers","text":"To tell the Intel compilers to build for SSE4.2 instructions and no AVX, add this to CFLAGS (and CXXFLAGS if relevant): CFLAGS=-axSSE4.2 (Also see icc -help codegen ).","title":"Intel compilers"},{"location":"Wiki_Export/Compiling/#gnu-compilers","text":"To tell GCC to build for SSE4.2 without AVX, add this to CFLAGS (and CXXFLAGS if relevant): CFLAGS=-march=nehalem","title":"GNU compilers"},{"location":"Wiki_Export/Compiling/#restrict-node-type","text":"To restrict a job to newer nodes only, put this in your script: #$ -ac allow=LMNOPQSTU You can also compile one version without AVX and one with, so you can take advantage of the newer nodes when possible. You could use hostname in your jobscript to check what type of node you were on and run the appropriate binary. Hostnames begin with node-x for an X-type node, node-u for a U-type and so on.","title":"Restrict node type"},{"location":"Wiki_Export/Compiling/#test-for-avx","text":"We have a script that will let you test whether your compiled code is using AVX instructions. If you pass it a directory it will recursively test everything in there. Note that if your code builds multiple kernels and so can choose based on where it runs which instructions to use (like MKL) then this will find AVX instructions but they won't cause your code to segfault. find /home/username/path/ -perm /111 -type f | xargs /shared/ucl/apps/rcops_scripts/hasavx -q","title":"Test for AVX"},{"location":"Wiki_Export/Compiling/#blas-and-lapack","text":"BLAS and LAPACK are provided as part of MKL, OpenBLAS or ATLAS. There are several different OpenBLAS and ATLAS modules on Legion for different compilers. MKL is available in the Intel compiler module. Your code may try to link -lblas -llapack : this isn't the right way to use BLAS and LAPACK with MKL or ATLAS (our OpenBLAS now has symlinks that allow you to do this). MKL on Legion OpenBLAS on Legion ATLAS on Legion","title":"BLAS and LAPACK"},{"location":"Wiki_Export/Compiling/#set-your-path-and-other-environment-variables","text":"After you have installed your software, you'll need to add it to your PATH environment variable so you can run it without having to give the full path to its location. Put this in your ~/.bashrc file so it will set this with every new session you create. Replace username with your username and point to the directory your binary was built in (frequently program/bin ). This adds it to the front of your PATH, so if you install a newer version of something, it will be found before the system one. export PATH=/home/username/location/of/software/binary:$PATH If you built a library that you'll go on to compile other software with, you probably want to also add the lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH, and the include directory to CPATH (add export statements as above). This may mean your configure step will pick your library up correctly without any further effort on your part. To make these changes to your .bashrc take effect in your current session: source ~/.bashrc","title":"Set your PATH and other environment variables"},{"location":"Wiki_Export/Compiling/#python","text":"There are python2/recommended and python3/recommended bundles. These use a virtualenv and have pip set up for you. They both have numpy and scipy available.","title":"Python"},{"location":"Wiki_Export/Compiling/#set-compiler-module","text":"The Python versions on Legion were built with GCC. You can run them with the default Intel compilers loaded because everything depends on the gcc-libs/4.9.2 module. When you are building your own Python packages you should have the GCC compiler module loaded however, to avoid the situation where you build a package with the Intel compiler and then try to run it with GCC, in which case it will be unable to find Intel-specific instructions. module unload compilers module load compilers/gnu/4.9.2 If you get an error like this when trying to run something, recheck what compiler you used. undefined symbol: __intel_sse2_strrchr","title":"Set compiler module"},{"location":"Wiki_Export/Compiling/#install-your-own-packages-in-the-same-virtualenv","text":"This will use our central virtualenv, which contains a number of packages already installed. pip install --user <python2pkg> pip3 install --user <python3pkg> These will install into .python2local or .python3local directories in your home directory, respectively. To see what is already installed, the Python-shared list shows what is installed for both Python2 and 3, while the Python2 list and Python3 list show what is only installed for one or the other. (There may also be prereqs that aren't listed explicitly - pip will tell you if something is already installed as long as you have the recommended module bundle loaded).","title":"Install your own packages in the same virtualenv"},{"location":"Wiki_Export/Compiling/#use-your-own-virtualenvs","text":"If you need different packages that are not compatible with the central installs, you can create a new virtualenv and only yours will be available. virtualenv <DIR> source <DIR>/bin/activate Your bash prompt will show you that a different virtualenv is active.","title":"Use your own virtualenvs"},{"location":"Wiki_Export/Compiling/#installing-via-setuppy","text":"If you need to install using setup.py, you can use the --user flag and as long as one of the python bundles is loaded, it will install into the same .python2local or .python3local as pip and you won't need to add any new paths to your environment. python setup.py install --user You can alternatively use --prefix in which case you will have to set the install prefix to somewhere in your space, and also set PYTHONPATH and PATH to include your install location. Some installs won't create the prefix directory for you, in which case create it first. This is useful if you want to keep this package entirely separate and only in your paths on demand. export PYTHONPATH=/home/username/your/path/lib/python2.7/site-packages:$PYTHONPATH # if necessary, create install path mkdir -p home/username/your/path/lib/python2.7/site-packages python setup.py install --prefix=/home/username/your/path # add these to your .bashrc or jobscript export PYTHONPATH=/home/username/your/path/lib/python2.7/site-packages:$PYTHONPATH export PATH=/home/username/your/path/bin:$PATH Check that the PATH is where your Python executables were installed, and the PYTHONPATH is correct. It will tend to tell you at install time if you need to change or create the PYTHONPATH directory.","title":"Installing via setup.py"},{"location":"Wiki_Export/Compiling/#python-script-executable-paths","text":"If you have an executable python script giving the location of python like this, and it fails because that python doesn't exist in that location or isn't the one that has the additional packages installed: 1 #!/usr/bin/python2.6 You should change it so it uses the first python found in your environment. 1 #!/usr/bin/env python","title":"Python script executable paths"},{"location":"Wiki_Export/Compiling/#perl","text":"Perl modules will freqently have a Makefile.PL (especially if you download the tar files from CPAN.org yourself). You can install manually as: perl Makefile.PL PREFIX=/home/username/your/perl/location make make install","title":"Perl"},{"location":"Wiki_Export/Compiling/#cpan","text":"You can use CPAN to download and install modules locally for you. The first time you run the cpan command, it will create a .cpan directory for you and ask you to give it configuration settings or allow it to set them automatically. You need to tell it where you want your install prefix to be. If it is automatically configured, you need to edit these lines in your .cpan/CPAN/MyConfig.pm , for example if you want it to be in a lib directory in your home (change username to your own username): 'make_install_arg' => q[PREFIX=/home/username/lib] , # other lines in here 'makepl_arg' => q[PREFIX=/home/username/lib] , 'mbuild_install_arg' => q[PREFIX=/home/username/lib] , 'mbuildpl_arg' => q[--install_base /home/username/lib] , It will download and build modules inside .cpan and install them where you specified.","title":"CPAN"},{"location":"Wiki_Export/Compiling/#set-perl5lib-paths","text":"If you install your own Perl or Perl modules, you will need to append them to your PERL5LIB: export PERL5LIB=/home/username/your/perl/location:$PERL5LIB If you installed with CPAN, you may need to add several paths to this based on the layout it creates inside your nominated Perl directory.","title":"Set PERL5LIB paths"},{"location":"Wiki_Export/Compiling/#errors-when-using-non-default-perl-versions","text":"","title":"Errors when using non-default Perl versions"},{"location":"Wiki_Export/Compiling/#warningspm","text":"If you are using a version of Perl that is not the default system Perl and get strange errors when trying to run a Perl script, particularly ones about warnings.pm: Search pattern not terminated at /shared/ucl/apps/perl/5.20.0/lib/5.20.0/warnings.pm line 1099 then you need to edit the script so that instead of beginning with #!/usr/bin/perl , it begins with #!/usr/bin/env perl . Otherwise it will try to use the old system Perl libraries with your newer Perl executable, which won't work.","title":"warnings.pm"},{"location":"Wiki_Export/Compiling/#libperlso-not-found","text":"You probably built perl without telling it to build the shared library too. Add -Duseshrplib to your build flags.","title":"libperl.so not found"},{"location":"Wiki_Export/Compiling/#r","text":"There are instructions on installing and using local R packages in Using your own local R packages .","title":"R"},{"location":"Wiki_Export/Compiling/#compiling-with-mpi","text":"OpenMPI and Intel MPI are available. Certain programs do not work well with one or the other, so if you are having problems try the other one. Intel MPI is based on MPICH, so if the program you are compiling mentions that, try Intel MPI first. The Intel MPI is threadsafe; some versions of OpenMPI aren't. Note that OpenMPI 1.8.4 had a segv bug in non-blocking collectives that is fixed in OpenMPI 1.10.1.","title":"Compiling with MPI"},{"location":"Wiki_Export/Compiling/#enabling-openmp","text":"To enable OpenMP with the Intel compilers, you simply need to add -openmp to your compile line. With the GNU compilers you need to add -fopenmp .","title":"Enabling OpenMP"},{"location":"Wiki_Export/Compiling/#problems","text":"If you experience problems building your applications, please contact your local IT support in the first instance. We are available at rc-support AT ucl.ac.uk to help you if you still cannot build your app or if you need to report a problem with our software stack.","title":"Problems"},{"location":"Wiki_Export/Connecting_to_RC_Systems/","text":"Connecting to RC Systems \u00a7 After you have applied for and been granted access to our services , you can log in using a terminal application that supports the ssh access protocol. Aristotle is a teaching machine accessible to everyone with a UCL user ID and does not need to be applied for. Logging in from Linux or Mac OS X \u00a7 Use the terminal and run the below command to ssh into the correct machine. Replace ccaaxyz with your central UCL username. Legion \u00a7 ssh ccaaxyz@legion.rc.ucl.ac.uk Myriad \u00a7 ssh ccaaxyz@myriad.rc.ucl.ac.uk Grace \u00a7 ssh ccaaxyz@grace.rc.ucl.ac.uk Aristotle \u00a7 ssh ccaaxyz@aristotle.rc.ucl.ac.uk` You will then be asked to enter your UCL password. This user ID and password are those provided to you by Information Services Division . If you will want to run graphical applications, read on to \"Running graphical applications using X-forwarding\". Logging in from Windows \u00a7 On Windows you need something that will give you a suitable terminal and ssh - usually PuTTY, although you could also use Cygwin if you wanted a full Linux-like environment. Using PuTTY \u00a7 PuTTY is a common SSH client on Windows and is available on Desktop@UCL. You can find it under Start > All Programs > Applications O-P > PuTTY You will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY. Replace \"legion\" in the hostname with \"myriad\", \"grace\", or \"aristotle\" as appropriate. You will then have a screen come up that asks you for your username and password. Only enter your username, not \"@legion.rc.ucl.ac.uk\". The password field will remain entirely blank when you enter it - it does not show placeholders to indicate you have typed something. Login problems \u00a7 If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you still cannot get access but can access other UCL services like Socrates, please contact us on rc-support@ucl.ac.uk . If you cannot access anything, please see UCL MyAccount - you may need to request a password reset from the Service Desk. Accessing services from outside UCL \u00a7 If you wish to access any of our machines from outside UCL, you cannot do so directly as they are behind UCL's firewall. To do so you will have to either use ssh to connect to UCL's gateway first: ssh ccaaxyz@socrates.ucl.ac.uk and from there connect to the correct host as described above, or login first to your departmental gateway (if you have one) and then login from there. IS VPN Service \u00a7 Alternatively, you can use the ISD VPN service to connect to UCL using a virtual private network. This makes your computer a part of the UCL network, once the connection has been established, so you can establish an ssh connection to the host machine directly, for example: ssh ccaaxyz@legion.rc.ucl.ac.uk Running graphical applications using X-forwarding \u00a7 X-forwarding allows users to run a graphical program on a remote computer and display the user interface on their own computer. X-forwarding on Linux \u00a7 If you wish to have X-windows functionality enabled you have to make sure that you add either the -X or -Y flags (see man ssh for details) on all ssh commands you have to run to establish a connection to Legion. For example: ssh -X ccaaxyz@legion.rc.ucl.ac.uk To use X-Forwarding from outside UCL, you must either use the VPN, or the appropriate flags with both ssh steps, for example: [me@my_computer ~]$ ssh -X ccaaxyz@socrates.ucl.ac.uk [...] [ccaaxyz@socrates-a ~]$ ssh -X ccaaxyz@legion.rc.ucl.ac.uk X-forwarding on Mac OS X \u00a7 You will need to install XQuartz to provide an X-Window System for Mac OS X. (Previously known as X11.app). You can then follow the Linux instructions using the Mac OS X Terminal. X-forwarding on Windows \u00a7 You will need: An SSH client; e.g., PuTTY An X server program; e.g., Exceed, Xming Exceed is available on Desktop@UCL machines and downloadable from the UCL software database . Xming is open source (and mentioned here without testing). Exceed on Desktop@UCL \u00a7 Load Exceed. You can find it under Start > All Programs > Applications O-P > Open Text Exceed 14 > Exceed Open PuTTY (Applications O-P > PuTTY) In PuTTY, set up the connection with the host machine as usual: Host name: legion.rc.ucl.ac.uk (for example) Port: 22 Connection type: SSH Then, from the Category menu, select Connection > SSH > X11 for 'Options controlling SSH X11 forwarding' Make sure the box marked 'Enable X11 forwarding' is checked. Return to the session menu and save these settings with a new identifiable name for reuse in future. Click 'Open' and login to the host as usual To test that X-forwarding is working try one of these test applications: nedit: a text editor xeyes: to bring up a set of eyes that track the mouse position on the screen glxgears: to bring up an animated set of gears xclock: a clock If these work, you have successfully enabled X forwarding for graphical applications. (Note they may not be available on all systems). Installing Xming on your own computer \u00a7 Xming is a popular open source X server for Windows. These are instructions for using it alongside PuTTY. Other SSH clients and X servers are available. We cannot verify how well it may be working. Install both PuTTY and Xming if you have not done so already. During Xming installation, choose not to install an SSH client. Open Xming - the Xming icon should appear on the task bar. Open PuTTY Set up PuTTY as shown in the Exceed section. Transferring files \u00a7 Read on to Managing data on RC systems .","title":"Connecting to RC Systems"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#connecting-to-rc-systems","text":"After you have applied for and been granted access to our services , you can log in using a terminal application that supports the ssh access protocol. Aristotle is a teaching machine accessible to everyone with a UCL user ID and does not need to be applied for.","title":"Connecting to RC Systems"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#logging-in-from-linux-or-mac-os-x","text":"Use the terminal and run the below command to ssh into the correct machine. Replace ccaaxyz with your central UCL username.","title":"Logging in from Linux or Mac OS X"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#legion","text":"ssh ccaaxyz@legion.rc.ucl.ac.uk","title":"Legion"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#myriad","text":"ssh ccaaxyz@myriad.rc.ucl.ac.uk","title":"Myriad"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#grace","text":"ssh ccaaxyz@grace.rc.ucl.ac.uk","title":"Grace"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#aristotle","text":"ssh ccaaxyz@aristotle.rc.ucl.ac.uk` You will then be asked to enter your UCL password. This user ID and password are those provided to you by Information Services Division . If you will want to run graphical applications, read on to \"Running graphical applications using X-forwarding\".","title":"Aristotle"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#logging-in-from-windows","text":"On Windows you need something that will give you a suitable terminal and ssh - usually PuTTY, although you could also use Cygwin if you wanted a full Linux-like environment.","title":"Logging in from Windows"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#using-putty","text":"PuTTY is a common SSH client on Windows and is available on Desktop@UCL. You can find it under Start > All Programs > Applications O-P > PuTTY You will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY. Replace \"legion\" in the hostname with \"myriad\", \"grace\", or \"aristotle\" as appropriate. You will then have a screen come up that asks you for your username and password. Only enter your username, not \"@legion.rc.ucl.ac.uk\". The password field will remain entirely blank when you enter it - it does not show placeholders to indicate you have typed something.","title":"Using PuTTY"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#login-problems","text":"If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you still cannot get access but can access other UCL services like Socrates, please contact us on rc-support@ucl.ac.uk . If you cannot access anything, please see UCL MyAccount - you may need to request a password reset from the Service Desk.","title":"Login problems"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#accessing-services-from-outside-ucl","text":"If you wish to access any of our machines from outside UCL, you cannot do so directly as they are behind UCL's firewall. To do so you will have to either use ssh to connect to UCL's gateway first: ssh ccaaxyz@socrates.ucl.ac.uk and from there connect to the correct host as described above, or login first to your departmental gateway (if you have one) and then login from there.","title":"Accessing services from outside UCL"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#is-vpn-service","text":"Alternatively, you can use the ISD VPN service to connect to UCL using a virtual private network. This makes your computer a part of the UCL network, once the connection has been established, so you can establish an ssh connection to the host machine directly, for example: ssh ccaaxyz@legion.rc.ucl.ac.uk","title":"IS VPN Service"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#running-graphical-applications-using-x-forwarding","text":"X-forwarding allows users to run a graphical program on a remote computer and display the user interface on their own computer.","title":"Running graphical applications using X-forwarding"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#x-forwarding-on-linux","text":"If you wish to have X-windows functionality enabled you have to make sure that you add either the -X or -Y flags (see man ssh for details) on all ssh commands you have to run to establish a connection to Legion. For example: ssh -X ccaaxyz@legion.rc.ucl.ac.uk To use X-Forwarding from outside UCL, you must either use the VPN, or the appropriate flags with both ssh steps, for example: [me@my_computer ~]$ ssh -X ccaaxyz@socrates.ucl.ac.uk [...] [ccaaxyz@socrates-a ~]$ ssh -X ccaaxyz@legion.rc.ucl.ac.uk","title":"X-forwarding on Linux"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#x-forwarding-on-mac-os-x","text":"You will need to install XQuartz to provide an X-Window System for Mac OS X. (Previously known as X11.app). You can then follow the Linux instructions using the Mac OS X Terminal.","title":"X-forwarding on Mac OS X"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#x-forwarding-on-windows","text":"You will need: An SSH client; e.g., PuTTY An X server program; e.g., Exceed, Xming Exceed is available on Desktop@UCL machines and downloadable from the UCL software database . Xming is open source (and mentioned here without testing).","title":"X-forwarding on Windows"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#exceed-on-desktopucl","text":"Load Exceed. You can find it under Start > All Programs > Applications O-P > Open Text Exceed 14 > Exceed Open PuTTY (Applications O-P > PuTTY) In PuTTY, set up the connection with the host machine as usual: Host name: legion.rc.ucl.ac.uk (for example) Port: 22 Connection type: SSH Then, from the Category menu, select Connection > SSH > X11 for 'Options controlling SSH X11 forwarding' Make sure the box marked 'Enable X11 forwarding' is checked. Return to the session menu and save these settings with a new identifiable name for reuse in future. Click 'Open' and login to the host as usual To test that X-forwarding is working try one of these test applications: nedit: a text editor xeyes: to bring up a set of eyes that track the mouse position on the screen glxgears: to bring up an animated set of gears xclock: a clock If these work, you have successfully enabled X forwarding for graphical applications. (Note they may not be available on all systems).","title":"Exceed on Desktop@UCL"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#installing-xming-on-your-own-computer","text":"Xming is a popular open source X server for Windows. These are instructions for using it alongside PuTTY. Other SSH clients and X servers are available. We cannot verify how well it may be working. Install both PuTTY and Xming if you have not done so already. During Xming installation, choose not to install an SSH client. Open Xming - the Xming icon should appear on the task bar. Open PuTTY Set up PuTTY as shown in the Exceed section.","title":"Installing Xming on your own computer"},{"location":"Wiki_Export/Connecting_to_RC_Systems/#transferring-files","text":"Read on to Managing data on RC systems .","title":"Transferring files"},{"location":"Wiki_Export/Connecting_to_Research_Data_Services/","text":"The Research Data Services group (RDS) run a number of systems designed to help with data storage during and after a project. Several solutions for copying data between RDS and each of the central UCL research computing platforms are presented below. Sections of the example code surrounded by angle brackets (\\<>) should be replaced by the information indicated (do not keep the angle brackets in). Between Legion and RDS \u00a7 If you already have an account with Research Data Services, you can transfer data directly between Legion and Research Data Storage using the Secure Copy ( scp ) command. From RDS to Legion \u00a7 If you are on a research data login node, you can transfer data to Legion\u2019s Scratch area at the highest rate currently possible by running the command: scp data_file.tgz login05.external.legion.ucl.ac.uk:~/Scratch/ Or from somewhere within Legion (including compute nodes in running jobs) running the command: scp ssh.rd.ucl.ac.uk:~/data_file.tgz ~/Scratch/ From Legion to RDS \u00a7 From Legion, send data to your project space on RDS by running the command: scp data_file.tgz ccaaxyz@ssh.rd.ucl.ac.uk:<path_to_project_space> The RDS support pages provide more information: http://www.ucl.ac.uk/isd/services/research-it/research-data/storage/access-guide","title":"Connecting to Research Data Services"},{"location":"Wiki_Export/Connecting_to_Research_Data_Services/#between-legion-and-rds","text":"If you already have an account with Research Data Services, you can transfer data directly between Legion and Research Data Storage using the Secure Copy ( scp ) command.","title":"Between Legion and RDS"},{"location":"Wiki_Export/Connecting_to_Research_Data_Services/#from-rds-to-legion","text":"If you are on a research data login node, you can transfer data to Legion\u2019s Scratch area at the highest rate currently possible by running the command: scp data_file.tgz login05.external.legion.ucl.ac.uk:~/Scratch/ Or from somewhere within Legion (including compute nodes in running jobs) running the command: scp ssh.rd.ucl.ac.uk:~/data_file.tgz ~/Scratch/","title":"From RDS to Legion"},{"location":"Wiki_Export/Connecting_to_Research_Data_Services/#from-legion-to-rds","text":"From Legion, send data to your project space on RDS by running the command: scp data_file.tgz ccaaxyz@ssh.rd.ucl.ac.uk:<path_to_project_space> The RDS support pages provide more information: http://www.ucl.ac.uk/isd/services/research-it/research-data/storage/access-guide","title":"From Legion to RDS"},{"location":"Wiki_Export/Example_Submission_Scripts/","text":"On this page we describe some basic example scripts to submit jobs to Legion or Grace. For a full description of data management policies, please refer to the data management section of the user guide. After creating your script, submit it to the scheduler with: qsub my_script.sh Service Differences \u00a7 These scripts are all applicable to Legion , Grace , Myriad , Thomas , and Michael , but node sizes (core count, memory, and temporary storage sizes) differ between the different machines, and Grace and Thomas are more suited to MPI and hybrid MPI/OpenMP jobs. Working Directories and Output \u00a7 The parallel filesystems we use to provide the home and scratch filesystems perform best when reading or writing single large files, and worst when operating on many different small files. To avoid causing problems, many of the scripts below are written to create all their files in the temporary TMPDIR storage, and compress and copy them to the scratch area at the end of the job. This can be a problem if your job is not finishing and you need to see the output, or if your job is crashing or failing to produce what you expected. Feel free to modify the scripts to read from or write to scratch directly, however, your performance will generally not be as good as writing to TMPDIR, and you may impact the general performance of the machine if you do this with many jobs simultaneously. Note that there is also the option of using the Local2Scratch process ( see below ), which takes place after the job has finished, in the clean-up step. This gives you the option of always getting the contents of TMPDIR back, at the cost of possibly getting incomplete files and not having any control over where the files go. Note about Projects \u00a7 Projects are a system used in the scheduler and the accounting system to track budgets and access controls. The majority of users of UCL's internal clusters - Legion, Grace, and Myriad - will not need to specify a project and will default to the AllUsers project. Users of the Thomas and Michael services should refer to the specific pages for those machines, and the information they were given when they registered. To specify a project ID in a job script, use the -P object as below: #$ -P <your_project_id> Serial Job Script Example \u00a7 The most basic type of job a user can submit to the Legion cluster is a serial job. These jobs run on a single processor with a single thread. Shown below is a simple job script that runs /bin/date (which prints the current date) on the compute node, and puts the output into the job's output file. #!/bin/bash -l # Batch script to run a serial job on Legion under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Serial_Job # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/workspace # Your work should be done in $TMPDIR cd $TMPDIR # Run the application. /bin/date > date.txt # Preferably, tar-up (archive) all output files onto the shared scratch area tar -zcvf $HOME /Scratch/files_from_job_ $JOB_ID .tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete! Multi-threaded Job Example \u00a7 For programs that can use multiple threads, you can request multiple processor cores using the -pe smp <number> option. One common method for using multiple threads in a program is OpenMP, and the OMP_NUM_THREADS environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though ( via a mechanism called cgroups ). Note that this job script works directly in scratch instead of in the temporary TMPDIR storage. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash -l # Batch script to run an OpenMP threaded job on Legion under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM for each core/thread (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Multi-threaded Job # Request 16 cores. #$ -pe smp 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # 8. Run the application. $HOME/my_program/example MPI Job Script Example \u00a7 The default MPI implementation on our clusters is the Intel MPI stack. MPI programs don\u2019t use a shared memory model so they can be run across multiple nodes. This script differs considerably from the serial and OpenMP jobs in that MPI programs need to be invoked by a program called gerun. This is a wrapper for mpirun and takes care of passing the number of processors and a file called a machine file. '''Important''': If you wish to pass a file or stream of data to the standard input (stdin) of an MPI program, there are specific command-line options you need to use to control which MPI tasks are able to receive it. ( -s for Intel MPI, --stdin for OpenMPI.) Please consult the help output of the mpirun command for further information. The gerun launcher does not automatically handle this. If you use OpenMPI, you need to make sure the Intel mpi modules are removed and the OpenMPI modules are loaded, either in your shell start-up files (e.g. ~/.bashrc ), or else in the script itself. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash -l # Batch script to run an MPI parallel job under SGE with Intel MPI. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N MadScience_1_16 # Select the MPI parallel environment and 16 processes. #$ -pe mpi 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID : #$ -wd /home/<your_UCL_id>/Scratch/output # Run our MPI job. GERun is a wrapper that launches MPI jobs on our clusters. gerun $HOME/src/science/simulate Array Job Script Example \u00a7 If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the -t option to get Sun Grid Engine to run 10,000 copies of this job numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the SGE_TASK_ID environment variable in each task. All the usual SGE output files have the task ID appended. MPI jobs and parallel shared memory jobs can also be submitted as arrays. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash -l # Batch script to run a serial array job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N MyArrayJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Run the application. echo \"$JOB_NAME $SGE_TASK_ID\" Array Job Script Example Using Parameter File \u00a7 Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces. For example: 0001 1.5 3 aardvark 0002 1.1 13 guppy 0003 1.23 5 elephant 0004 1.112 23 panda 0005 ... Assuming that this file is stored in ~/Scratch/input/params.txt (you can call this file anything you want) then the user can use awk/sed to get the appropriate variables out of the file as with the script below which stores them in $index , $variable1 , $variable2 and $variable3 . So for example in task 4, $index = 0004 , $variable1 = 1.112 , $variable2 = 23 and $variable3 = panda . Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash -l # Batch script to run an array job. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 1000 tasks # numbered 1 to 1000. #$ -t 1-1000 # Set the name of the job. #$ -N array-params # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Parse parameter file to get variables. number=$SGE_TASK_ID paramfile=/home/<your_UCL_id>/Scratch/input/params.txt index=\"`sed -n ${number}p $paramfile | awk '{print $1}'`\" variable1=\"`sed -n ${number}p $paramfile | awk '{print $2}'`\" variable2=\"`sed -n ${number}p $paramfile | awk '{print $3}'`\" variable3=\"`sed -n ${number}p $paramfile | awk '{print $4}'`\" # Run the program (replace echo with your binary and options). echo \"$index\" \"$variable1\" \"$variable2\" \"$variable3\" Example Array Job Using Local2Scratch \u00a7 Users can automate the transfer of data from $TMPDIR to their scratch space by adding the text #Local2Scratch to their script on a line alone as a special comment. During the clean-up phase of the job, a tool checks whether the script contains that text, and if so, files are transferred from $TMPDIR to a directory in scratch with the structure <job id>/<job id>.<task id>.<queue>/ . The example below does this for a job array, but this works for any job type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash -l # Batch script to run an array job under SGE and # transfer the output to Scratch from local. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N local2scratcharray # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Run the application in TMPDIR. cd $TMPDIR hostname > hostname.txt Array Job Script with a Stride \u00a7 If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task. Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs. For example, a stride of 10 will give you these task IDs: 1, 11, 21... Your script can then have a loop that runs task IDs from $SGE_TASK_ID to $SGE_TASK_ID + 9 , so each task is doing ten times as many runs as it was before. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash -l # Batch script to run an array job with strided task IDs under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested task IDs # numbered 1 to 10000 with a stride of 10. #$ -t 1-10000:10 # Set the name of the job. #$ -N arraystride # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Do your work in $TMPDIR cd $TMPDIR # 10. Loop through the IDs covered by this stride and run the application if # the input file exists. (This is because the last stride may not have that # many inputs available). Or you can leave out the check and get an error. for (( i=$SGE_TASK_ID; i<$SGE_TASK_ID+10; i++ )) do if [ -f \"input.$i\" ] then echo \"$JOB_NAME\" \"$SGE_TASK_ID\" \"input.$i\" fi done GPU Job Script Example \u00a7 To use NVIDIA GPUs with the CUDA libraries, you need to load the CUDA runtime libraries module or else set up the environment yourself. The script below shows what you'll need to unload and load the appropriate modules. You also need to use the -l gpu=<number> option to request the GPUs from the scheduler. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash -l # Batch script to run a GPU job on Legion under SGE. # Request a number of GPU cards, in this case 2 (the maximum) #$ -l gpu=2 # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Change into temporary directory to run work cd $TMPDIR # load the cuda module (in case you are running a CUDA program module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18/gnu-4.9.2 # Run the application - the line below is just a random example. mygpucode # 10. Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME/Scratch/files_from_job_$JOB_ID.tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete! Job using MPI and GPUs \u00a7 It is possible to run MPI programs that use GPUs but our clusters currently only support this within a single node. The script below shows how to run a program using 2 gpus and 12 cpus. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash -l # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 12 cores, 2 GPUs, 1 gigabyte of RAM per CPU, 15 gigabyte of TMPDIR space #$ -l mem=1G #$ -l gpu=2 #$ -pe mpi 12 #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUMPIrun # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your user id>/Scratch/output/ # Run our MPI job. You can choose OpenMPI or IntelMPI for GCC. module unload compilers mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.10.1/gnu-4.9.2 module load cuda/7.5.18/gnu-4.9.2 gerun myGPUapp","title":"Example Submission Scripts"},{"location":"Wiki_Export/Example_Submission_Scripts/#service-differences","text":"These scripts are all applicable to Legion , Grace , Myriad , Thomas , and Michael , but node sizes (core count, memory, and temporary storage sizes) differ between the different machines, and Grace and Thomas are more suited to MPI and hybrid MPI/OpenMP jobs.","title":"Service Differences"},{"location":"Wiki_Export/Example_Submission_Scripts/#working-directories-and-output","text":"The parallel filesystems we use to provide the home and scratch filesystems perform best when reading or writing single large files, and worst when operating on many different small files. To avoid causing problems, many of the scripts below are written to create all their files in the temporary TMPDIR storage, and compress and copy them to the scratch area at the end of the job. This can be a problem if your job is not finishing and you need to see the output, or if your job is crashing or failing to produce what you expected. Feel free to modify the scripts to read from or write to scratch directly, however, your performance will generally not be as good as writing to TMPDIR, and you may impact the general performance of the machine if you do this with many jobs simultaneously. Note that there is also the option of using the Local2Scratch process ( see below ), which takes place after the job has finished, in the clean-up step. This gives you the option of always getting the contents of TMPDIR back, at the cost of possibly getting incomplete files and not having any control over where the files go.","title":"Working Directories and Output"},{"location":"Wiki_Export/Example_Submission_Scripts/#note-about-projects","text":"Projects are a system used in the scheduler and the accounting system to track budgets and access controls. The majority of users of UCL's internal clusters - Legion, Grace, and Myriad - will not need to specify a project and will default to the AllUsers project. Users of the Thomas and Michael services should refer to the specific pages for those machines, and the information they were given when they registered. To specify a project ID in a job script, use the -P object as below: #$ -P <your_project_id>","title":"Note about Projects"},{"location":"Wiki_Export/Example_Submission_Scripts/#serial-job-script-example","text":"The most basic type of job a user can submit to the Legion cluster is a serial job. These jobs run on a single processor with a single thread. Shown below is a simple job script that runs /bin/date (which prints the current date) on the compute node, and puts the output into the job's output file. #!/bin/bash -l # Batch script to run a serial job on Legion under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer followed by M, G, or T) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Serial_Job # Set the working directory to somewhere in your scratch space. # This is a necessary step as compute nodes cannot write to $HOME. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/workspace # Your work should be done in $TMPDIR cd $TMPDIR # Run the application. /bin/date > date.txt # Preferably, tar-up (archive) all output files onto the shared scratch area tar -zcvf $HOME /Scratch/files_from_job_ $JOB_ID .tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete!","title":"Serial Job Script Example"},{"location":"Wiki_Export/Example_Submission_Scripts/#multi-threaded-job-example","text":"For programs that can use multiple threads, you can request multiple processor cores using the -pe smp <number> option. One common method for using multiple threads in a program is OpenMP, and the OMP_NUM_THREADS environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though ( via a mechanism called cgroups ). Note that this job script works directly in scratch instead of in the temporary TMPDIR storage. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash -l # Batch script to run an OpenMP threaded job on Legion under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM for each core/thread (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N Multi-threaded Job # Request 16 cores. #$ -pe smp 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # 8. Run the application. $HOME/my_program/example","title":"Multi-threaded Job Example"},{"location":"Wiki_Export/Example_Submission_Scripts/#mpi-job-script-example","text":"The default MPI implementation on our clusters is the Intel MPI stack. MPI programs don\u2019t use a shared memory model so they can be run across multiple nodes. This script differs considerably from the serial and OpenMP jobs in that MPI programs need to be invoked by a program called gerun. This is a wrapper for mpirun and takes care of passing the number of processors and a file called a machine file. '''Important''': If you wish to pass a file or stream of data to the standard input (stdin) of an MPI program, there are specific command-line options you need to use to control which MPI tasks are able to receive it. ( -s for Intel MPI, --stdin for OpenMPI.) Please consult the help output of the mpirun command for further information. The gerun launcher does not automatically handle this. If you use OpenMPI, you need to make sure the Intel mpi modules are removed and the OpenMPI modules are loaded, either in your shell start-up files (e.g. ~/.bashrc ), or else in the script itself. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash -l # Batch script to run an MPI parallel job under SGE with Intel MPI. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N MadScience_1_16 # Select the MPI parallel environment and 16 processes. #$ -pe mpi 16 # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID : #$ -wd /home/<your_UCL_id>/Scratch/output # Run our MPI job. GERun is a wrapper that launches MPI jobs on our clusters. gerun $HOME/src/science/simulate","title":"MPI Job Script Example"},{"location":"Wiki_Export/Example_Submission_Scripts/#array-job-script-example","text":"If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the -t option to get Sun Grid Engine to run 10,000 copies of this job numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the SGE_TASK_ID environment variable in each task. All the usual SGE output files have the task ID appended. MPI jobs and parallel shared memory jobs can also be submitted as arrays. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash -l # Batch script to run a serial array job under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N MyArrayJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Run the application. echo \"$JOB_NAME $SGE_TASK_ID\"","title":"Array Job Script Example"},{"location":"Wiki_Export/Example_Submission_Scripts/#array-job-script-example-using-parameter-file","text":"Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces. For example: 0001 1.5 3 aardvark 0002 1.1 13 guppy 0003 1.23 5 elephant 0004 1.112 23 panda 0005 ... Assuming that this file is stored in ~/Scratch/input/params.txt (you can call this file anything you want) then the user can use awk/sed to get the appropriate variables out of the file as with the script below which stores them in $index , $variable1 , $variable2 and $variable3 . So for example in task 4, $index = 0004 , $variable1 = 1.112 , $variable2 = 23 and $variable3 = panda . Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash -l # Batch script to run an array job. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 1000 tasks # numbered 1 to 1000. #$ -t 1-1000 # Set the name of the job. #$ -N array-params # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Parse parameter file to get variables. number=$SGE_TASK_ID paramfile=/home/<your_UCL_id>/Scratch/input/params.txt index=\"`sed -n ${number}p $paramfile | awk '{print $1}'`\" variable1=\"`sed -n ${number}p $paramfile | awk '{print $2}'`\" variable2=\"`sed -n ${number}p $paramfile | awk '{print $3}'`\" variable3=\"`sed -n ${number}p $paramfile | awk '{print $4}'`\" # Run the program (replace echo with your binary and options). echo \"$index\" \"$variable1\" \"$variable2\" \"$variable3\"","title":"Array Job Script Example Using Parameter File"},{"location":"Wiki_Export/Example_Submission_Scripts/#example-array-job-using-local2scratch","text":"Users can automate the transfer of data from $TMPDIR to their scratch space by adding the text #Local2Scratch to their script on a line alone as a special comment. During the clean-up phase of the job, a tool checks whether the script contains that text, and if so, files are transferred from $TMPDIR to a directory in scratch with the structure <job id>/<job id>.<task id>.<queue>/ . The example below does this for a job array, but this works for any job type. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash -l # Batch script to run an array job under SGE and # transfer the output to Scratch from local. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested 10000 tasks # numbered 1 to 10000. #$ -t 1-10000 # Set the name of the job. #$ -N local2scratcharray # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Run the application in TMPDIR. cd $TMPDIR hostname > hostname.txt","title":"Example Array Job Using Local2Scratch"},{"location":"Wiki_Export/Example_Submission_Scripts/#array-job-script-with-a-stride","text":"If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task. Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs. For example, a stride of 10 will give you these task IDs: 1, 11, 21... Your script can then have a loop that runs task IDs from $SGE_TASK_ID to $SGE_TASK_ID + 9 , so each task is doing ten times as many runs as it was before. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash -l # Batch script to run an array job with strided task IDs under SGE. # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set up the job array. In this instance we have requested task IDs # numbered 1 to 10000 with a stride of 10. #$ -t 1-10000:10 # Set the name of the job. #$ -N arraystride # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Automate transfer of output to Scratch from $TMPDIR. #Local2Scratch # Do your work in $TMPDIR cd $TMPDIR # 10. Loop through the IDs covered by this stride and run the application if # the input file exists. (This is because the last stride may not have that # many inputs available). Or you can leave out the check and get an error. for (( i=$SGE_TASK_ID; i<$SGE_TASK_ID+10; i++ )) do if [ -f \"input.$i\" ] then echo \"$JOB_NAME\" \"$SGE_TASK_ID\" \"input.$i\" fi done","title":"Array Job Script with a Stride"},{"location":"Wiki_Export/Example_Submission_Scripts/#gpu-job-script-example","text":"To use NVIDIA GPUs with the CUDA libraries, you need to load the CUDA runtime libraries module or else set up the environment yourself. The script below shows what you'll need to unload and load the appropriate modules. You also need to use the -l gpu=<number> option to request the GPUs from the scheduler. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash -l # Batch script to run a GPU job on Legion under SGE. # Request a number of GPU cards, in this case 2 (the maximum) #$ -l gpu=2 # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUJob # Set the working directory to somewhere in your scratch space. # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/output # Change into temporary directory to run work cd $TMPDIR # load the cuda module (in case you are running a CUDA program module unload compilers mpi module load compilers/gnu/4.9.2 module load cuda/7.5.18/gnu-4.9.2 # Run the application - the line below is just a random example. mygpucode # 10. Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME/Scratch/files_from_job_$JOB_ID.tar.gz $TMPDIR # Make sure you have given enough time for the copy to complete!","title":"GPU Job Script Example"},{"location":"Wiki_Export/Example_Submission_Scripts/#job-using-mpi-and-gpus","text":"It is possible to run MPI programs that use GPUs but our clusters currently only support this within a single node. The script below shows how to run a program using 2 gpus and 12 cpus. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash -l # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 12 cores, 2 GPUs, 1 gigabyte of RAM per CPU, 15 gigabyte of TMPDIR space #$ -l mem=1G #$ -l gpu=2 #$ -pe mpi 12 #$ -l tmpfs=15G # Set the name of the job. #$ -N GPUMPIrun # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your user id>/Scratch/output/ # Run our MPI job. You can choose OpenMPI or IntelMPI for GCC. module unload compilers mpi module load compilers/gnu/4.9.2 module load mpi/openmpi/1.10.1/gnu-4.9.2 module load cuda/7.5.18/gnu-4.9.2 gerun myGPUapp","title":"Job using MPI and GPUs"},{"location":"Wiki_Export/FAQ/","text":"Frequently-Asked Questions \u00a7 This page attempts to address some of the topics we most frequently receive questions about, or to which the answers are most complex. Why is my job in Eqw status? \u00a7 If your job goes straight into Eqw state, there was an error in your jobscript that meant your job couldn't be started. The standard qstat job information command will give you a truncated version of the error: qstat -j <job_ID> To see the full error instead: qexplain <job_ID> The qexplain script is part of our userscripts set -- if you try to use it and get an error that it doesn't exist, load the userscripts module: module load userscripts The most common reason jobs go into this error state is that a file or directory your job is trying to use doesn't exist. Creating it after the job is in the Eqw state won't make the job run: it'll still have to be deleted and re-submitted. \"Unable to determine job requirements\" error \u00a7 Unable to run job: Rejected by ucl_jsv4h Reason:Unable to determine job requirements. Exiting. The #$ directives are missing from your script, or have extra white space before them. This means SGE isn't picking them up and doesn't know what resources you are requesting. Add them or remove the spaces and it will work. \"/bin/bash: invalid option\" error \u00a7 This is a sign that your jobscript is a DOS-formatted text file and not a Unix one - the line break characters are different. Type dos2unix <yourscriptname> in your terminal to convert it. Sometimes the offending characters will be visible in the error. You can see here it's trying to parse ^M as an option. Your Scratch space goes missing \u00a7 You may have accidentally deleted or replaced the link to your Scratch space. Do an ls -al in your home - if set up correctly, it should look like this: lrwxrwxrwx 1 username private 24 Apr 14 2014 Scratch -> /scratch/scratch/username where username is your UCL user ID. You can recreate the symlink with ln -s /scratch/scratch/username Scratch Which MKL library files should I use to build my application? \u00a7 Depending on which whether you wish to use BLAS/LAPACK/ScaLAPACK/etc... there is a specific set of libraries that you need to pass to your compilation command line. Fortunately, Intel have released a tool that allows you to determine which libraries to link and in which order for a number of compilers and operating systems: http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/ See also: MKL_on_Legion . SSH known_hosts \u00a7 1. If you get a warning when connecting in via ssh, we may have updated the login nodes, and you probably need to delete old host keys from your ~/.ssh/known_hosts . You can also delete the whole file, and the ssh command will recreate it (asking you to check) next time you try to connect. 2. If you look in the error file for your job, you may find a number of errors like the one below. Please ignore these as they are the result of compute nodes being unable to write to your home directory and do not indicate a problem. Failed to add the RSA host key for IP address '10.143.9.1' to the list of known hosts (/home/uccaoke/.ssh/known_hosts) \"ssh: Unsupported option - -x\" errors \u00a7 These errors indicate that you are attempting to use the QLogic version of mpirun in the OpenMPI parallel environment. It is likely you are doing this by accident and probably intend to use the OpenMPI mpirun but do not have your modules configured correctly. Please add the lines below, either after default modules (defmods) are loaded in your .bashrc, or else in your job script before mpirun: Note : the above assumes you are using the Intel compilers. You get \"Program not started through mpirun. Exiting...\" but are using mpirun! \u00a7 This is most often caused by launching a program built with QLogic MPI with the mpirun from another MPI implementation (e.g. OpenMPI). You can determine which version of MPI your program was built with by running ldd on the application binary. You want to know where the libraries loaded via modules system are on disk \u00a7 Look at the contents of the default modules to find the path to those libraries on the current system. Look at the following command listing: As you can see, the modules system sets the paths to libraries in environment variables which the system uses to locate files. Unable to run job: JSV stderr: perl: warning: Setting locale failed. \u00a7 This error is generally because your SSH client is passing LANG through as part of the SSH command, and is passing something that conflicts with what Legion has it set to. You may be more likely to come across this with newer versions of Mac OS X - if your client is different, have a look for an equivalent option. In Mac OS X Terminal, click Settings and under International untick the box that says \"Set locale environment variables on startup\". Per session, you can try LANG=C ssh userid@legion.rc.ucl.ac.uk Why can't I find out when my job will run? \u00a7 An informative discussion on this matter can be found in the Scheduler section of the User Guide. What can I do to minimise the time I need to wait for my job(s) to run? \u00a7 Minimise the amount of wall clock time you request. Use job arrays instead of submitting large numbers of jobs (see our job script examples ). Plan your work so that you can do other things while your jobs are being scheduled - the rule of thumb is that you will have to wait about twice the requested wall clock time (on average). What is my project code (short string) / project ID? \u00a7 Prior to July 2014, every user had a project code. Now all users belong to the default project \"AllUsers\" and no longer have to specify this. If you see older job script examples mentioning a project ID, you can delete that section. Only projects with access to paid or specialised resources need to give a project code in order to use those resources. If you do not know yours, contact rc-support .","title":"FAQ"},{"location":"Wiki_Export/FAQ/#frequently-asked-questions","text":"This page attempts to address some of the topics we most frequently receive questions about, or to which the answers are most complex.","title":"Frequently-Asked Questions"},{"location":"Wiki_Export/FAQ/#why-is-my-job-in-eqw-status","text":"If your job goes straight into Eqw state, there was an error in your jobscript that meant your job couldn't be started. The standard qstat job information command will give you a truncated version of the error: qstat -j <job_ID> To see the full error instead: qexplain <job_ID> The qexplain script is part of our userscripts set -- if you try to use it and get an error that it doesn't exist, load the userscripts module: module load userscripts The most common reason jobs go into this error state is that a file or directory your job is trying to use doesn't exist. Creating it after the job is in the Eqw state won't make the job run: it'll still have to be deleted and re-submitted.","title":"Why is my job in Eqw status?"},{"location":"Wiki_Export/FAQ/#unable-to-determine-job-requirements-error","text":"Unable to run job: Rejected by ucl_jsv4h Reason:Unable to determine job requirements. Exiting. The #$ directives are missing from your script, or have extra white space before them. This means SGE isn't picking them up and doesn't know what resources you are requesting. Add them or remove the spaces and it will work.","title":"\"Unable to determine job requirements\" error"},{"location":"Wiki_Export/FAQ/#binbash-invalid-option-error","text":"This is a sign that your jobscript is a DOS-formatted text file and not a Unix one - the line break characters are different. Type dos2unix <yourscriptname> in your terminal to convert it. Sometimes the offending characters will be visible in the error. You can see here it's trying to parse ^M as an option.","title":"\"/bin/bash: invalid option\" error"},{"location":"Wiki_Export/FAQ/#your-scratch-space-goes-missing","text":"You may have accidentally deleted or replaced the link to your Scratch space. Do an ls -al in your home - if set up correctly, it should look like this: lrwxrwxrwx 1 username private 24 Apr 14 2014 Scratch -> /scratch/scratch/username where username is your UCL user ID. You can recreate the symlink with ln -s /scratch/scratch/username Scratch","title":"Your Scratch space goes missing"},{"location":"Wiki_Export/FAQ/#which-mkl-library-files-should-i-use-to-build-my-application","text":"Depending on which whether you wish to use BLAS/LAPACK/ScaLAPACK/etc... there is a specific set of libraries that you need to pass to your compilation command line. Fortunately, Intel have released a tool that allows you to determine which libraries to link and in which order for a number of compilers and operating systems: http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/ See also: MKL_on_Legion .","title":"Which MKL library files should I use to build my application?"},{"location":"Wiki_Export/FAQ/#ssh-known_hosts","text":"1. If you get a warning when connecting in via ssh, we may have updated the login nodes, and you probably need to delete old host keys from your ~/.ssh/known_hosts . You can also delete the whole file, and the ssh command will recreate it (asking you to check) next time you try to connect. 2. If you look in the error file for your job, you may find a number of errors like the one below. Please ignore these as they are the result of compute nodes being unable to write to your home directory and do not indicate a problem. Failed to add the RSA host key for IP address '10.143.9.1' to the list of known hosts (/home/uccaoke/.ssh/known_hosts)","title":"SSH known_hosts"},{"location":"Wiki_Export/FAQ/#ssh-unsupported-option-x-errors","text":"These errors indicate that you are attempting to use the QLogic version of mpirun in the OpenMPI parallel environment. It is likely you are doing this by accident and probably intend to use the OpenMPI mpirun but do not have your modules configured correctly. Please add the lines below, either after default modules (defmods) are loaded in your .bashrc, or else in your job script before mpirun: Note : the above assumes you are using the Intel compilers.","title":"\"ssh: Unsupported option - -x\" errors"},{"location":"Wiki_Export/FAQ/#you-get-program-not-started-through-mpirun-exiting-but-are-using-mpirun","text":"This is most often caused by launching a program built with QLogic MPI with the mpirun from another MPI implementation (e.g. OpenMPI). You can determine which version of MPI your program was built with by running ldd on the application binary.","title":"You get \"Program not started through mpirun. Exiting...\" but are using mpirun!"},{"location":"Wiki_Export/FAQ/#you-want-to-know-where-the-libraries-loaded-via-modules-system-are-on-disk","text":"Look at the contents of the default modules to find the path to those libraries on the current system. Look at the following command listing: As you can see, the modules system sets the paths to libraries in environment variables which the system uses to locate files.","title":"You want to know where the libraries loaded via modules system are on disk"},{"location":"Wiki_Export/FAQ/#unable-to-run-job-jsv-stderr-perl-warning-setting-locale-failed","text":"This error is generally because your SSH client is passing LANG through as part of the SSH command, and is passing something that conflicts with what Legion has it set to. You may be more likely to come across this with newer versions of Mac OS X - if your client is different, have a look for an equivalent option. In Mac OS X Terminal, click Settings and under International untick the box that says \"Set locale environment variables on startup\". Per session, you can try LANG=C ssh userid@legion.rc.ucl.ac.uk","title":"Unable to run job: JSV stderr: perl: warning: Setting locale failed."},{"location":"Wiki_Export/FAQ/#why-cant-i-find-out-when-my-job-will-run","text":"An informative discussion on this matter can be found in the Scheduler section of the User Guide.","title":"Why can't I find out when my job will run?"},{"location":"Wiki_Export/FAQ/#what-can-i-do-to-minimise-the-time-i-need-to-wait-for-my-jobs-to-run","text":"Minimise the amount of wall clock time you request. Use job arrays instead of submitting large numbers of jobs (see our job script examples ). Plan your work so that you can do other things while your jobs are being scheduled - the rule of thumb is that you will have to wait about twice the requested wall clock time (on average).","title":"What can I do to minimise the time I need to wait for my job(s) to run?"},{"location":"Wiki_Export/FAQ/#what-is-my-project-code-short-string-project-id","text":"Prior to July 2014, every user had a project code. Now all users belong to the default project \"AllUsers\" and no longer have to specify this. If you see older job script examples mentioning a project ID, you can delete that section. Only projects with access to paid or specialised resources need to give a project code in order to use those resources. If you do not know yours, contact rc-support .","title":"What is my project code (short string) / project ID?"},{"location":"Wiki_Export/Getting_an_Account/","text":"Getting an Account \u00a7 We have a single simplified application process for access to our internal UCL clusters: Legion, Myriad, and Grace. These are available to all UCL researcher staff, and available to research students upon sponsorship by an appropriate member of UCL staff. (See application process below.) Use of these services is subject to a common set of terms and conditions . The online application form can be found here: User account application form (UCL login required via Shibboleth) We also run clusters that serve particular user groups including users from outside UCL. For information about applying for accounts on those resources, please see their information pages: Molecular Modelling and Materials Hub -- Thomas Molecular Modelling and Materials Hub -- Michael Account sponsors \u00a7 If you are a student or postdoctoral researcher, your application must be approved by a permanent member of staff (normally your supervisor or PI). You will need to enter the UCL username (e.g. ccaaxyz, not the UPI) of this sponsor in the application form, and an email will be sent to this person asking them to approve the application before the account can be created. Permanent members of staff do not need a sponsor and their accounts will be automatically approved. Note You and your sponsor will need a central UCL username and password. For most people, this is the account they use to access their UCL email. Application process \u00a7 The application process then runs as follows: Enter your UCL userid and password to access the application form. Complete the application form, reading the instructions carefully. Tip: Hover your mouse over text boxes for more information. Upon completion, your nominated sponsor will receive an email asking for the application to be approved. (If they do not receive this, contact rc-support@ucl.ac.uk). If you do not require a sponsor, your account will be automatically approved. Your sponsor should click on the link in the email and approve the account. You will receive an email when your account is approved if you have a sponsor. You should receive an email once we have created your account. Please note that there may be a delay of up to one working day between an application being approved and it being created. If your sponsor does not receive the email, you can send them the link to your application directly (which will look like https://signup.rc.ucl.ac.uk/computing/requests/0000 ). Accounts for visitors \u00a7 Applications for UCL visitors to use Research Computing services are welcomed. Please note that: Applicants must have a central UCL account. Queries about how to obtain a UCL account for visitors should be addressed, in the first instance, to your Departmental Administrator. Account applications should specify the UCL grant under which the work is being carried out, if possible, as well as an associated UCL group or researcher. Account applications may not be submitted on behalf of another except to cover accessibility requirements, as the account application process includes agreeing to relevant legal terms and conditions. Accounts for honorary staff \u00a7 UCL Staff Members may sponsor Honorary members (named individuals) to be provided with access to Research Computing services where this is beneficial to UCL\u2019s research interests. This application process is identical to the process above for students and non-permanent staff. All accounts thus provided are subject to all other \u2018standard\u2019 terms and conditions relating to their use of Research Computing services. Charges for use of research computing services \u00a7 No direct charges are currently applicable for non-exceptional use of any Research Computing services in accordance with standard resource allocation policy as defined by the Computational Resource Allocation Group (CRAG). Several methods are available to researchers who wish to gain access to additional resources, or obtain 'priority' use, including chargeable options. Details are available here .","title":"Getting an Account"},{"location":"Wiki_Export/Getting_an_Account/#getting-an-account","text":"We have a single simplified application process for access to our internal UCL clusters: Legion, Myriad, and Grace. These are available to all UCL researcher staff, and available to research students upon sponsorship by an appropriate member of UCL staff. (See application process below.) Use of these services is subject to a common set of terms and conditions . The online application form can be found here: User account application form (UCL login required via Shibboleth) We also run clusters that serve particular user groups including users from outside UCL. For information about applying for accounts on those resources, please see their information pages: Molecular Modelling and Materials Hub -- Thomas Molecular Modelling and Materials Hub -- Michael","title":"Getting an Account"},{"location":"Wiki_Export/Getting_an_Account/#account-sponsors","text":"If you are a student or postdoctoral researcher, your application must be approved by a permanent member of staff (normally your supervisor or PI). You will need to enter the UCL username (e.g. ccaaxyz, not the UPI) of this sponsor in the application form, and an email will be sent to this person asking them to approve the application before the account can be created. Permanent members of staff do not need a sponsor and their accounts will be automatically approved. Note You and your sponsor will need a central UCL username and password. For most people, this is the account they use to access their UCL email.","title":"Account sponsors"},{"location":"Wiki_Export/Getting_an_Account/#application-process","text":"The application process then runs as follows: Enter your UCL userid and password to access the application form. Complete the application form, reading the instructions carefully. Tip: Hover your mouse over text boxes for more information. Upon completion, your nominated sponsor will receive an email asking for the application to be approved. (If they do not receive this, contact rc-support@ucl.ac.uk). If you do not require a sponsor, your account will be automatically approved. Your sponsor should click on the link in the email and approve the account. You will receive an email when your account is approved if you have a sponsor. You should receive an email once we have created your account. Please note that there may be a delay of up to one working day between an application being approved and it being created. If your sponsor does not receive the email, you can send them the link to your application directly (which will look like https://signup.rc.ucl.ac.uk/computing/requests/0000 ).","title":"Application process"},{"location":"Wiki_Export/Getting_an_Account/#accounts-for-visitors","text":"Applications for UCL visitors to use Research Computing services are welcomed. Please note that: Applicants must have a central UCL account. Queries about how to obtain a UCL account for visitors should be addressed, in the first instance, to your Departmental Administrator. Account applications should specify the UCL grant under which the work is being carried out, if possible, as well as an associated UCL group or researcher. Account applications may not be submitted on behalf of another except to cover accessibility requirements, as the account application process includes agreeing to relevant legal terms and conditions.","title":"Accounts for visitors"},{"location":"Wiki_Export/Getting_an_Account/#accounts-for-honorary-staff","text":"UCL Staff Members may sponsor Honorary members (named individuals) to be provided with access to Research Computing services where this is beneficial to UCL\u2019s research interests. This application process is identical to the process above for students and non-permanent staff. All accounts thus provided are subject to all other \u2018standard\u2019 terms and conditions relating to their use of Research Computing services.","title":"Accounts for honorary staff"},{"location":"Wiki_Export/Getting_an_Account/#charges-for-use-of-research-computing-services","text":"No direct charges are currently applicable for non-exceptional use of any Research Computing services in accordance with standard resource allocation policy as defined by the Computational Resource Allocation Group (CRAG). Several methods are available to researchers who wish to gain access to additional resources, or obtain 'priority' use, including chargeable options. Details are available here .","title":"Charges for use of research computing services"},{"location":"Wiki_Export/Installing_PGI/","text":"PGI Compiler Suite Installation at UCL \u00a7 UCL has two floating licences for PGI Fortran/C/C++ Server for Linux, purchased primarily for building Gaussian 03 and Gaussian 09 on UCL computers. To install follow the procedure below. If you are installing on a system outside the Institutional Firewall, please connect to the UCL VPN service first. Download version 13.9 or 11.9 from the UCL Software Database on your Linux system in a empty directory. You will need to login with your UCL userid and password. Untar the installer files using: tar xvzf pgilinux-2013-139.tar.gz Run the installer as root and follow the instructions: ./install` During the installation you will be asked: Do you wish to generate license keys? (y/n) - Enter n as you will need to use the central UCL licence server. Add the PGI Suite bin directory to your PATH. The default location used by the installer is /opt/pgi/linux86-64/13.9/bin . Open access to TCP ports 27000 and 27055 in your local firewall and any departmental firewall. Setup access to the licence server by setting the LM_LICENSE_FILE environment variable. Use either: export LM_LICENSE_FILE=27000@lic-pgi.ucl.ac.uk export LM_LICENSE_FILE=$LM_LICENSE_FILE:27000@lic-pgi.ucl.ac.uk Use the second version if you have other licence managers already defined. The PGI compilers should now be installed and working.","title":"Installing PGI"},{"location":"Wiki_Export/Installing_PGI/#pgi-compiler-suite-installation-at-ucl","text":"UCL has two floating licences for PGI Fortran/C/C++ Server for Linux, purchased primarily for building Gaussian 03 and Gaussian 09 on UCL computers. To install follow the procedure below. If you are installing on a system outside the Institutional Firewall, please connect to the UCL VPN service first. Download version 13.9 or 11.9 from the UCL Software Database on your Linux system in a empty directory. You will need to login with your UCL userid and password. Untar the installer files using: tar xvzf pgilinux-2013-139.tar.gz Run the installer as root and follow the instructions: ./install` During the installation you will be asked: Do you wish to generate license keys? (y/n) - Enter n as you will need to use the central UCL licence server. Add the PGI Suite bin directory to your PATH. The default location used by the installer is /opt/pgi/linux86-64/13.9/bin . Open access to TCP ports 27000 and 27055 in your local firewall and any departmental firewall. Setup access to the licence server by setting the LM_LICENSE_FILE environment variable. Use either: export LM_LICENSE_FILE=27000@lic-pgi.ucl.ac.uk export LM_LICENSE_FILE=$LM_LICENSE_FILE:27000@lic-pgi.ucl.ac.uk Use the second version if you have other licence managers already defined. The PGI compilers should now be installed and working.","title":"PGI Compiler Suite Installation at UCL"},{"location":"Wiki_Export/Interactive_Jobs/","text":"Interactive Job Sessions \u00a7 For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for live visualisation, software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete. Please note that time limits are restricted to two hours for interactive sessions, and available core counts are limited. Requesting Access \u00a7 You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. It typically takes the form: qrsh -pe mpi 8 -l mem=512M,h_rt=2:00:00 -now no In this example you are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours (the maximum allowed for interactive sessions). All job types we support on the system are supported via an interactive session (see our examples section ). Likewise, all qsub options are supported like regular job submission with the difference that with qrsh they must be given at the command line, and not with any job script (or via -@). In addition the -now option is useful when a cluster is busy. By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The -now no option tells it to keep waiting until it gets scheduled.\u2028Pressing Ctrl+C (i.e. the control key and the C key at the same time) will safely cancel the request if it doesn't seem to be able to get you a session. Interactive X sessions \u00a7 You can get an interactive X session from the head node of the job back to the login\u2028 node. The way to do this is to run the qrsh command in the following generic fashion: qrsh <options> <command> <arguments to <command>> Where <command> is either a command to launch an X terminal like Xterm or Mrxvt or a GUI application like XMGrace or GaussView. To make effective use of the X forwarding you will need to have logged in to the login node with ssh -X or some equivalent method.\u2028\u2028 Here is an example of how you can get a X terminal session with the qrsh command:\u2028 qrsh -l mem=512M,h_rt=0:30:0 \\ /shared/ucl/apps/mrxvt/0.5.4/bin/mrxvt -title 'User Test Node' Working on the nodes \u00a7 If you want to run a command on one of your allocated nodes which is not the headnode, you can use a standard ssh command: ssh <hostname> <command> [args] Where <hostname> can be obtained by inspecting the file $TMPDIR/machines. GPU test nodes \u00a7 You can also run GPU jobs interactively simply by adding the -l gpu=1 or -l gpu=2 options to the qrsh command as normal. For more information, please contact us on rc-support@ucl.ac.uk","title":"Interactive Job Sessions"},{"location":"Wiki_Export/Interactive_Jobs/#interactive-job-sessions","text":"For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for live visualisation, software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete. Please note that time limits are restricted to two hours for interactive sessions, and available core counts are limited.","title":"Interactive Job Sessions"},{"location":"Wiki_Export/Interactive_Jobs/#requesting-access","text":"You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. It typically takes the form: qrsh -pe mpi 8 -l mem=512M,h_rt=2:00:00 -now no In this example you are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours (the maximum allowed for interactive sessions). All job types we support on the system are supported via an interactive session (see our examples section ). Likewise, all qsub options are supported like regular job submission with the difference that with qrsh they must be given at the command line, and not with any job script (or via -@). In addition the -now option is useful when a cluster is busy. By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The -now no option tells it to keep waiting until it gets scheduled.\u2028Pressing Ctrl+C (i.e. the control key and the C key at the same time) will safely cancel the request if it doesn't seem to be able to get you a session.","title":"Requesting Access"},{"location":"Wiki_Export/Interactive_Jobs/#interactive-x-sessions","text":"You can get an interactive X session from the head node of the job back to the login\u2028 node. The way to do this is to run the qrsh command in the following generic fashion: qrsh <options> <command> <arguments to <command>> Where <command> is either a command to launch an X terminal like Xterm or Mrxvt or a GUI application like XMGrace or GaussView. To make effective use of the X forwarding you will need to have logged in to the login node with ssh -X or some equivalent method.\u2028\u2028 Here is an example of how you can get a X terminal session with the qrsh command:\u2028 qrsh -l mem=512M,h_rt=0:30:0 \\ /shared/ucl/apps/mrxvt/0.5.4/bin/mrxvt -title 'User Test Node'","title":"Interactive X sessions"},{"location":"Wiki_Export/Interactive_Jobs/#working-on-the-nodes","text":"If you want to run a command on one of your allocated nodes which is not the headnode, you can use a standard ssh command: ssh <hostname> <command> [args] Where <hostname> can be obtained by inspecting the file $TMPDIR/machines.","title":"Working on the nodes"},{"location":"Wiki_Export/Interactive_Jobs/#gpu-test-nodes","text":"You can also run GPU jobs interactively simply by adding the -l gpu=1 or -l gpu=2 options to the qrsh command as normal. For more information, please contact us on rc-support@ucl.ac.uk","title":"GPU test nodes"},{"location":"Wiki_Export/Known_Issues/","text":"This page contains known issues and proposed work-arounds, if available. Check our issue tracker on GitHub . Github clones are regularly stalling after a small percentage \u00a7 This can happen on Legion's login nodes and appears to be an as-yet-unidentified network issue. Try sshing into the dedicated transfer node login05.external.legion.ucl.ac.uk as this has a slightly different setup which does not appear to have the problem.","title":"Known Issues"},{"location":"Wiki_Export/Known_Issues/#github-clones-are-regularly-stalling-after-a-small-percentage","text":"This can happen on Legion's login nodes and appears to be an as-yet-unidentified network issue. Try sshing into the dedicated transfer node login05.external.legion.ucl.ac.uk as this has a slightly different setup which does not appear to have the problem.","title":"Github clones are regularly stalling after a small percentage"},{"location":"Wiki_Export/News_and_Events/","text":"Recent News and Upcoming Events \u00a7 Previous Events \u00a7","title":"News and Events"},{"location":"Wiki_Export/News_and_Events/#recent-news-and-upcoming-events","text":"","title":"Recent News and Upcoming Events"},{"location":"Wiki_Export/News_and_Events/#previous-events","text":"","title":"Previous Events"},{"location":"Wiki_Export/Planned_Service_Outages/","text":"Below is the list of planned outages, partial or otherwise, which Research Computing Platforms will have to undergo for service improvements or due to external dependencies, such as data centre infrastructural works. Date: specifies the period of time during which the outage is expected to take place. Service Impact: details the Service and specific hardware affected by the outage. Comments: provides additional information such as details about how the service will be affected and advice. Date Service Impact Comments Thurs 24 May 2018 Legion partial outage Node types LMNOQSUY will be drained for 8am for work to take place on the power to the racks in TP3 datacentre. Node types XPTVZ will be running jobs. This work is not expected to take all day. Tues 10 - Thurs 12 Apr 2018 Thomas full outage Update of Lustre firmware. Jobs will be drained for 8am on 10th and the cluster will be out of service for that day, login nodes included. Possibility of overrun and 13th should be considered at risk. Outage extended by one day as a member of staff was required on site in the datacentre. Tues 27 - Weds 28 Feb 2018 Thomas full outage Update of Lustre firmware. Jobs will be drained for 8am on 27th and the cluster will be out of service for that day, login nodes included. Possibility of overrun and 28th should be considered at risk. ''(This was rescheduled to April due to strike action.)'' Fri 17 Nov - Mon 20 Nov 2017 Legion partial outage All the L, M, N, O, P, S, U and Y nodes are draining for 3pm on Friday 17th November so nodes and infrastructure currently in Wolfson House datacentre can be moved back out. Login08 is also being moved. We will attempt to have everything back online during Monday. Weds 27 Sept - Mon 2 Oct 2017 Grace outage Our vendors need to replace some hardware components and run further tests. Grace will be unavailable for submissions or data access from 08:00 on Wed 27th Sep 2017. The service will be restored at or before 10:00 on Mon 02 Oct 2017. Following this, Intel expect to issue a permanent firmware fix within two months, likely requiring a further downtime period. 31 July 2017 Grace queue lengths and outage Another outage is required for OCF to replace the broken cable. Mon 31st July will be an all-day outage. Full-length jobs will be running over the weekend. Thurs 20 July 2017 Grace outage Grace is being drained on the 19th so OCF can swap a cable that has problems. This work is expected to be finished at some point in the afternoon. There is likely to be another day's outage at a later date to replace it - we do not have details at present. Thurs 22 June 2017 Legion outage We will be updating the firmware of Legion's Lustre metadata controller. Jobs are being drained and Legion will be down entirely for the morning and at risk in the afternoon. Thurs 15 June 2017 Thomas outage Thomas will have a brief outage in the morning for us to reconfigure it to have no link to Grace's Infiniband. Home directories are being moved (data is being rsynced to Thomas' Lustre). The queues are being drained in preparation. Weds 14 - Thurs 15 June 2017 Grace outage Grace is down today in preparation to reconfigure the Infiniband network so there is no link to Thomas, which we believe is causing the problems. We will bring Grace back up later on Thursday and hopefully the issues will be resolved. Weds 17 May 2017 Grace and Thomas outage A faulty module is being replaced in a Grace switch. The queues have been drained so jobs are not running. This should allow the subset of Grace nodes that are still down to be brought back into use. Mon 8 May 2017 Grace and Thomas outage Cables needed to be switched in order for the work intended for the previous outage to be carried out, and the switch has been done. The queues are being drained for 10am and will be re-enabled as soon as we know no further work needs to be carried out. Jobs will run over the weekend but will not start if they do not have time to finish. Thurs 4 - Fri 5 May 2017 Grace and Thomas outage Our vendors are doing some work on the network equipment in Grace (we narrowed down the Infiniband problems to specific switches). Jobs on Grace and Thomas are drained. Thomas login nodes will not be available. Thurs 27 April 2017 Grace and Thomas outage We are investigating intermittent Infiniband problems on Grace and jobs have been drained for today. We cannot guarantee that the login nodes will remain available. (Thomas is still in pilot, but this may also make home directories inaccessible for parts of the day and affect running jobs). Sun 26th - Tues 28th Feb 2017 Legion outage We are replacing the NFS file servers on Legion with upgraded ones and as a result there is a planned outage from 5PM Sunday 26th Feb until morning Tuesday 28th Feb. You will be unable to log into the service, existing logins will be logged out and jobs will not be running during the outage. The service should be considered \"at risk\" for the rest of the 28th. Weds 7th - Fri 9th Dec 2016 Grace outage Due to some remedial work dating back to the last Grace upgrade and preparation for the coming deployment of the Tier 2 materials centre it is necessary to have a three day outage of the Grace service to adjust the configuration of the storage. There will be no access during this time. Fri 18 Nov - Tues 22 Nov 2016 Legion reduced service The TXYZ nodes will be up and running, but all other nodes will be down during this time as they need to be moved. They may be back as early as Monday lunchtime, but we cannot guarantee this and they could be unavailable until end of day on Tuesday 22nd November. Mon 26 Sept - Weds 28 Sept 2016 Legion outage This is to update Lustre firmware. The system should be considered at risk on the 29th and 30th after this. Legion login nodes will be unavailable from the morning of Monday 26th. If you are going to need any of your data during this time, please remember to copy it elsewhere before the outage, as there will be no access during this time. This will also mean a service interruption for the Research Software Development \"Jenkins\" service, which depends on Legion. Mon 11 July - Tues 30 August 2016 Grace expansion outage Grace will taken out of service for this period in order to be undertake an expansion and upgrade of the compute, storage and interconnect fabric of the machine. These works will provide an additional 324 nodes (5,184 cores), a doubling in storage (scratch and home) and an InfiniBand network capable of scaling to circa 1000 nodes. This will effectively double the capacity of Grace in the short term and provide a much easier pathway for future expansions of the system. We have discussed the length of the outage, and potential options for mitigating this, with the Computational Resource Allocation Group. However, both the CRAG and Research IT Services members agree that the need to take a single long outage is the right decision in this instance given the breadth and complexity of the work that needs to be undertaken. We will be providing additional information, progress updates and any actions required from users prior to and during the system outage via the grace-users mailing list. Thurs 12 May 2016 Grace outage There will be an all-day network outage at Slough so Grace will be down all day and not running jobs. Mon 9 May 2016 Legion outage We are draining jobs for Monday so we can install updates to fix a kernel bug. Mon 18 - Thurs 21 April 2016 Legion outage Legion will be unavailable while we do some updates, test Lustre and enable Scratch quotas. It should be considered at risk for the rest of the week. Update: Work still ongoing on Thurs 21. Fri 1 April 2016 Login05 outage The dedicated transfer node login05 will be re-imaged with the new Legion OS so will not be available for data transfer for part of the day. Thurs 11 Feb 2016, 8-9am Grace connectivity at risk Network routing tests to Slough are being done between 8-9am. There may be some issues connecting to Grace during that window. Mon 29 - Tues 30 June 2015 Legion outage Legion will be unavailable while we replace an NFS controller and re-enable Lustre quotas. Weds 1 July should be considered at risk. Tues 5 - Thurs 7 May 2015 Legion outage Legion will be unavailable while we carry out a necessary software update to the parallel file system. The service should also be considered at risk on Fri 8 May. Mon 9 - Tues 10 Mar 2015 login05 outage Legion's dedicated transfer node, login05, will be unavailable from 10am on March 10th so we can move it to a new datacentre. It won't allow new logins after 10am on March 9th. Mon 19th Jan to Weds 21st Jan 2015 Legion outage Legion will be down while we update the Lustre firmware. The 22nd and 23rd should also be considered at risk. Fri 29th Nov to Mon 1st Dec 2014 Legion outage Wolfson House Data Centre shutdown for remedial work to be carried out by Estates. Midday Fri 31st Oct to Mon 10th Nov 2014 Complete outage of Legion while electrical testing is done at Torrington Place data centre. During this time we also intend to move the remaining core infrastructure for Legion to the Torrington Place datacentre so that we avoid being affected by planned outages at the other datacentre later this year.","title":"Planned Service Outages"},{"location":"Wiki_Export/Points_of_Contact/","text":"This page contains tools and information for the nominated Points of Contact. Other system-specific information is at Thomas or Michael . These commands can all be run as thomas-command or michael-command : they run the same thing and the different names are for convenience. Displaying user information \u00a7 thomas-show or michael-show is a tool that enables you to find a lot of information about users. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. At the top level, --user shows all information for one user, in multiple tables. --contacts shows all points of contact - useful for getting the IDs, and --institutes is the same. --allusers will show everyone's basic info. --getmmm will show the most recently used mmm username. thomas-show -h usage: thomas-show [-h] [--user username] [--contacts] [--institutes] [--allusers] [--getmmm] {recentusers,getusers,whois} ... Show data from the Thomas database. Use [positional argument -h] for more help. positional arguments: {recentusers,getusers,whois} recentusers Show the n newest users (5 by default) getusers Show all users with this project, institute, contact whois Search for users matching the given requirements optional arguments: -h, --help show this help message and exit --user username Show all current info for this user --contacts Show all allowed values for contact --institutes Show all allowed values for institute --allusers Show all current users --getmmm Show the highest mmm username used Show recent users \u00a7 thomas-show recentusers or michael-show recentusers shows you the most recently-added N users, default 5. thomas-show recentusers -h usage: thomas-show recentusers [-h] [-n N] optional arguments: -h, --help show this help message and exit -n N Show users with a given project, institute, contact \u00a7 thomas-show getusers or michael-show getusers will search for exact matches to the given project, institute, contact combination. thomas-show getusers -h usage: thomas-show getusers [-h] [-p PROJECT] [-i INST_ID] [-c POC_ID] optional arguments: -h, --help show this help message and exit -p PROJECT, --project PROJECT Project name -i INST_ID, --institute INST_ID Institute ID -c POC_ID, --contact POC_ID Point of Contact ID Search for users based on partial information \u00a7 thomas-show whois or michael-show whois can be used to search for partial matches to username, name, email fragments, including all of those in combination. thomas-show whois -h usage: thomas-show whois [-h] [-u USERNAME] [-e EMAIL] [-n GIVEN_NAME] [-s SURNAME] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user contains -e EMAIL, --email EMAIL Email address of user contains -n GIVEN_NAME, --name GIVEN_NAME Given name of user contains -s SURNAME, --surname SURNAME Surname of user contains Adding user information and new projects \u00a7 thomas-add or michael-add will add information to the database. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. Please note that all options have a --debug flag that will allow you to see the query generated without committing the changes to the database - double-check that the information you are adding is correct. thomas-add -h usage: thomas-add [-h] {user,project,projectuser,poc,institute} ... Add data to the Thomas database. Use [positional argument -h] for more help. positional arguments: {user,project,projectuser,poc,institute} user Adding a new user with their initial project project Adding a new project projectuser Adding a new user-project-contact relationship poc Adding a new Point of Contact institute Adding a new institute/consortium optional arguments: -h, --help show this help message and exit Add a new user \u00a7 thomas-add user or michael-add user allows you to add a new user, with their initial project and point of contact. This does not create their account, but does email us with everything we need in order to create it. If you run this, you do not need to email us separately. The project specified must exist. thomas-add user -h usage: thomas-add user [-h] -u USERNAME -n GIVEN_NAME [-s SURNAME] -e EMAIL_ADDRESS -k \"SSH_KEY\" -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user -n GIVEN_NAME, --name GIVEN_NAME Given name of user -s SURNAME, --surname SURNAME Surname of user (optional) -e EMAIL_ADDRESS, --email EMAIL_ADDRESS Institutional email address of user -k \"SSH_KEY\", --key \"SSH_KEY\" User's public ssh key (quotes necessary) -p PROJECT_ID, --project PROJECT_ID Initial project the user belongs to -c POC_ID, --contact POC_ID Short ID of the user's Point of Contact --verbose Show SQL queries that are being submitted --debug Show SQL query submitted without committing the change SSH key formats \u00a7 It will verify the provided ssh key by default. Note that it has to be in the form ssh-xxx keystartshere . If someone has sent in a key which has line breaks and header items, make it into this format by adding the key type to the start and removing the line breaks from the key body. This key: ---- BEGIN SSH2 PUBLIC KEY ---- Comment: \"comment goes here\" AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K 7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+ 9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW 7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJW OeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV 3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== ---- END SSH2 PUBLIC KEY ---- should be converted into ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJWOeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== Other types of keys (dss etc) will say what they are in the first line, and you should change the ssh-rsa appropriately. The guide linked at Creating an ssh key in Windows also shows where users can get the second format out of PuTTY. Add a new project \u00a7 thomas-add project or michael-add project will create a new project, associated with an institution. It will not show in Gold until it also has a user in it. A project ID should begin with your institute ID, followed by an underscore and a project name. thomas-add project -h usage: thomas-add project [-h] -p PROJECT_ID -i INST_ID [--debug] optional arguments: -h, --help show this help message and exit -p PROJECT_ID, --project PROJECT_ID A new unique project ID -i INST_ID, --institute INST_ID Institute ID this project belongs to --debug Show SQL query submitted without committing the change Add a new project/user pairing \u00a7 thomas-add projectuser or michael-add projectuser will add an existing user to an existing project. Creating a new user for an existing project also creates this relationship. After a new project-user relationship is added, a cron job will pick that up within 15 minutes and create that project for that user in Gold, with no allocation. thomas-add projectuser -h usage: thomas-add projectuser [-h] -u USERNAME -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME An existing UCL username -p PROJECT_ID, --project PROJECT_ID An existing project ID -c POC_ID, --contact POC_ID An existing Point of Contact ID --debug Show SQL query submitted without committing the change Gold resource allocation \u00a7 We are currently using Gold to manage allocations on Thomas and Michael. There is one Gold database, so all the projects exist on both, but they are only active on specific clusters. Reporting from Gold \u00a7 There are wrapper scripts for a number of Gold commands (these exist in the userscripts module, loaded by default). These are all set to report in cpu-hours with the -h flag, as that is our main unit. If you wish to change anything about the wrappers, they live in /shared/ucl/apps/cluster-scripts/ so you can take a copy and add your preferred options. They all have a --man option to see the man pages for that command. Here are some basic useful options and what they do. They can all be given more options for more specific searches. gusage -p project_name [-s start_time] # Show the Gold usage per user in this project, in the given timeframe if specified. gbalance # Show the balance for every project, split into total, reserved and available. glsuser # Shows all the users in Gold. glsproject # Shows all the projects and which users are in them. glsres # Show all the current reservatioms, inc user and project. The Name column is the SGE job ID. gstatement # Produce a reporting statement showing beginning and end balances, credits and debits. # Less useful commands glstxn # Show all Gold transactions. Filter or it will take forever to run. glsalloc # Show all the allocations. These can be run by any user. The date format is YYYY-MM-DD. Eg. gstatement -p PROJECT -s 2017-08-01 will show all credits and debits for the given project since the given date, saying which user and job ID each charge was associated with. Transferring Gold \u00a7 As the point of contact, you can transfer Gold from your allocation account into other project accounts. As before, we've put -h in the wrapper so it is always working in cpu-hours. gtransfer --fromProject xxx_allocation --toProject xxx_subproject cpu_hours You can also transfer in the opposite direction, from the subproject back into your allocation account. Note that you are able to transfer your allocation into another institute's projects, but you cannot transfer it back again - only the other institute's point of contact (or rc-support) can give it back, so be careful which project you specify. When two allocations are active \u00a7 There is now an overlap period of a week when two allocations can be active. By default, gtransfer will transfer from active allocations in the order of earliest expiring first. To transfer from the new allocation only, you need to specify the allocation id. gtransfer -i allocation_ID --fromProject xxx_allocation --toProject xxx_subproject cpu_hours glsalloc shows you all allocations that ever existed, and the first column is the id. Id Account Projects StartTime EndTime Amount Deposited Description --- ------- --------------------- ---------- ---------- ---------- ---------- -------------- 87 38 UKCP_allocation 2017-08-07 2017-11-05 212800.00 3712800.00 97 38 UKCP_allocation 2017-10-30 2018-02-04 3712800.00 3712800.00","title":"Points of Contact"},{"location":"Wiki_Export/Points_of_Contact/#displaying-user-information","text":"thomas-show or michael-show is a tool that enables you to find a lot of information about users. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. At the top level, --user shows all information for one user, in multiple tables. --contacts shows all points of contact - useful for getting the IDs, and --institutes is the same. --allusers will show everyone's basic info. --getmmm will show the most recently used mmm username. thomas-show -h usage: thomas-show [-h] [--user username] [--contacts] [--institutes] [--allusers] [--getmmm] {recentusers,getusers,whois} ... Show data from the Thomas database. Use [positional argument -h] for more help. positional arguments: {recentusers,getusers,whois} recentusers Show the n newest users (5 by default) getusers Show all users with this project, institute, contact whois Search for users matching the given requirements optional arguments: -h, --help show this help message and exit --user username Show all current info for this user --contacts Show all allowed values for contact --institutes Show all allowed values for institute --allusers Show all current users --getmmm Show the highest mmm username used","title":"Displaying user information"},{"location":"Wiki_Export/Points_of_Contact/#show-recent-users","text":"thomas-show recentusers or michael-show recentusers shows you the most recently-added N users, default 5. thomas-show recentusers -h usage: thomas-show recentusers [-h] [-n N] optional arguments: -h, --help show this help message and exit -n N","title":"Show recent users"},{"location":"Wiki_Export/Points_of_Contact/#show-users-with-a-given-project-institute-contact","text":"thomas-show getusers or michael-show getusers will search for exact matches to the given project, institute, contact combination. thomas-show getusers -h usage: thomas-show getusers [-h] [-p PROJECT] [-i INST_ID] [-c POC_ID] optional arguments: -h, --help show this help message and exit -p PROJECT, --project PROJECT Project name -i INST_ID, --institute INST_ID Institute ID -c POC_ID, --contact POC_ID Point of Contact ID","title":"Show users with a given project, institute, contact"},{"location":"Wiki_Export/Points_of_Contact/#search-for-users-based-on-partial-information","text":"thomas-show whois or michael-show whois can be used to search for partial matches to username, name, email fragments, including all of those in combination. thomas-show whois -h usage: thomas-show whois [-h] [-u USERNAME] [-e EMAIL] [-n GIVEN_NAME] [-s SURNAME] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user contains -e EMAIL, --email EMAIL Email address of user contains -n GIVEN_NAME, --name GIVEN_NAME Given name of user contains -s SURNAME, --surname SURNAME Surname of user contains","title":"Search for users based on partial information"},{"location":"Wiki_Export/Points_of_Contact/#adding-user-information-and-new-projects","text":"thomas-add or michael-add will add information to the database. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied. Please note that all options have a --debug flag that will allow you to see the query generated without committing the changes to the database - double-check that the information you are adding is correct. thomas-add -h usage: thomas-add [-h] {user,project,projectuser,poc,institute} ... Add data to the Thomas database. Use [positional argument -h] for more help. positional arguments: {user,project,projectuser,poc,institute} user Adding a new user with their initial project project Adding a new project projectuser Adding a new user-project-contact relationship poc Adding a new Point of Contact institute Adding a new institute/consortium optional arguments: -h, --help show this help message and exit","title":"Adding user information and new projects"},{"location":"Wiki_Export/Points_of_Contact/#add-a-new-user","text":"thomas-add user or michael-add user allows you to add a new user, with their initial project and point of contact. This does not create their account, but does email us with everything we need in order to create it. If you run this, you do not need to email us separately. The project specified must exist. thomas-add user -h usage: thomas-add user [-h] -u USERNAME -n GIVEN_NAME [-s SURNAME] -e EMAIL_ADDRESS -k \"SSH_KEY\" -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME UCL username of user -n GIVEN_NAME, --name GIVEN_NAME Given name of user -s SURNAME, --surname SURNAME Surname of user (optional) -e EMAIL_ADDRESS, --email EMAIL_ADDRESS Institutional email address of user -k \"SSH_KEY\", --key \"SSH_KEY\" User's public ssh key (quotes necessary) -p PROJECT_ID, --project PROJECT_ID Initial project the user belongs to -c POC_ID, --contact POC_ID Short ID of the user's Point of Contact --verbose Show SQL queries that are being submitted --debug Show SQL query submitted without committing the change","title":"Add a new user"},{"location":"Wiki_Export/Points_of_Contact/#ssh-key-formats","text":"It will verify the provided ssh key by default. Note that it has to be in the form ssh-xxx keystartshere . If someone has sent in a key which has line breaks and header items, make it into this format by adding the key type to the start and removing the line breaks from the key body. This key: ---- BEGIN SSH2 PUBLIC KEY ---- Comment: \"comment goes here\" AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K 7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+ 9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW 7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJW OeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV 3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== ---- END SSH2 PUBLIC KEY ---- should be converted into ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJWOeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj== Other types of keys (dss etc) will say what they are in the first line, and you should change the ssh-rsa appropriately. The guide linked at Creating an ssh key in Windows also shows where users can get the second format out of PuTTY.","title":"SSH key formats"},{"location":"Wiki_Export/Points_of_Contact/#add-a-new-project","text":"thomas-add project or michael-add project will create a new project, associated with an institution. It will not show in Gold until it also has a user in it. A project ID should begin with your institute ID, followed by an underscore and a project name. thomas-add project -h usage: thomas-add project [-h] -p PROJECT_ID -i INST_ID [--debug] optional arguments: -h, --help show this help message and exit -p PROJECT_ID, --project PROJECT_ID A new unique project ID -i INST_ID, --institute INST_ID Institute ID this project belongs to --debug Show SQL query submitted without committing the change","title":"Add a new project"},{"location":"Wiki_Export/Points_of_Contact/#add-a-new-projectuser-pairing","text":"thomas-add projectuser or michael-add projectuser will add an existing user to an existing project. Creating a new user for an existing project also creates this relationship. After a new project-user relationship is added, a cron job will pick that up within 15 minutes and create that project for that user in Gold, with no allocation. thomas-add projectuser -h usage: thomas-add projectuser [-h] -u USERNAME -p PROJECT_ID -c POC_ID [--debug] optional arguments: -h, --help show this help message and exit -u USERNAME, --user USERNAME An existing UCL username -p PROJECT_ID, --project PROJECT_ID An existing project ID -c POC_ID, --contact POC_ID An existing Point of Contact ID --debug Show SQL query submitted without committing the change","title":"Add a new project/user pairing"},{"location":"Wiki_Export/Points_of_Contact/#gold-resource-allocation","text":"We are currently using Gold to manage allocations on Thomas and Michael. There is one Gold database, so all the projects exist on both, but they are only active on specific clusters.","title":"Gold resource allocation"},{"location":"Wiki_Export/Points_of_Contact/#reporting-from-gold","text":"There are wrapper scripts for a number of Gold commands (these exist in the userscripts module, loaded by default). These are all set to report in cpu-hours with the -h flag, as that is our main unit. If you wish to change anything about the wrappers, they live in /shared/ucl/apps/cluster-scripts/ so you can take a copy and add your preferred options. They all have a --man option to see the man pages for that command. Here are some basic useful options and what they do. They can all be given more options for more specific searches. gusage -p project_name [-s start_time] # Show the Gold usage per user in this project, in the given timeframe if specified. gbalance # Show the balance for every project, split into total, reserved and available. glsuser # Shows all the users in Gold. glsproject # Shows all the projects and which users are in them. glsres # Show all the current reservatioms, inc user and project. The Name column is the SGE job ID. gstatement # Produce a reporting statement showing beginning and end balances, credits and debits. # Less useful commands glstxn # Show all Gold transactions. Filter or it will take forever to run. glsalloc # Show all the allocations. These can be run by any user. The date format is YYYY-MM-DD. Eg. gstatement -p PROJECT -s 2017-08-01 will show all credits and debits for the given project since the given date, saying which user and job ID each charge was associated with.","title":"Reporting from Gold"},{"location":"Wiki_Export/Points_of_Contact/#transferring-gold","text":"As the point of contact, you can transfer Gold from your allocation account into other project accounts. As before, we've put -h in the wrapper so it is always working in cpu-hours. gtransfer --fromProject xxx_allocation --toProject xxx_subproject cpu_hours You can also transfer in the opposite direction, from the subproject back into your allocation account. Note that you are able to transfer your allocation into another institute's projects, but you cannot transfer it back again - only the other institute's point of contact (or rc-support) can give it back, so be careful which project you specify.","title":"Transferring Gold"},{"location":"Wiki_Export/Points_of_Contact/#when-two-allocations-are-active","text":"There is now an overlap period of a week when two allocations can be active. By default, gtransfer will transfer from active allocations in the order of earliest expiring first. To transfer from the new allocation only, you need to specify the allocation id. gtransfer -i allocation_ID --fromProject xxx_allocation --toProject xxx_subproject cpu_hours glsalloc shows you all allocations that ever existed, and the first column is the id. Id Account Projects StartTime EndTime Amount Deposited Description --- ------- --------------------- ---------- ---------- ---------- ---------- -------------- 87 38 UKCP_allocation 2017-08-07 2017-11-05 212800.00 3712800.00 97 38 UKCP_allocation 2017-10-30 2018-02-04 3712800.00 3712800.00","title":"When two allocations are active"},{"location":"Wiki_Export/Publications/","text":"Publications \u00a7 Some key publications arising from use of the Legion HPC service . Papers preceded by *** resulted from research conducted on the Legion cluster via 'non\u2011standard' use of resources (see Requesting Additional or Unusual Resources ). Astrophysics and Remote Sensing \u00a7 \u2018Cosmological simulations using GCMHD+\u2019 by Barnes, David J., Kawata, Daisuke and Wu, Kinwah. Monthly Notices of the Royal Astronomical Society 420 (2012) 3195. 1 \u2018Axial symmetry breaking of Saturn's thermosphere\u2019 by Smith, C. G. A. and Achilleos, N. Monthly Notices of the Royal Astronomical Society 422 (2012) 1460. 2 \u2018Methane in the atmosphere of the transiting hot Neptune GJ436b?\u2019 by Beaulieu JP, Tinetti G, Kipping DM, Ribas I, Barber RJ, Cho JYK, Polichtchouk I, Tennyson J, Yurchenko SN, Griffith CA, Batista V, Waldmann I, Miller S, Carey S, Mousis O and Fossey SJ. Astrophysical Journal 731 (2011) 2041. 3 \u2018A variationally computed line list for hot NH3\u2019 by Yurchenko SN, Barber RJ and Tennyson J. Monthly Notices of the Royal Astronomical Society 413 (2011) 1828. 4 \u2018Maser Sources in Astrophysics\u2019 by Gray, M. D. Cambridge University Press (2012). 5 Bioinformatics and Computational Biology \u00a7 \u2018AIP Mutation in Pituitary Adenomas in the 18th Century and Today\u2019 by Chahal HS, Stals K, Unterlander M, Balding DJ, Thomas MG, Kumar AV, Besser GM, Atkinson AB, Morrison PJ, Howlett TA, Levy MJ, Orme SM, Akker SA, Abel RL, Grossman AB, Burger J, Ellard S, and Korbonits M. New England Journal of Medicine 364 (2011) 43. 6 \u2018Genetic Discontinuity Between Local Hunter-Gatherers and Central Europe's First Farmers\u2019 by Bramanti B, Thomas MG, Haak W, Unterlaender M, Jores P, Tambets K, Antanaitis-Jacobs I, Haidle MN, Jankauskas R, Kind C-J, Lueth F, Terberger T, Hiller J, Matsumura S, Forster P and Burger J. Science 326 (2009) 137. 7 \u2018Accurate de novo structure prediction of large transmembrane protein domains using fragment assembly and correlated mutation analysis\u2019 by Nugent T and Jones DT. Proceedings of the National Academy of Sciences (2012). . 8 \u2018GeMMA: functional subfamily classification within superfamilies of predicted protein structural domains\u2019 by Lee DA, Rentzsch R and Orengo C. Nucleic Acids Research 38 (2010) 720. 9 *** \u2018The effect of insertions, deletions and alignment errors on the branch-site test of positive selection\u2019 by Fletcher W and Yang Z. Molecular Biology and Evolution 27 (2010) 2257. 10 Earth Materials \u00a7 'Lattice electrical resistivity of magnetic bcc iron from first-principles calculations' by D. Alf\u00e8, M. Pozzo, and M. P. Desjarlais. Physical Review B 85, (2012) 024102 1-5. 11 Epidemiology \u00a7 \u2018Effect on transmission of HIV-1 resistance of timing of implementation of viral load monitoring to determine switches from first to second line antiretroviral regimens in resource-limited settings\u2019 by Phillips AN, Pillay D, Garnett G, Bennett D, Vitoria M, Cambiano V and Lundgren JD. AIDS 25 (2011) 843. 12 \u2018Projected life expectancy of people with HIV according to timing of diagnosis\u2019 by Nakagawa F, Lodwick RK, Smith CJ, Smith R, Cambiano V, Lundgren JD, Delpech V and Phillips AN. AIDS 26 (2012) 335. 13 \u2018HIV Treatment as Prevention: Systematic Comparison of Mathematical Models of the Potential Impact of Antiretroviral Therapy on HIV Incidence in South Africa\u2019 by Jeffrey W. Eaton, Leigh F. Johnson, Joshua A. Salomon, Till B\u00e4rnighausen, Eran Bendavid, Anna Bershteyn, David E. Bloom, Valentina Cambiano, Christophe Fraser, Jan A. C. Hontelez, Salal Humair, Daniel J. Klein, Elisa F. Long, Andrew N. Phillips, Carel Pretorius, John Stover, Edward A. Wenger, Brian G. Williams and Timothy B. Hallett. Public Library of Science Medicine Medicine 9 (2012) e1001245. 14 \u2018Threshold Haemoglobin Levels and the Prognosis of Stable Coronary Disease: Two New Cohorts and a Systematic Review and Meta-Analysis\u2019 by Anoop D. Shah, Owen Nicholas, Adam D. Timmis, Gene Feder, Keith R. Abrams, Ruoling Chen, Aroon D. Hingorani and Harry Hemingway. Public Library of Science Medicine 8 (2011) e1000439. 15 Molecular Quantum Dynamics and Electronic Structure \u00a7 \u2018Experimental and computational studies of small molecule activation by uranium tris(aryloxides): binding of N2, coupling of CO and deoxygenation insertion of CO2 under ambient conditions\u2019 by Stephen M. Mansell, Nikolas Kaltsoyannis and Polly L. Arnold. Journal of the American Chemical Society 133 (2011) 9036. 16 \u2018A combined NMR/MD/QM approach for structure and dynamics elucidations in the solution state: pilot studies using tetrapeptides\u2019 by Aliev, A. E., Courtier-Murias, D., Bhandal, S. and Zhou, S. Chemical Communications 46 (2010) 695. 17 \u2018A stable two-coordinate acyclic silylene\u2019 by Andrey V. Protchenko, Krishna Hassomal Birjkumar, Deepak Dange, Andrew D. Schwarz, Dragoslav Vidovic, Cameron Jones, Nikolas Kaltsoyannis, Philip Mountford and Simon Aldridge. Journal of the American Chemical Society 134 (2012) 6500. 18 Work subsequently highlighted in Nature 485 (2012) 49. 19 \u2018A global, high accuracy ab initio dipole moment surface for the electronic ground state of the water molecule\u2019 by Lorenzo Lodi, Jonathan Tennyson and Oleg L. Polyanski. Journal of Chemical Physics 135 (2011) 034113. 20 \u2018Line lists for H218O and H217O based on empirical line positions and ab initio intensities\u2019 by Lorenzo Lodi and Jonathan Tennyson. Journal of Quantitative Spectroscopy & Radiative Transfer 113 (2012) 850. 21 Nanoscience and Defects \u00a7 'Point defects at the ice (0001) surface\u2019 by Matthew Watkin, Joost VandeVondele and Ben Slater. Proceedings of the National Academy of Sciences 107 (2010) 12429. 22 \u2018Aluminosilicate glasses as yttrium vectors for in-situ radiotherapy: understanding composition-durability effects through molecular dynamics simulations\u2019 by J. K. Christie and A. Tilocca. Chem. Mater. 22 (2010) 3725. 23 \u2018Aerobic Oxidation of Hydrocarbons Catalyzed by Mn-Doped Nanoporous Aluminophosphates(I): Preactivation of the Mn Sites\u2019 by Gomez-Hortiguela, L and Cora, F and Catlow, CRA. ACS Catalysis 1 (2011) 18. (Cover article in the first ever edition of this journal.) 24 \u2018Protonated Carboxyl Anchor for Stable Adsorption of Ru N749 Dye (Black Dye) on a TiO2 Anatase (101) Surface\u2019 by K. Sodeyama, M. Sumita, C. O'Rourke, U. Terranova, A. Islam, L. Han, D. R. Bowler and Y. Tateyama. J. Phys. Chem. Lett. 3 (2012) 472. 25 \u2018Mechanistic insight into the blocking of CO diffusion in [NiFe]-hydrogenase mutants through multiscale simulation\u2019 by P. Wang and J. Blumberger. Proceedings of the National Academy of Sciences 109 (2012) 6399. 26 27 Neuroscience \u00a7 \u2018Rapid Desynchronization of an Electrically Coupled Interneuron Network with Sparse Excitatory Synaptic Input\u2019 by Koen Vervaeke, Andrea L\u0151rincz, Padraig Gleeson, Matteo Farinella, Zoltan Nusser, R. Angus Silver. Neuron 67 (2010) 435. 28 Surface Science and Catalysis \u00a7 \u2018Ab initio molecular dynamics simulations of the cooperative adsorption of hydrazine and water on copper surfaces: Implications for shape control of nanoparticles\u2019 by T.D. Daff and N.H. de Leeuw. Chemistry of Materials 23 (2011) 2718. 29 \u2018Density functional theory and interatomic potential study of structural, mechanical and surface properties of calcium oxalate materials\u2019 by D. Di Tommaso, S.E. Ruiz-Hernandez, Z. Du and N.H. de Leeuw. RSC Advances 2 (2012) 4664. 30 \u2018Catalytic Reaction Mechanism of Mn-Doped Nanoporous Aluminophosphates for the Aerobic Oxidation of Hydrocarbons\u2019 by Luis G\u00f3mez-Hortig\u00fcela, Furio Cor\u00e0, Gopinathan Sankar, Claudio M. Zicovich-Wilson, and C. Richard A. Catlow. Chemistry, A European Journal 16 (2010) 13553. (Cover article.) 31 \u2018A molecular dynamics study of the interprotein interactions in collagen fibrils\u2019 by I. Streeter and N.H. de Leeuw. Soft Matter 7 (2011) 3373. 32 \u2018Molecular Dynamics simulation of the early stages of nucleation of hydroxyapatite at a collagen template\u2019 by N. Almora-Barrios and N.H. de Leeuw. Crystal Growth & Design 12 (2012) 756. 33 Systems Biomedicine \u00a7 \u2018Resolution of Discordant HIV-1 Protease Resistance Rankings Using Molecular Dynamics Simulations\u2019 by D. Wright and P. V. Coveney. Journal of Chemical Information and Modeling 51 (2011) 2636. 34 \u2018Rapid and accurate ranking of binding affinities of epidermal growth factor receptor sequences with selected lung cancer drugs\u2019 by S. Wan and P. V. Coveney. J. R. Soc. Interface 8 (2011) 1114. 35 \u2018Clay Minerals Mediate Folding and Regioselective Interactions of RNA: A Large-Scale Atomistic Simulation Study\u2019 by J. B. Swadling, P. V. Coveney and H. C. Greenwell. Journal of the American Chemical Society 132 (2010) 13750. 36 Other \u00a7 \u2018Universality of Performance Parameters in Vehicular ad hoc Networks\u2019 by T. Hewer, M. Nekovee and P. V. Coveney. IEEE Communications Letters 15 (2011) 947. 37 38","title":"Publications"},{"location":"Wiki_Export/Publications/#publications","text":"Some key publications arising from use of the Legion HPC service . Papers preceded by *** resulted from research conducted on the Legion cluster via 'non\u2011standard' use of resources (see Requesting Additional or Unusual Resources ).","title":"Publications"},{"location":"Wiki_Export/Publications/#astrophysics-and-remote-sensing","text":"\u2018Cosmological simulations using GCMHD+\u2019 by Barnes, David J., Kawata, Daisuke and Wu, Kinwah. Monthly Notices of the Royal Astronomical Society 420 (2012) 3195. 1 \u2018Axial symmetry breaking of Saturn's thermosphere\u2019 by Smith, C. G. A. and Achilleos, N. Monthly Notices of the Royal Astronomical Society 422 (2012) 1460. 2 \u2018Methane in the atmosphere of the transiting hot Neptune GJ436b?\u2019 by Beaulieu JP, Tinetti G, Kipping DM, Ribas I, Barber RJ, Cho JYK, Polichtchouk I, Tennyson J, Yurchenko SN, Griffith CA, Batista V, Waldmann I, Miller S, Carey S, Mousis O and Fossey SJ. Astrophysical Journal 731 (2011) 2041. 3 \u2018A variationally computed line list for hot NH3\u2019 by Yurchenko SN, Barber RJ and Tennyson J. Monthly Notices of the Royal Astronomical Society 413 (2011) 1828. 4 \u2018Maser Sources in Astrophysics\u2019 by Gray, M. D. Cambridge University Press (2012). 5","title":"Astrophysics and Remote Sensing"},{"location":"Wiki_Export/Publications/#bioinformatics-and-computational-biology","text":"\u2018AIP Mutation in Pituitary Adenomas in the 18th Century and Today\u2019 by Chahal HS, Stals K, Unterlander M, Balding DJ, Thomas MG, Kumar AV, Besser GM, Atkinson AB, Morrison PJ, Howlett TA, Levy MJ, Orme SM, Akker SA, Abel RL, Grossman AB, Burger J, Ellard S, and Korbonits M. New England Journal of Medicine 364 (2011) 43. 6 \u2018Genetic Discontinuity Between Local Hunter-Gatherers and Central Europe's First Farmers\u2019 by Bramanti B, Thomas MG, Haak W, Unterlaender M, Jores P, Tambets K, Antanaitis-Jacobs I, Haidle MN, Jankauskas R, Kind C-J, Lueth F, Terberger T, Hiller J, Matsumura S, Forster P and Burger J. Science 326 (2009) 137. 7 \u2018Accurate de novo structure prediction of large transmembrane protein domains using fragment assembly and correlated mutation analysis\u2019 by Nugent T and Jones DT. Proceedings of the National Academy of Sciences (2012). . 8 \u2018GeMMA: functional subfamily classification within superfamilies of predicted protein structural domains\u2019 by Lee DA, Rentzsch R and Orengo C. Nucleic Acids Research 38 (2010) 720. 9 *** \u2018The effect of insertions, deletions and alignment errors on the branch-site test of positive selection\u2019 by Fletcher W and Yang Z. Molecular Biology and Evolution 27 (2010) 2257. 10","title":"Bioinformatics and Computational Biology"},{"location":"Wiki_Export/Publications/#earth-materials","text":"'Lattice electrical resistivity of magnetic bcc iron from first-principles calculations' by D. Alf\u00e8, M. Pozzo, and M. P. Desjarlais. Physical Review B 85, (2012) 024102 1-5. 11","title":"Earth Materials"},{"location":"Wiki_Export/Publications/#epidemiology","text":"\u2018Effect on transmission of HIV-1 resistance of timing of implementation of viral load monitoring to determine switches from first to second line antiretroviral regimens in resource-limited settings\u2019 by Phillips AN, Pillay D, Garnett G, Bennett D, Vitoria M, Cambiano V and Lundgren JD. AIDS 25 (2011) 843. 12 \u2018Projected life expectancy of people with HIV according to timing of diagnosis\u2019 by Nakagawa F, Lodwick RK, Smith CJ, Smith R, Cambiano V, Lundgren JD, Delpech V and Phillips AN. AIDS 26 (2012) 335. 13 \u2018HIV Treatment as Prevention: Systematic Comparison of Mathematical Models of the Potential Impact of Antiretroviral Therapy on HIV Incidence in South Africa\u2019 by Jeffrey W. Eaton, Leigh F. Johnson, Joshua A. Salomon, Till B\u00e4rnighausen, Eran Bendavid, Anna Bershteyn, David E. Bloom, Valentina Cambiano, Christophe Fraser, Jan A. C. Hontelez, Salal Humair, Daniel J. Klein, Elisa F. Long, Andrew N. Phillips, Carel Pretorius, John Stover, Edward A. Wenger, Brian G. Williams and Timothy B. Hallett. Public Library of Science Medicine Medicine 9 (2012) e1001245. 14 \u2018Threshold Haemoglobin Levels and the Prognosis of Stable Coronary Disease: Two New Cohorts and a Systematic Review and Meta-Analysis\u2019 by Anoop D. Shah, Owen Nicholas, Adam D. Timmis, Gene Feder, Keith R. Abrams, Ruoling Chen, Aroon D. Hingorani and Harry Hemingway. Public Library of Science Medicine 8 (2011) e1000439. 15","title":"Epidemiology"},{"location":"Wiki_Export/Publications/#molecular-quantum-dynamics-and-electronic-structure","text":"\u2018Experimental and computational studies of small molecule activation by uranium tris(aryloxides): binding of N2, coupling of CO and deoxygenation insertion of CO2 under ambient conditions\u2019 by Stephen M. Mansell, Nikolas Kaltsoyannis and Polly L. Arnold. Journal of the American Chemical Society 133 (2011) 9036. 16 \u2018A combined NMR/MD/QM approach for structure and dynamics elucidations in the solution state: pilot studies using tetrapeptides\u2019 by Aliev, A. E., Courtier-Murias, D., Bhandal, S. and Zhou, S. Chemical Communications 46 (2010) 695. 17 \u2018A stable two-coordinate acyclic silylene\u2019 by Andrey V. Protchenko, Krishna Hassomal Birjkumar, Deepak Dange, Andrew D. Schwarz, Dragoslav Vidovic, Cameron Jones, Nikolas Kaltsoyannis, Philip Mountford and Simon Aldridge. Journal of the American Chemical Society 134 (2012) 6500. 18 Work subsequently highlighted in Nature 485 (2012) 49. 19 \u2018A global, high accuracy ab initio dipole moment surface for the electronic ground state of the water molecule\u2019 by Lorenzo Lodi, Jonathan Tennyson and Oleg L. Polyanski. Journal of Chemical Physics 135 (2011) 034113. 20 \u2018Line lists for H218O and H217O based on empirical line positions and ab initio intensities\u2019 by Lorenzo Lodi and Jonathan Tennyson. Journal of Quantitative Spectroscopy & Radiative Transfer 113 (2012) 850. 21","title":"Molecular Quantum Dynamics and Electronic Structure"},{"location":"Wiki_Export/Publications/#nanoscience-and-defects","text":"'Point defects at the ice (0001) surface\u2019 by Matthew Watkin, Joost VandeVondele and Ben Slater. Proceedings of the National Academy of Sciences 107 (2010) 12429. 22 \u2018Aluminosilicate glasses as yttrium vectors for in-situ radiotherapy: understanding composition-durability effects through molecular dynamics simulations\u2019 by J. K. Christie and A. Tilocca. Chem. Mater. 22 (2010) 3725. 23 \u2018Aerobic Oxidation of Hydrocarbons Catalyzed by Mn-Doped Nanoporous Aluminophosphates(I): Preactivation of the Mn Sites\u2019 by Gomez-Hortiguela, L and Cora, F and Catlow, CRA. ACS Catalysis 1 (2011) 18. (Cover article in the first ever edition of this journal.) 24 \u2018Protonated Carboxyl Anchor for Stable Adsorption of Ru N749 Dye (Black Dye) on a TiO2 Anatase (101) Surface\u2019 by K. Sodeyama, M. Sumita, C. O'Rourke, U. Terranova, A. Islam, L. Han, D. R. Bowler and Y. Tateyama. J. Phys. Chem. Lett. 3 (2012) 472. 25 \u2018Mechanistic insight into the blocking of CO diffusion in [NiFe]-hydrogenase mutants through multiscale simulation\u2019 by P. Wang and J. Blumberger. Proceedings of the National Academy of Sciences 109 (2012) 6399. 26 27","title":"Nanoscience and Defects"},{"location":"Wiki_Export/Publications/#neuroscience","text":"\u2018Rapid Desynchronization of an Electrically Coupled Interneuron Network with Sparse Excitatory Synaptic Input\u2019 by Koen Vervaeke, Andrea L\u0151rincz, Padraig Gleeson, Matteo Farinella, Zoltan Nusser, R. Angus Silver. Neuron 67 (2010) 435. 28","title":"Neuroscience"},{"location":"Wiki_Export/Publications/#surface-science-and-catalysis","text":"\u2018Ab initio molecular dynamics simulations of the cooperative adsorption of hydrazine and water on copper surfaces: Implications for shape control of nanoparticles\u2019 by T.D. Daff and N.H. de Leeuw. Chemistry of Materials 23 (2011) 2718. 29 \u2018Density functional theory and interatomic potential study of structural, mechanical and surface properties of calcium oxalate materials\u2019 by D. Di Tommaso, S.E. Ruiz-Hernandez, Z. Du and N.H. de Leeuw. RSC Advances 2 (2012) 4664. 30 \u2018Catalytic Reaction Mechanism of Mn-Doped Nanoporous Aluminophosphates for the Aerobic Oxidation of Hydrocarbons\u2019 by Luis G\u00f3mez-Hortig\u00fcela, Furio Cor\u00e0, Gopinathan Sankar, Claudio M. Zicovich-Wilson, and C. Richard A. Catlow. Chemistry, A European Journal 16 (2010) 13553. (Cover article.) 31 \u2018A molecular dynamics study of the interprotein interactions in collagen fibrils\u2019 by I. Streeter and N.H. de Leeuw. Soft Matter 7 (2011) 3373. 32 \u2018Molecular Dynamics simulation of the early stages of nucleation of hydroxyapatite at a collagen template\u2019 by N. Almora-Barrios and N.H. de Leeuw. Crystal Growth & Design 12 (2012) 756. 33","title":"Surface Science and Catalysis"},{"location":"Wiki_Export/Publications/#systems-biomedicine","text":"\u2018Resolution of Discordant HIV-1 Protease Resistance Rankings Using Molecular Dynamics Simulations\u2019 by D. Wright and P. V. Coveney. Journal of Chemical Information and Modeling 51 (2011) 2636. 34 \u2018Rapid and accurate ranking of binding affinities of epidermal growth factor receptor sequences with selected lung cancer drugs\u2019 by S. Wan and P. V. Coveney. J. R. Soc. Interface 8 (2011) 1114. 35 \u2018Clay Minerals Mediate Folding and Regioselective Interactions of RNA: A Large-Scale Atomistic Simulation Study\u2019 by J. B. Swadling, P. V. Coveney and H. C. Greenwell. Journal of the American Chemical Society 132 (2010) 13750. 36","title":"Systems Biomedicine"},{"location":"Wiki_Export/Publications/#other","text":"\u2018Universality of Performance Parameters in Vehicular ad hoc Networks\u2019 by T. Hewer, M. Nekovee and P. V. Coveney. IEEE Communications Letters 15 (2011) 947. 37 38","title":"Other"},{"location":"Wiki_Export/Quick_Start/","text":"This is a quick start guide to our clusters for users already familiar with operating in an HPC environment. Accessing A Cluster \u00a7 Before accessing the Legion cluster, it is necessary to apply for an account . Once you have received notification that your account has been created, you may log in via SSH to: Legion: legion.rc.ucl.ac.uk Myriad: myriad.rc.ucl.ac.uk Grace: grace.rc.ucl.ac.uk Your username and password are the same as those for your central UCL user id. Legion, Myriad, and Grace are only accessible from within UCL\u2019s network. If you need to access them from outside, you need to log in via Socrates, a departmental machine, or install the IS VPN service. More details on connecting to these services are provided on the Accessing RC Systems page. Managing data on the our clusters \u00a7 Users on our clusters have access to three pools of storage. They have a home directory which is mounted read only on the compute nodes and therefore cannot be written to by running jobs. They have a \"scratch\" area which is writable from jobs and intended for live data and job output. There is a link to this area called \"Scratch\" within the user\u2019s home directory. Finally, within a job users have access to temporary local storage on the nodes (environmental variable $TMPDIR ) which is cleared at the end of the job. Legion has slow external network connections, so there is a dedicated transfer node with 10 gigabit network links to and from Legion available at: login05.external.legion.ucl.ac.uk Transfers to Grace and Myriad may use any of the normal login nodes. For more details on the fairly complicated data management structures within Legion, see the Managing Data on RC Systems page. User Environment \u00a7 Our clusters run an operating system based on Red Hat Enterprise Linux 7 with the Son of Grid Engine batch scheduler. UCL-supported and provided packages are made available to users through the use of the modules system. module avail lists available modules module load loads a module module remove removes a module The module system handles dependency and conflict information. You can find out more about the modules system on Legion on the RC Systems user environment page. Compiling your code \u00a7 We provide Intel and GNU compilers, and OpenMPI and Intel MPI through the modules system, with the usual wrappers. For a full list of the development tools available see here or in the development tools/compilers sections of the modules system. You can find out more about compiling code on a cluster on the Compiling page. Job scheduling policy and projects \u00a7 A fair-share resource allocation model has been implemented on all our clusters. See Resource Allocation for more information and context. Submission scripts \u00a7 Jobs submitted to the scheduler (with \"qsub\") are shell scripts with directives preceded by #$ . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash -l # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process. #$ -l mem=1G # Set the name of the job. #$ -N SomeScience_1_16 # Select the MPI parallel environment and 16 processes. #$ -pe mpi 16 # Select the project that this job will run under. (Only if you have # access to paid resources). #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your_UCL_id>/Scratch/output You can then follow these directives with the commands your script would execute. Legion supports a wide variety of job types and we would strongly recommend you study the example scripts . Jobs can be controlled with qsub (submit job), qstat (list jobs) and qdel (delete job). See the Introduction to batch processing page for more details. Testing jobs using Interactive Jobs \u00a7 As well as batch access to the system, it is possible to run short, small jobs with interactive access through the scheduler. These can be requested though the qrsh command. You need to provide qrsh with the same options you would include in your job submission script, so: qrsh -pe mpi 8 -l mem=512M -l h_rt=2:0:0 Is functionally equivalent to: 1 2 3 4 5 #!/bin/bash #$ -S /bin/bash #$ -pe mpi 8 #$ -l mem=512M #$ -l h_rt=2:0:0 Except, of course, that the result of qrsh is an interactive shell. For more details, see the Interactive Jobs page. More information \u00a7 How the scheduler works Example submission scripts Acknowledging the use of our services in publications Contact and support FAQ Known issues Reporting Problems","title":"Quick Start"},{"location":"Wiki_Export/Quick_Start/#accessing-a-cluster","text":"Before accessing the Legion cluster, it is necessary to apply for an account . Once you have received notification that your account has been created, you may log in via SSH to: Legion: legion.rc.ucl.ac.uk Myriad: myriad.rc.ucl.ac.uk Grace: grace.rc.ucl.ac.uk Your username and password are the same as those for your central UCL user id. Legion, Myriad, and Grace are only accessible from within UCL\u2019s network. If you need to access them from outside, you need to log in via Socrates, a departmental machine, or install the IS VPN service. More details on connecting to these services are provided on the Accessing RC Systems page.","title":"Accessing A Cluster"},{"location":"Wiki_Export/Quick_Start/#managing-data-on-the-our-clusters","text":"Users on our clusters have access to three pools of storage. They have a home directory which is mounted read only on the compute nodes and therefore cannot be written to by running jobs. They have a \"scratch\" area which is writable from jobs and intended for live data and job output. There is a link to this area called \"Scratch\" within the user\u2019s home directory. Finally, within a job users have access to temporary local storage on the nodes (environmental variable $TMPDIR ) which is cleared at the end of the job. Legion has slow external network connections, so there is a dedicated transfer node with 10 gigabit network links to and from Legion available at: login05.external.legion.ucl.ac.uk Transfers to Grace and Myriad may use any of the normal login nodes. For more details on the fairly complicated data management structures within Legion, see the Managing Data on RC Systems page.","title":"Managing data on the our clusters"},{"location":"Wiki_Export/Quick_Start/#user-environment","text":"Our clusters run an operating system based on Red Hat Enterprise Linux 7 with the Son of Grid Engine batch scheduler. UCL-supported and provided packages are made available to users through the use of the modules system. module avail lists available modules module load loads a module module remove removes a module The module system handles dependency and conflict information. You can find out more about the modules system on Legion on the RC Systems user environment page.","title":"User Environment"},{"location":"Wiki_Export/Quick_Start/#compiling-your-code","text":"We provide Intel and GNU compilers, and OpenMPI and Intel MPI through the modules system, with the usual wrappers. For a full list of the development tools available see here or in the development tools/compilers sections of the modules system. You can find out more about compiling code on a cluster on the Compiling page.","title":"Compiling your code"},{"location":"Wiki_Export/Quick_Start/#job-scheduling-policy-and-projects","text":"A fair-share resource allocation model has been implemented on all our clusters. See Resource Allocation for more information and context.","title":"Job scheduling policy and projects"},{"location":"Wiki_Export/Quick_Start/#submission-scripts","text":"Jobs submitted to the scheduler (with \"qsub\") are shell scripts with directives preceded by #$ . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash -l # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM per process. #$ -l mem=1G # Set the name of the job. #$ -N SomeScience_1_16 # Select the MPI parallel environment and 16 processes. #$ -pe mpi 16 # Select the project that this job will run under. (Only if you have # access to paid resources). #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your_UCL_id>/Scratch/output You can then follow these directives with the commands your script would execute. Legion supports a wide variety of job types and we would strongly recommend you study the example scripts . Jobs can be controlled with qsub (submit job), qstat (list jobs) and qdel (delete job). See the Introduction to batch processing page for more details.","title":"Submission scripts"},{"location":"Wiki_Export/Quick_Start/#testing-jobs-using-interactive-jobs","text":"As well as batch access to the system, it is possible to run short, small jobs with interactive access through the scheduler. These can be requested though the qrsh command. You need to provide qrsh with the same options you would include in your job submission script, so: qrsh -pe mpi 8 -l mem=512M -l h_rt=2:0:0 Is functionally equivalent to: 1 2 3 4 5 #!/bin/bash #$ -S /bin/bash #$ -pe mpi 8 #$ -l mem=512M #$ -l h_rt=2:0:0 Except, of course, that the result of qrsh is an interactive shell. For more details, see the Interactive Jobs page.","title":"Testing jobs using Interactive Jobs"},{"location":"Wiki_Export/Quick_Start/#more-information","text":"How the scheduler works Example submission scripts Acknowledging the use of our services in publications Contact and support FAQ Known issues Reporting Problems","title":"More information"},{"location":"Wiki_Export/RC_Systems/","text":"We provide access to three computing clusters: Legion , Myriad , and Grace ( What is a computing cluster? ). All UCL researchers are eligible to use these platforms on a fair share basis and at no cost. This page provides a brief overview of our computing services and their technical specifications. We support Aristotle , which provides a limited interactive Linux environment to support training. We also host Thomas and Michael , clusters associated with the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling. Legion \u00a7 Legion is our general-use cluster. It contains nodes of different types, some specialist hardware, and nodes belonging to research groups. You can run serial or parallel jobs on it, and the combinations of resources you may request depend on the node type. Legion technical specs \u00a7 The Legion cluster nodes have several different types. Type T: 6 Dell 2U R820 large memory or fat nodes - four socket, eight cores per processor Each node has 32 cores at 48GB RAM per core Each node can work as a 32 core SMP system with 1.5TB of RAM 2TB local disk per node (1536G available for tmpfs) Type U: Exact hardware may vary, as these are bought in sections 160 Dell C6220 nodes (4 nodes per 2u) - dual processor, eight cores per processor 2560 cores, at 4GB RAM per core (64GB per node) Each node can work as a 16 core SMP device with 64GB of RAM 108 nodes are also connected with Infiniband configured as three 36 node 'islands' 1TB of local disk per node (792G available for tmpfs) Type X: 144 Dell C6100 nodes (4 nodes per 2u) - dual processor, six cores per processor 1728 cores, as 2GB RAM per core (24GB per node) Organized into 6 Computational Units of 288 cores Each server can function as a 12 core SMP device QDR InfiniPath chip-to-chip connectivity 250GB of local disk per server (173G available for tmpfs) Type Y: 108 Dell C6100 nodes (4 nodes per 2u) - dual processor, six cores per processor 1296 cores, each with 2GB RAM (24GB per node) Each server can function as a 12 core SMP device 500GB of local disk per server (406G available for tmpfs) Type Z: 4 Dell C6100 nodes (4 nodes in 2u) - dual processor, six cores per processor 48 cores, each with 4GB RAM (48GB per node) Each server can function as a 12 core SMP device 500GB of local disk per server (173G available for tmpfs) Type P GPU: 1 node - dual processor, six cores per processor Each server with 12 CPU cores and 1 NVIDIA K40c GPU card Each card having 2880 CUDA cores and 11GB RAM Each server can function as a 12 core SMP device 932GB of local disk per server (112G available for tmpfs) Type V GPU: 8 Dell C6100 nodes (4 nodes per 2u) - dual processor, six cores per processor Each server with 12 CPU cores and two M2070 GPU cards Each card having 448 CUDA cores and 6GB RAM Each server can function as a 12 core SMP device 500GB of local disk per server (358G available for tmpfs) Type S (experimental MIC nodes): 10 Dell 2U R720 nodes - two socket, 8 or 16 cores per processor Nodes have 16 CPU cores and two Xeon Phi 7120p MICs Nodes can work as a 16 core SMP device with 64GB of RAM 2TB of local disk per node (1536G available for tmpfs) Operating system is Red Hat Linux 6 The cluster is coupled to a large-scale high-performance file system: 342TB of RAID 6 storage Uses Lustre Cluster File System 4 servers to stripe data across the disks and 2 servers to store the metadata Aggregate speed of 6GB/s for Read/Writes Please note that this file system is for active data and not archive data. It should be used for temporary storage only . Myriad \u00a7 Myriad is our high-I/O, high-throughput cluster. It contains nodes of a few different types including GPUs. It runs jobs that will run within a single node rather than multi-node parallel jobs. Myriad technical specs \u00a7 Myriad consists of Lenovo SD530 nodes. There are two login nodes and two transfer nodes. There are three types of compute nodes: Type H: Lenovo SD530 Standard Nodes 2 x Intel Xeon Gold 6140 18C 140W 2.3GHz Processor (36 cores total) 12 x 16GB TruDDR4 RDIMM (192 GB total) 1 x 2TB 7.2K SATA HDD 1 x Mellanox ConnectX-5 EDR/100Gb IB single port VPI HCA Type I high memory: Lenovo SD530 High Memory Nodes 2 x Intel Xeon Gold 6140 18C 140W 2.3GHz Processor (36 cores total) 24 x 64GB TruDDR4 RDIMM (1.5TB total) 1 x 2TB 7.2K SAS HDD 1 x Mellanox ConnectX-4 2x100Gb/EDR IB VPI Adapter Type J GPU: Lenovo SD530 GPU Nodes 2 x Intel Xeon Gold 6140 18C 140W 2.3GHz Processor (36 cores total) 2 x nVidia Tesla P100 Adapter 12 x 16GB TruDDR4 RDIMM (192 GB total) 1 x 2TB 7.2K SATA HDD 1 x Mellanox ConnectX-5 EDR/100Gb IB single port VPI HCA Filesystem: Lenovo Lustre storage 200TB home 1PB Scratch 2 MDS servers 2 OSS servers Mellanox ConnectX-4 2x100Gb/EDR InfiniBand storage network Grace \u00a7 Grace is intended for large multinode parallel jobs. As per CRAG policy, jobs that require less than 32 cores are subject to a dramatic priority penalty. Grace technical specs \u00a7 Grace consists of 684 identical Lenovo NeXtScale nodes connected by non-blocking Intel TrueScale QDR Infiniband to each other and a 1.1 PetaByte DDN Lustre storage appliance. Four of the nodes are used as login and admin nodes. The remainder are available for running jobs. Each node has the following specs: 2x 8 core Intel Xeon E5-2630v3 processors (16 cores total) 64 Gigabytes of RAM 120 Gigabyte SSD for OS and TMPDIR Intel TrueScale QDR Infiniband adaptor Thomas \u00a7 Thomas is the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling, a domain-specific multi-institute machine hosted by UCL. Further details about Thomas Thomas technical specs \u00a7 Thomas consists of 720 Lenovo Intel x86-64 nodes, giving 17.2k cores in total, with Intel OmniPath interconnect (1:1 nonblocking in 36 node blocks, 3:1 between blocks and across the system). Each node has the following specs: 2 x 12 core Intel Broadwell processors (24 cores total) 128GB RAM 120GB SSD Aristotle \u00a7 Aristotle is a teaching machine, usable by everyone. It does not have a batch system - you run programs directly on the nodes and share resources with all other users. Use with consideration! Aristotle technical specs \u00a7 4x 16 core Dell servers with Intel(R) Xeon(R) CPU E5-2650 v2 64 gigabytes of RAM per node RedHat 7.2 No Infiniband so MPI may only be used within a node How do I apply for an account? \u00a7 Please apply at Account Services for everything other than Thomas - you will be approved for a Legion and Myriad account. You will be approved for a Grace account if the resources you request meet the requirements. For Thomas, see Applying for a Thomas account . Access to Aristotle \u00a7 Everyone with a UCL account has access to Aristotle. Login via SSH to: aristotle.rc.ucl.ac.uk","title":"Research Computing Platforms"},{"location":"Wiki_Export/RC_Systems/#legion","text":"Legion is our general-use cluster. It contains nodes of different types, some specialist hardware, and nodes belonging to research groups. You can run serial or parallel jobs on it, and the combinations of resources you may request depend on the node type.","title":"Legion"},{"location":"Wiki_Export/RC_Systems/#legion-technical-specs","text":"The Legion cluster nodes have several different types. Type T: 6 Dell 2U R820 large memory or fat nodes - four socket, eight cores per processor Each node has 32 cores at 48GB RAM per core Each node can work as a 32 core SMP system with 1.5TB of RAM 2TB local disk per node (1536G available for tmpfs) Type U: Exact hardware may vary, as these are bought in sections 160 Dell C6220 nodes (4 nodes per 2u) - dual processor, eight cores per processor 2560 cores, at 4GB RAM per core (64GB per node) Each node can work as a 16 core SMP device with 64GB of RAM 108 nodes are also connected with Infiniband configured as three 36 node 'islands' 1TB of local disk per node (792G available for tmpfs) Type X: 144 Dell C6100 nodes (4 nodes per 2u) - dual processor, six cores per processor 1728 cores, as 2GB RAM per core (24GB per node) Organized into 6 Computational Units of 288 cores Each server can function as a 12 core SMP device QDR InfiniPath chip-to-chip connectivity 250GB of local disk per server (173G available for tmpfs) Type Y: 108 Dell C6100 nodes (4 nodes per 2u) - dual processor, six cores per processor 1296 cores, each with 2GB RAM (24GB per node) Each server can function as a 12 core SMP device 500GB of local disk per server (406G available for tmpfs) Type Z: 4 Dell C6100 nodes (4 nodes in 2u) - dual processor, six cores per processor 48 cores, each with 4GB RAM (48GB per node) Each server can function as a 12 core SMP device 500GB of local disk per server (173G available for tmpfs) Type P GPU: 1 node - dual processor, six cores per processor Each server with 12 CPU cores and 1 NVIDIA K40c GPU card Each card having 2880 CUDA cores and 11GB RAM Each server can function as a 12 core SMP device 932GB of local disk per server (112G available for tmpfs) Type V GPU: 8 Dell C6100 nodes (4 nodes per 2u) - dual processor, six cores per processor Each server with 12 CPU cores and two M2070 GPU cards Each card having 448 CUDA cores and 6GB RAM Each server can function as a 12 core SMP device 500GB of local disk per server (358G available for tmpfs) Type S (experimental MIC nodes): 10 Dell 2U R720 nodes - two socket, 8 or 16 cores per processor Nodes have 16 CPU cores and two Xeon Phi 7120p MICs Nodes can work as a 16 core SMP device with 64GB of RAM 2TB of local disk per node (1536G available for tmpfs) Operating system is Red Hat Linux 6 The cluster is coupled to a large-scale high-performance file system: 342TB of RAID 6 storage Uses Lustre Cluster File System 4 servers to stripe data across the disks and 2 servers to store the metadata Aggregate speed of 6GB/s for Read/Writes Please note that this file system is for active data and not archive data. It should be used for temporary storage only .","title":"Legion technical specs"},{"location":"Wiki_Export/RC_Systems/#myriad","text":"Myriad is our high-I/O, high-throughput cluster. It contains nodes of a few different types including GPUs. It runs jobs that will run within a single node rather than multi-node parallel jobs.","title":"Myriad"},{"location":"Wiki_Export/RC_Systems/#myriad-technical-specs","text":"Myriad consists of Lenovo SD530 nodes. There are two login nodes and two transfer nodes. There are three types of compute nodes: Type H: Lenovo SD530 Standard Nodes 2 x Intel Xeon Gold 6140 18C 140W 2.3GHz Processor (36 cores total) 12 x 16GB TruDDR4 RDIMM (192 GB total) 1 x 2TB 7.2K SATA HDD 1 x Mellanox ConnectX-5 EDR/100Gb IB single port VPI HCA Type I high memory: Lenovo SD530 High Memory Nodes 2 x Intel Xeon Gold 6140 18C 140W 2.3GHz Processor (36 cores total) 24 x 64GB TruDDR4 RDIMM (1.5TB total) 1 x 2TB 7.2K SAS HDD 1 x Mellanox ConnectX-4 2x100Gb/EDR IB VPI Adapter Type J GPU: Lenovo SD530 GPU Nodes 2 x Intel Xeon Gold 6140 18C 140W 2.3GHz Processor (36 cores total) 2 x nVidia Tesla P100 Adapter 12 x 16GB TruDDR4 RDIMM (192 GB total) 1 x 2TB 7.2K SATA HDD 1 x Mellanox ConnectX-5 EDR/100Gb IB single port VPI HCA Filesystem: Lenovo Lustre storage 200TB home 1PB Scratch 2 MDS servers 2 OSS servers Mellanox ConnectX-4 2x100Gb/EDR InfiniBand storage network","title":"Myriad technical specs"},{"location":"Wiki_Export/RC_Systems/#grace","text":"Grace is intended for large multinode parallel jobs. As per CRAG policy, jobs that require less than 32 cores are subject to a dramatic priority penalty.","title":"Grace"},{"location":"Wiki_Export/RC_Systems/#grace-technical-specs","text":"Grace consists of 684 identical Lenovo NeXtScale nodes connected by non-blocking Intel TrueScale QDR Infiniband to each other and a 1.1 PetaByte DDN Lustre storage appliance. Four of the nodes are used as login and admin nodes. The remainder are available for running jobs. Each node has the following specs: 2x 8 core Intel Xeon E5-2630v3 processors (16 cores total) 64 Gigabytes of RAM 120 Gigabyte SSD for OS and TMPDIR Intel TrueScale QDR Infiniband adaptor","title":"Grace technical specs"},{"location":"Wiki_Export/RC_Systems/#thomas","text":"Thomas is the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling, a domain-specific multi-institute machine hosted by UCL. Further details about Thomas","title":"Thomas"},{"location":"Wiki_Export/RC_Systems/#thomas-technical-specs","text":"Thomas consists of 720 Lenovo Intel x86-64 nodes, giving 17.2k cores in total, with Intel OmniPath interconnect (1:1 nonblocking in 36 node blocks, 3:1 between blocks and across the system). Each node has the following specs: 2 x 12 core Intel Broadwell processors (24 cores total) 128GB RAM 120GB SSD","title":"Thomas technical specs"},{"location":"Wiki_Export/RC_Systems/#aristotle","text":"Aristotle is a teaching machine, usable by everyone. It does not have a batch system - you run programs directly on the nodes and share resources with all other users. Use with consideration!","title":"Aristotle"},{"location":"Wiki_Export/RC_Systems/#aristotle-technical-specs","text":"4x 16 core Dell servers with Intel(R) Xeon(R) CPU E5-2650 v2 64 gigabytes of RAM per node RedHat 7.2 No Infiniband so MPI may only be used within a node","title":"Aristotle technical specs"},{"location":"Wiki_Export/RC_Systems/#how-do-i-apply-for-an-account","text":"Please apply at Account Services for everything other than Thomas - you will be approved for a Legion and Myriad account. You will be approved for a Grace account if the resources you request meet the requirements. For Thomas, see Applying for a Thomas account .","title":"How do I apply for an account?"},{"location":"Wiki_Export/RC_Systems/#access-to-aristotle","text":"Everyone with a UCL account has access to Aristotle. Login via SSH to: aristotle.rc.ucl.ac.uk","title":"Access to Aristotle"},{"location":"Wiki_Export/RC_Systems_user_environment/","text":"RC Systems User Environment Operating System \u00a7 Legion, Myriad, and Grace \u00a7 Legion, Myriad, and Grace run a software stack based upon Red Hat Enterprise Linux 7 and Son of Grid Engine. The environment provided should be familiar to users of UNIX-like operating systems. Here is a A Quick Introduction to Unix for those not familiar with this essential operating system. Aristotle \u00a7 Aristotle runs Red Hat Enterprise Linux 6.5. Software \u00a7 As well as the system software, there are a number of applications, libraries and development tools available on our machines, the details of which may be found on the software pages . Modules \u00a7 Our systems use the environment modules system to manage packages. A module configures your current login session or job to use a particular piece of software. For example, this may involve altering your PATH and LD_LIBRARY_PATH environment variables to make the associated commands and/or libraries available at compile-time and/or run-time, without explicitly having to know the relevant paths. A module can for instance be associated with a particular version of the Intel compiler, or particular MPI libraries, or applications software, etc. The default environment has the most commonly required modules already loaded for your convenience. You can see what modules are currently loaded by using the command module list . The default module set is shown in the example below: $ module list Currently Loaded Modulefiles: 1) gcc-libs/4.9.2 8) screen/4.2.1 15) tmux/2.2 2) cmake/3.2.1 9) gerun 16) mrxvt/0.5.4 3) flex/2.5.39 10) nano/2.4.2 17) userscripts/1.3.0 4) git/2.10.2 11) nedit/5.6-aug15 18) rcps-core/1.0.0 5) apr/1.5.2 12) dos2unix/7.3 19) compilers/intel/2017/update1 6) apr-util/1.5.4 13) giflib/5.1.1 20) mpi/intel/2017/update1/intel 7) subversion/1.8.13 14) emacs/24.5 21) default-modules/2017 This output indicates that the Intel compilers are loaded, the Intel MPI environment, editor nedit and some other utilities. In addition to those made available in your default environment, we provide a rich set of additional modules for your use. These can be listed by typing: module whatis Or in a shorter form by typing: module avail You can load additional modules into your current session by using the command: module load For example, to add the module for FFTW 2.1.5 for the Intel compilers, type: module load fftw/2.1.5/intel-2015-update2 Typing module list will now show the above with the addition of the fftw module. You can unload modules from your current session by using the command: module unload For example, to remove the FFTW module, type: module unload fftw/2.1.5/intel-2015-update2 One commonly required change is to switch from using the Intel compiler and associated libraries (which are provided in the default environment), to using the GCC compiler. This would be achieved by typing the following commands: module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/intel/2015/update3/gnu-4.9.2 Note that the order in which you execute these commands is vital! You must always unload modules before loading their replacements. Typing module list again will show the changes. You can permanently change what modules are loaded by default in your environment by editing your ~/.bashrc file to add the appropriate module load and unload commands at the end. When you first start using a new application, typing module help <module> (where <module> is the name of the application module) will provide you with additional Legion-specific instructions on how to use the application if any are necessary. Module Commands \u00a7 module load loads a module module unload unloads a module module purge unloads all modules module list shows currently loaded modules module avail shows available modules module whatis shows available modules with brief explanations module show List the contents of the module fire. Shows environment variables set-up by the module module help Shows helpful information about a module, including instructions on how to use the application Aristotle-Specific Modules \u00a7 Aristotle mounts the Research Computing software stack, so you will see all the same modules. They won't necessarily all work - everything built specifically for Aristotle will have Aristotle in the module name or else be in the extra module section that will show up at the bottom when using module avail : -------------------- /shared/ucl/apps/eb_ivybridge_noib/modules/all -------------------- Bison/2.7-goolf-1.4.10 OpenMPI/1.6.4-GCC-4.7.2 CMake/2.8.11-goolf-1.4.10 PCRE/8.12-goolf-1.4.10 Docutils/0.9.1-goolf-1.4.10-Python-2.7.3 Python/2.7.3-goolf-1.4.10 Doxygen/1.8.3.1-goolf-1.4.10 ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2 EasyBuild/1.15.1 Sphinx/1.1.3-goolf-1.4.10-Python-2.7.3 FFTW/3.3.3-gompi-1.4.10 Szip/2.1-goolf-1.4.10 GCC/4.7.2 bzip2/1.0.6-goolf-1.4.10 GDAL/1.9.2-goolf-1.4.10 flex/2.5.37-goolf-1.4.10 GEOS/3.3.5-goolf-1.4.10 gompi/1.4.10 GMT/5.1.1-goolf-1.4.10 goolf/1.4.10 Ghostscript/9.10-goolf-1.4.10 hwloc/1.6.2-GCC-4.7.2 HDF5/1.8.10-patch1-goolf-1.4.10 libreadline/6.2-goolf-1.4.10 Jinja2/2.6-goolf-1.4.10-Python-2.7.3 ncurses/5.9-goolf-1.4.10 LibTIFF/4.0.3-goolf-1.4.10 netCDF/4.2.1.1-goolf-1.4.10 M4/1.4.16-goolf-1.4.10 setuptools/0.6c11-goolf-1.4.10-Python-2.7.3 OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2 zlib/1.2.7-goolf-1.4.10 The others are mixed in with the general modules: here are a few: matlab/full/r2015a/8.5-aristotle recommended/r-aristotle python/2.7.9/gnu.4.7.2-Aristotle gnuplot/5.0.1-Aristotle Aristotle has different default modules: $ module show default-modules-aristotle ------------------------------------------------------------------- /shared/ucl/apps/modulefiles2/bundles/default-modules-aristotle: module-whatis Adds default Aristotle modules to your environment. module load compilers/gnu/4.6.3 module load nedit/5.6 module load mrxvt/0.5.4 -------------------------------------------------------------------","title":"RC Systems user environment"},{"location":"Wiki_Export/RC_Systems_user_environment/#operating-system","text":"","title":"Operating System"},{"location":"Wiki_Export/RC_Systems_user_environment/#legion-myriad-and-grace","text":"Legion, Myriad, and Grace run a software stack based upon Red Hat Enterprise Linux 7 and Son of Grid Engine. The environment provided should be familiar to users of UNIX-like operating systems. Here is a A Quick Introduction to Unix for those not familiar with this essential operating system.","title":"Legion, Myriad, and Grace"},{"location":"Wiki_Export/RC_Systems_user_environment/#aristotle","text":"Aristotle runs Red Hat Enterprise Linux 6.5.","title":"Aristotle"},{"location":"Wiki_Export/RC_Systems_user_environment/#software","text":"As well as the system software, there are a number of applications, libraries and development tools available on our machines, the details of which may be found on the software pages .","title":"Software"},{"location":"Wiki_Export/RC_Systems_user_environment/#modules","text":"Our systems use the environment modules system to manage packages. A module configures your current login session or job to use a particular piece of software. For example, this may involve altering your PATH and LD_LIBRARY_PATH environment variables to make the associated commands and/or libraries available at compile-time and/or run-time, without explicitly having to know the relevant paths. A module can for instance be associated with a particular version of the Intel compiler, or particular MPI libraries, or applications software, etc. The default environment has the most commonly required modules already loaded for your convenience. You can see what modules are currently loaded by using the command module list . The default module set is shown in the example below: $ module list Currently Loaded Modulefiles: 1) gcc-libs/4.9.2 8) screen/4.2.1 15) tmux/2.2 2) cmake/3.2.1 9) gerun 16) mrxvt/0.5.4 3) flex/2.5.39 10) nano/2.4.2 17) userscripts/1.3.0 4) git/2.10.2 11) nedit/5.6-aug15 18) rcps-core/1.0.0 5) apr/1.5.2 12) dos2unix/7.3 19) compilers/intel/2017/update1 6) apr-util/1.5.4 13) giflib/5.1.1 20) mpi/intel/2017/update1/intel 7) subversion/1.8.13 14) emacs/24.5 21) default-modules/2017 This output indicates that the Intel compilers are loaded, the Intel MPI environment, editor nedit and some other utilities. In addition to those made available in your default environment, we provide a rich set of additional modules for your use. These can be listed by typing: module whatis Or in a shorter form by typing: module avail You can load additional modules into your current session by using the command: module load For example, to add the module for FFTW 2.1.5 for the Intel compilers, type: module load fftw/2.1.5/intel-2015-update2 Typing module list will now show the above with the addition of the fftw module. You can unload modules from your current session by using the command: module unload For example, to remove the FFTW module, type: module unload fftw/2.1.5/intel-2015-update2 One commonly required change is to switch from using the Intel compiler and associated libraries (which are provided in the default environment), to using the GCC compiler. This would be achieved by typing the following commands: module unload compilers module unload mpi module load compilers/gnu/4.9.2 module load mpi/intel/2015/update3/gnu-4.9.2 Note that the order in which you execute these commands is vital! You must always unload modules before loading their replacements. Typing module list again will show the changes. You can permanently change what modules are loaded by default in your environment by editing your ~/.bashrc file to add the appropriate module load and unload commands at the end. When you first start using a new application, typing module help <module> (where <module> is the name of the application module) will provide you with additional Legion-specific instructions on how to use the application if any are necessary.","title":"Modules"},{"location":"Wiki_Export/RC_Systems_user_environment/#module-commands","text":"module load loads a module module unload unloads a module module purge unloads all modules module list shows currently loaded modules module avail shows available modules module whatis shows available modules with brief explanations module show List the contents of the module fire. Shows environment variables set-up by the module module help Shows helpful information about a module, including instructions on how to use the application","title":"Module Commands"},{"location":"Wiki_Export/RC_Systems_user_environment/#aristotle-specific-modules","text":"Aristotle mounts the Research Computing software stack, so you will see all the same modules. They won't necessarily all work - everything built specifically for Aristotle will have Aristotle in the module name or else be in the extra module section that will show up at the bottom when using module avail : -------------------- /shared/ucl/apps/eb_ivybridge_noib/modules/all -------------------- Bison/2.7-goolf-1.4.10 OpenMPI/1.6.4-GCC-4.7.2 CMake/2.8.11-goolf-1.4.10 PCRE/8.12-goolf-1.4.10 Docutils/0.9.1-goolf-1.4.10-Python-2.7.3 Python/2.7.3-goolf-1.4.10 Doxygen/1.8.3.1-goolf-1.4.10 ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2 EasyBuild/1.15.1 Sphinx/1.1.3-goolf-1.4.10-Python-2.7.3 FFTW/3.3.3-gompi-1.4.10 Szip/2.1-goolf-1.4.10 GCC/4.7.2 bzip2/1.0.6-goolf-1.4.10 GDAL/1.9.2-goolf-1.4.10 flex/2.5.37-goolf-1.4.10 GEOS/3.3.5-goolf-1.4.10 gompi/1.4.10 GMT/5.1.1-goolf-1.4.10 goolf/1.4.10 Ghostscript/9.10-goolf-1.4.10 hwloc/1.6.2-GCC-4.7.2 HDF5/1.8.10-patch1-goolf-1.4.10 libreadline/6.2-goolf-1.4.10 Jinja2/2.6-goolf-1.4.10-Python-2.7.3 ncurses/5.9-goolf-1.4.10 LibTIFF/4.0.3-goolf-1.4.10 netCDF/4.2.1.1-goolf-1.4.10 M4/1.4.16-goolf-1.4.10 setuptools/0.6c11-goolf-1.4.10-Python-2.7.3 OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2 zlib/1.2.7-goolf-1.4.10 The others are mixed in with the general modules: here are a few: matlab/full/r2015a/8.5-aristotle recommended/r-aristotle python/2.7.9/gnu.4.7.2-Aristotle gnuplot/5.0.1-Aristotle Aristotle has different default modules: $ module show default-modules-aristotle ------------------------------------------------------------------- /shared/ucl/apps/modulefiles2/bundles/default-modules-aristotle: module-whatis Adds default Aristotle modules to your environment. module load compilers/gnu/4.6.3 module load nedit/5.6 module load mrxvt/0.5.4 -------------------------------------------------------------------","title":"Aristotle-Specific Modules"},{"location":"Wiki_Export/Reporting_problems/","text":"RC Systems' support process is based around a ticketing system. To submit a ticket please send an e-mail to rc-support@ucl.ac.uk . In order to help our support team deal with your ticket efficiently, please ensure the following is the case: You have a clear description of your problem. This should include: Which system you are using (Myriad, Legion, Grace, Aristotle etc). Your userid. What you were doing when you got the error. What the error was (including a copy of the exact error message on the email body or a relevant extract). Job IDs, job scripts and job output/error files if relevant. Please avoid creating duplicate tickets. If you are following up on an existing problem, please reply to a response from the ticketing system (or at least a message with the ticket ID in the subject) for the original ticket. This way our support staff will be able to see a complete history of your problem, and it is considerably more likely that your issue will be dealt with by a member of staff who is familiar with the history of your problem.","title":"Reporting problems"},{"location":"Wiki_Export/Research_Computing_Glossary/","text":"Bash \u00a7 A shell and scripting language, which is the default command processor on most Linux operating systems. Cluster \u00a7 A cluster consists of a set of computer nodes connected together over a fast local area network. A message passing protocol such as MPI allows individual nodes to work together as a single system. Core \u00a7 A core refers to a processing unit within a node . A node may have multiple cores which can work in parallel on a single task, operating on the same data in memory. This kind of parallelism is coordinated using the OpenMP library. Alternatively, cores may work independently on different tasks. Cores may or may not also share cache. Interconnect \u00a7 The interconnect is the network which is used to transfer data between nodes in a cluster . Different types of interconnect operate at different bandwidths and with different amounts of latency, which affects the suitability of a collection of nodes for jobs which use message passing ( MPI ). Job \u00a7 In the context of Batch Processing , a job refers to a computational task to be performed such as a single simulation or analysis. Job Script \u00a7 A job script is essentially a special kind of script used to specify the parameters of a job. Users can specify the data to input, program to use, and the computing resources required. The job script is specified when a job is submitted to SGE, which reads lines starting with #$ . MPI \u00a7 The Message Passing Interface (MPI) system is a set of portable libraries which can be incorporated into programs in order to control parallel computation. Specifically it coordinates effort between nodes which do not share the same memory address space cf. OpenMP . Node \u00a7 In cluster computing, a node refers to a computational unit which is capable of operating independently of other parts of the cluster. As a minimum it consists of one (or more) processing cores , has its own memory, and runs its own operating system. OpenMP \u00a7 Open Multi-Processing. OpenMP supports multithreading, a process whereby a master thread generates a number of slave threads to run a task which is divided among them. OpenMP applies to processes running on shared memory platforms, i.e. jobs running on a single node . Hybrid applications may make use of both OpenMP and MPI . Process \u00a7 A process is a single instance of a program that is running on a computer. A single process may consist of many threads acting concurrently, and there may multiple instances of a program running as separate processes. Script \u00a7 A shell script enables users to list commands to be run consecutively by typing them into a text file instead of typing them out live. The first line of the script uses the shebang notation #! to designate the scripting language interpreter program to be used to interpret the commands, e.g. bash . Shebang \u00a7 \"Shebang\" is a common abbreviation for \"hash-bang\" \u2014 the character sequence #! \u2014 which is placed at the start of a script to specify the interpreter that should be used. When the shebang is found in the first line of a script, the program loader reads the rest of the line as the path to the required interpreter (e.g. /bin/bash is the usual path to the bash shell). The specified interpreter is then run with the path to the script passed as an argument to it. Shell \u00a7 A command line interpreter which provides an interface for users to type instructions to be interpreted by the operating system and display output via the monitor. Users type specific shell commands in order to run processes , e.g. ls to list directory contents. Son of Grid Engine (SGE or SoGE) \u00a7 The queuing system used by many cluster computing systems (including, currently, all the ones we run) to organise and schedule jobs . Once jobs are submitted to SGE, it takes care of executing them when the required resources become available. Job priority is subject to the local fair use policy. Sun Grid Engine (SGE) \u00a7 The original software written by Sun Microsystems that was later modified to make Son of Grid Engine (among other products, like Univa Grid Engine). Documentation may refer to Sun Grid Engine instead of Son of Grid Engine, and for most user purposes, the terms are interchangeable. Thread \u00a7 A thread refers to a serial computational process which can run on a single core . The number of threads generated by a parallel job may exceed the number of cores available though, in which case cores may alternate between running different threads. Threads are a software concept whereas cores are physical hardware.","title":"Research Computing Glossary"},{"location":"Wiki_Export/Research_Computing_Glossary/#bash","text":"A shell and scripting language, which is the default command processor on most Linux operating systems.","title":"Bash"},{"location":"Wiki_Export/Research_Computing_Glossary/#cluster","text":"A cluster consists of a set of computer nodes connected together over a fast local area network. A message passing protocol such as MPI allows individual nodes to work together as a single system.","title":"Cluster"},{"location":"Wiki_Export/Research_Computing_Glossary/#core","text":"A core refers to a processing unit within a node . A node may have multiple cores which can work in parallel on a single task, operating on the same data in memory. This kind of parallelism is coordinated using the OpenMP library. Alternatively, cores may work independently on different tasks. Cores may or may not also share cache.","title":"Core"},{"location":"Wiki_Export/Research_Computing_Glossary/#interconnect","text":"The interconnect is the network which is used to transfer data between nodes in a cluster . Different types of interconnect operate at different bandwidths and with different amounts of latency, which affects the suitability of a collection of nodes for jobs which use message passing ( MPI ).","title":"Interconnect"},{"location":"Wiki_Export/Research_Computing_Glossary/#job","text":"In the context of Batch Processing , a job refers to a computational task to be performed such as a single simulation or analysis.","title":"Job"},{"location":"Wiki_Export/Research_Computing_Glossary/#job-script","text":"A job script is essentially a special kind of script used to specify the parameters of a job. Users can specify the data to input, program to use, and the computing resources required. The job script is specified when a job is submitted to SGE, which reads lines starting with #$ .","title":"Job Script"},{"location":"Wiki_Export/Research_Computing_Glossary/#mpi","text":"The Message Passing Interface (MPI) system is a set of portable libraries which can be incorporated into programs in order to control parallel computation. Specifically it coordinates effort between nodes which do not share the same memory address space cf. OpenMP .","title":"MPI"},{"location":"Wiki_Export/Research_Computing_Glossary/#node","text":"In cluster computing, a node refers to a computational unit which is capable of operating independently of other parts of the cluster. As a minimum it consists of one (or more) processing cores , has its own memory, and runs its own operating system.","title":"Node"},{"location":"Wiki_Export/Research_Computing_Glossary/#openmp","text":"Open Multi-Processing. OpenMP supports multithreading, a process whereby a master thread generates a number of slave threads to run a task which is divided among them. OpenMP applies to processes running on shared memory platforms, i.e. jobs running on a single node . Hybrid applications may make use of both OpenMP and MPI .","title":"OpenMP"},{"location":"Wiki_Export/Research_Computing_Glossary/#process","text":"A process is a single instance of a program that is running on a computer. A single process may consist of many threads acting concurrently, and there may multiple instances of a program running as separate processes.","title":"Process"},{"location":"Wiki_Export/Research_Computing_Glossary/#script","text":"A shell script enables users to list commands to be run consecutively by typing them into a text file instead of typing them out live. The first line of the script uses the shebang notation #! to designate the scripting language interpreter program to be used to interpret the commands, e.g. bash .","title":"Script"},{"location":"Wiki_Export/Research_Computing_Glossary/#shebang","text":"\"Shebang\" is a common abbreviation for \"hash-bang\" \u2014 the character sequence #! \u2014 which is placed at the start of a script to specify the interpreter that should be used. When the shebang is found in the first line of a script, the program loader reads the rest of the line as the path to the required interpreter (e.g. /bin/bash is the usual path to the bash shell). The specified interpreter is then run with the path to the script passed as an argument to it.","title":"Shebang"},{"location":"Wiki_Export/Research_Computing_Glossary/#shell","text":"A command line interpreter which provides an interface for users to type instructions to be interpreted by the operating system and display output via the monitor. Users type specific shell commands in order to run processes , e.g. ls to list directory contents.","title":"Shell"},{"location":"Wiki_Export/Research_Computing_Glossary/#son-of-grid-engine-sge-or-soge","text":"The queuing system used by many cluster computing systems (including, currently, all the ones we run) to organise and schedule jobs . Once jobs are submitted to SGE, it takes care of executing them when the required resources become available. Job priority is subject to the local fair use policy.","title":"Son of Grid Engine (SGE or SoGE)"},{"location":"Wiki_Export/Research_Computing_Glossary/#sun-grid-engine-sge","text":"The original software written by Sun Microsystems that was later modified to make Son of Grid Engine (among other products, like Univa Grid Engine). Documentation may refer to Sun Grid Engine instead of Son of Grid Engine, and for most user purposes, the terms are interchangeable.","title":"Sun Grid Engine (SGE)"},{"location":"Wiki_Export/Research_Computing_Glossary/#thread","text":"A thread refers to a serial computational process which can run on a single core . The number of threads generated by a parallel job may exceed the number of cores available though, in which case cores may alternate between running different threads. Threads are a software concept whereas cores are physical hardware.","title":"Thread"},{"location":"Wiki_Export/Terms_and_Conditions/","text":"All use of Research Computing Platforms is subject to the UCL Computing Regulations . All users will be required to renew their account once per year. Users will receive a reminder one month prior to suspension of their Legion account sent to their Live@UCL e-mail address. Funding information will need to be provided upon application, and publication information upon renewal. Users are forbidden from performing production runs on login nodes. The Research Computing Platform Services Team reserve the right to suspend or ban without prior warning any use of the system which impairs its operation. With the exception of in cases where there is imminent harm or risk to the service, the Research Computing Platform Services Team will not access your files without permission. Official service notifications are sent to the legion-users (or the equivalent for other services) mailing list. Users are automatically subscribed to this list using their Live@UCL e-mail address and should read notices sent there. The Research Computing Platform Services Team reserve the right to suspend users' accounts, without notice, in the event of a user being the subject of any UCL disciplinary procedure, or where a user is found to be in breach of UCL\u2019s Computing Regulations or best practice guidelines regarding password management, as provided by Information Services Division. Users are required to acknowledge their use of Research Computing services in any publications describing research that has been conducted, in any part, using our services. This should be done according to the descriptions here . All support requests should be sent by e-mail to rc-support@ucl.ac.uk .","title":"Terms and Conditions of Use"},{"location":"Wiki_Export/Training/","text":"Upcoming Courses \u00a7 Previous Courses \u00a7 Below is a list of some of the training courses we have run over the past year. Please email rits@ucl.ac.uk if you are interested in any of these courses and we will notify you the next time that course is being run. Recommended Online Resources \u00a7 In addition to providing our own courses and training materials we also regularly try out some of the many free online courses available for learning different aspects of research computing. Those we recommend are listed here by category.","title":"Training"},{"location":"Wiki_Export/Training/#upcoming-courses","text":"","title":"Upcoming Courses"},{"location":"Wiki_Export/Training/#previous-courses","text":"Below is a list of some of the training courses we have run over the past year. Please email rits@ucl.ac.uk if you are interested in any of these courses and we will notify you the next time that course is being run.","title":"Previous Courses"},{"location":"Wiki_Export/Training/#recommended-online-resources","text":"In addition to providing our own courses and training materials we also regularly try out some of the many free online courses available for learning different aspects of research computing. Those we recommend are listed here by category.","title":"Recommended Online Resources"},{"location":"Wiki_Export/User_Scripts/","text":"These are tools developed by either the Research Computing group or users of the services, which may be useful to others. If you have a tool which you think might be useful to others, please feel free to send it to rc-support@ucl.ac.uk . If we think it's appropriate, we'll give it a look over and possibly some polish, and add it to the list. These tools tend to be created for Legion in the first instance, so they may not all be appropriate on other systems. These are located in: /shared/ucl/apps/userscripts or can be used by loading the userscripts module: module load userscripts You should be able to obtain more information about these scripts by typing the name of the script followed by --help , for example: qexplain --help Script Description qexplain Prints the full error associated with a job in an error state. jobhist Shows recently finished jobs, along with when they finished and, optionally, other information about them. Displays the last 24 hours by default. nodesforjob Shows all the nodes that a currently-running job is running on, along with information on load, memory and swap being used. nodetypes Show a list of currently-available node types, including the number of cores and amount of RAM they have. (Nodes that are down will not be counted, so the numbers will fluctuate). to-grace, to-legion Copy files from Legion to Grace or vice versa. Uses login05 as the destination if copying to Legion. It will tar up the file/directory you give it, copy it to your home on the other machine and untar it again.","title":"User Scripts"},{"location":"Wiki_Export/X-Forwarding/","text":"X is a system and protocol that lets remote computers push interactive windows to your local computer over a network. We use a method known as X-Forwarding, together with an SSH client, to direct the network messages for X over the same connection you use for the command-line. The setup steps for getting this working on Windows, Linux, and macOS are each different, and we've put them below. Windows \u00a7 Windows doesn't natively have the ability to receive X windows, so you need to install an X server separately. There are a few choices; UCL has a site-wide license (covering UCL-owned and personal computers) for one called Exceed , which is pretty reliable and seems to handle 3D content well, but there's also Xming , which is free and open-source if that's a concern. Exceed is installed on all UCL's centrally-managed Windows computers. Installing Exceed \u00a7 Exceed comes in two parts: the main package, and the add-on that handles certain types of rendered 3D content. You can download both parts from the Exceed page in the UCL Software Database . First, install the main package, then the 3D package. Using PuTTY with Exceed \u00a7 When you run the Exceed program, it will automatically wait in the background until you run a remote application, so you only have to configure PuTTY to pass the data through. To do this, first fill in the details for your PuTTY connection as normal with the hostname and any other details you'd normally use to connect, and then, in the pane labelled \"Category\" on the left, click the \"+\" next to \"SSH\" and then select \"X11\". The main pane should change to one labelled \"Options controlling SSH X11 forwarding\". Check the box labelled \"Enable X11 Forwarding\". You can now click \"Open\" to start the connection, or you can return to the \"Session\" options to save this setup. macOS \u00a7 Like Windows, macOS doesn't come with an X server to receive X windows. The most commonly used X server for macOS is XQuartz . If you download and install that, you can follow the Linux instructions below. When you connect with X-Forwarding enabled, the XQuartz server program should start automatically, ready to present remote windows. Linux \u00a7 Almost all Linux versions that have a graphical desktop use an X server to provide it, so you don't have to install a separate one. You still have to set up your SSH client's connection to \"tunnel\" the X windows from the remote computer, though. You can do this by simply adding the -X option to your ssh command line, so for example to connect to Legion with X-Forwarding: ssh -X ccaaxyz@legion.rc.ucl.ac.uk To use X-Forwarding from outside UCL, you must either use the VPN, or use the Socrates gateway machine , with the appropriate flags for both ssh steps, for example: [me@my_computer ~]$ ssh -X ccaaxyz@socrates.ucl.ac.uk [...] [ccaaxyz@socrates-a ~]$ ssh -X ccaaxyz@legion.rc.ucl.ac.uk Note This assumes you use a Linux distribution that uses Xorg as its display server. If your distribution uses Wayland instead, and you aren't sure how to make this work, please contact us , letting us know what version of which distribution you're using. Checking your Setup \u00a7 There are some simple programs on the system that use X, which can be used to check whether your setup is working correctly. xterm is a terminal emulator -- it presents a terminal much like you would already be using. glxgears is a small test/benchmark program for the 3D remote rendering capability. It presents a small set of animated gears. If these work, you have a working X connection. If not, you should see an error when you try to run them that may look something like: xterm: Xt error: Can't open display: Or: Error: couldn't open display (null) If you see these, please check you have followed all the appropriate steps above, and if you still have problems, contact rc-support@ucl.ac.uk for assistance.","title":"X-Forwarding"},{"location":"Wiki_Export/X-Forwarding/#windows","text":"Windows doesn't natively have the ability to receive X windows, so you need to install an X server separately. There are a few choices; UCL has a site-wide license (covering UCL-owned and personal computers) for one called Exceed , which is pretty reliable and seems to handle 3D content well, but there's also Xming , which is free and open-source if that's a concern. Exceed is installed on all UCL's centrally-managed Windows computers.","title":"Windows"},{"location":"Wiki_Export/X-Forwarding/#installing-exceed","text":"Exceed comes in two parts: the main package, and the add-on that handles certain types of rendered 3D content. You can download both parts from the Exceed page in the UCL Software Database . First, install the main package, then the 3D package.","title":"Installing Exceed"},{"location":"Wiki_Export/X-Forwarding/#using-putty-with-exceed","text":"When you run the Exceed program, it will automatically wait in the background until you run a remote application, so you only have to configure PuTTY to pass the data through. To do this, first fill in the details for your PuTTY connection as normal with the hostname and any other details you'd normally use to connect, and then, in the pane labelled \"Category\" on the left, click the \"+\" next to \"SSH\" and then select \"X11\". The main pane should change to one labelled \"Options controlling SSH X11 forwarding\". Check the box labelled \"Enable X11 Forwarding\". You can now click \"Open\" to start the connection, or you can return to the \"Session\" options to save this setup.","title":"Using PuTTY with Exceed"},{"location":"Wiki_Export/X-Forwarding/#macos","text":"Like Windows, macOS doesn't come with an X server to receive X windows. The most commonly used X server for macOS is XQuartz . If you download and install that, you can follow the Linux instructions below. When you connect with X-Forwarding enabled, the XQuartz server program should start automatically, ready to present remote windows.","title":"macOS"},{"location":"Wiki_Export/X-Forwarding/#linux","text":"Almost all Linux versions that have a graphical desktop use an X server to provide it, so you don't have to install a separate one. You still have to set up your SSH client's connection to \"tunnel\" the X windows from the remote computer, though. You can do this by simply adding the -X option to your ssh command line, so for example to connect to Legion with X-Forwarding: ssh -X ccaaxyz@legion.rc.ucl.ac.uk To use X-Forwarding from outside UCL, you must either use the VPN, or use the Socrates gateway machine , with the appropriate flags for both ssh steps, for example: [me@my_computer ~]$ ssh -X ccaaxyz@socrates.ucl.ac.uk [...] [ccaaxyz@socrates-a ~]$ ssh -X ccaaxyz@legion.rc.ucl.ac.uk Note This assumes you use a Linux distribution that uses Xorg as its display server. If your distribution uses Wayland instead, and you aren't sure how to make this work, please contact us , letting us know what version of which distribution you're using.","title":"Linux"},{"location":"Wiki_Export/X-Forwarding/#checking-your-setup","text":"There are some simple programs on the system that use X, which can be used to check whether your setup is working correctly. xterm is a terminal emulator -- it presents a terminal much like you would already be using. glxgears is a small test/benchmark program for the 3D remote rendering capability. It presents a small set of animated gears. If these work, you have a working X connection. If not, you should see an error when you try to run them that may look something like: xterm: Xt error: Can't open display: Or: Error: couldn't open display (null) If you see these, please check you have followed all the appropriate steps above, and if you still have problems, contact rc-support@ucl.ac.uk for assistance.","title":"Checking your Setup"},{"location":"Wiki_Export/Background/Batch_Processing/","text":"An Introduction to Batch Processing \u00a7 When running jobs on Legion or Grace, users need to interact with the batch system. For users unfamiliar with HPC environments, this can be a way of working which is unfamiliar to them. What is a batch system? \u00a7 On a large, multi-user machine like Legion many users compete for relatively limited resources. There are two possible ways of organising access to this resource: Either allow everyone to run anything they want when they want but run the risk of people\u2019s jobs interfering with each other or else construct a system where users are allocated resources in turn. This is called a \"batch\" system. In a batch system, users submit their programs with a script to run them and a list of requirements and these jobs are run when resources are available. On Legion, the order jobs are run in is subject to a fair use policy which is discussed in the scheduling policy section. On other sites, users may be billed for their usage and most batch systems provide features for managing accounting in this scenario. When a user uses a batch system, they need to remember a number of important things. The first is that (with some exceptions) their jobs are not interactive. This means that a user must provide their application with inputs in advance (and if they are a developer design their program to operate in this manner). This means that some applications are not suitable for running in a batch system (visualisation for example). In most systems, each job is given a unique ID by the scheduler and this ID is used when interacting with jobs once they have been submitted. Once jobs have been submitted, users can log out and their jobs will execute even though they are not logged in. The second important thing to remember is that once a job has been submitted, a user has little control of when the job is actually run, because the time to completion (from submission) depends on how busy the machine is. It is therefore necessary for users to plan ahead and submit their jobs in a timely manner, rather than waiting until the last minute. Basic commands \u00a7 There are three basic commonly used commands in any batch system - one for submitting jobs, one for checking the status of jobs and one for deleting jobs. On Sun Grid Engine, these are qsub, qstat and qdel. qsub \u00a7 The qsub command submits your job to the batch queue. qsub myscript.sh You can override the settings in your script by specifying them on the command-line, so for example if you want to change the name of the job for this one instance of the job you can submit your script with: qsub -N NewName myscript.sh Or if you want to increase the wall-clock time to 24 hours: qsub -l h_rt=24:0:0 myscript.sh You can submit jobs with dependencies by using the -hold_jid option. For example, the command below submits a job that won't run until job 12345 has finished: qsub -hold_jid 12345 myscript.sh You may specify node type (see Resource Allocation section for more details) with the -ac allow= flags as below: qsub -ac allow=XYZ myscript.sh The example above would restrict the job to running on the older nodes. Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line, if possible. We can see your jobscript but not what command line you submitted with. qstat \u00a7 The qstat command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no-one else\u2019s). This makes it easier to keep track of your jobs. If you want to get more information on a particular job, note its job ID and then use the -f and -j flags to get full output about that job: `qstat -f -j 12345` If you see that your job is in Eqw state then an error occurred before your job could begin. You can see a truncated version of the error in the output of qstat -j - this is often enough to tell what the problem is if it is a file or directory not found. You can get the full error with qexplain . qexplain 12345 qdel \u00a7 The qdel command lets you delete a job from the queue. You need to provide qdel with a job ID like so: qdel 12345 You can delete all your jobs with: qdel '*' More Information \u00a7 If you wish to learn about additional commands, please run the command \"man qstat\" and take note of the commands shown in the \"SEE ALSO\" section of the manual page. Exit the manual page and then run the \"man\" command on those. If you cannot find the information you need in the man pages, then contact us at rc-support@ucl.ac.uk for assistance.","title":"Batch Processing"},{"location":"Wiki_Export/Background/Batch_Processing/#an-introduction-to-batch-processing","text":"When running jobs on Legion or Grace, users need to interact with the batch system. For users unfamiliar with HPC environments, this can be a way of working which is unfamiliar to them.","title":"An Introduction to Batch Processing"},{"location":"Wiki_Export/Background/Batch_Processing/#what-is-a-batch-system","text":"On a large, multi-user machine like Legion many users compete for relatively limited resources. There are two possible ways of organising access to this resource: Either allow everyone to run anything they want when they want but run the risk of people\u2019s jobs interfering with each other or else construct a system where users are allocated resources in turn. This is called a \"batch\" system. In a batch system, users submit their programs with a script to run them and a list of requirements and these jobs are run when resources are available. On Legion, the order jobs are run in is subject to a fair use policy which is discussed in the scheduling policy section. On other sites, users may be billed for their usage and most batch systems provide features for managing accounting in this scenario. When a user uses a batch system, they need to remember a number of important things. The first is that (with some exceptions) their jobs are not interactive. This means that a user must provide their application with inputs in advance (and if they are a developer design their program to operate in this manner). This means that some applications are not suitable for running in a batch system (visualisation for example). In most systems, each job is given a unique ID by the scheduler and this ID is used when interacting with jobs once they have been submitted. Once jobs have been submitted, users can log out and their jobs will execute even though they are not logged in. The second important thing to remember is that once a job has been submitted, a user has little control of when the job is actually run, because the time to completion (from submission) depends on how busy the machine is. It is therefore necessary for users to plan ahead and submit their jobs in a timely manner, rather than waiting until the last minute.","title":"What is a batch system?"},{"location":"Wiki_Export/Background/Batch_Processing/#basic-commands","text":"There are three basic commonly used commands in any batch system - one for submitting jobs, one for checking the status of jobs and one for deleting jobs. On Sun Grid Engine, these are qsub, qstat and qdel.","title":"Basic commands"},{"location":"Wiki_Export/Background/Batch_Processing/#qsub","text":"The qsub command submits your job to the batch queue. qsub myscript.sh You can override the settings in your script by specifying them on the command-line, so for example if you want to change the name of the job for this one instance of the job you can submit your script with: qsub -N NewName myscript.sh Or if you want to increase the wall-clock time to 24 hours: qsub -l h_rt=24:0:0 myscript.sh You can submit jobs with dependencies by using the -hold_jid option. For example, the command below submits a job that won't run until job 12345 has finished: qsub -hold_jid 12345 myscript.sh You may specify node type (see Resource Allocation section for more details) with the -ac allow= flags as below: qsub -ac allow=XYZ myscript.sh The example above would restrict the job to running on the older nodes. Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line, if possible. We can see your jobscript but not what command line you submitted with.","title":"qsub"},{"location":"Wiki_Export/Background/Batch_Processing/#qstat","text":"The qstat command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no-one else\u2019s). This makes it easier to keep track of your jobs. If you want to get more information on a particular job, note its job ID and then use the -f and -j flags to get full output about that job: `qstat -f -j 12345` If you see that your job is in Eqw state then an error occurred before your job could begin. You can see a truncated version of the error in the output of qstat -j - this is often enough to tell what the problem is if it is a file or directory not found. You can get the full error with qexplain . qexplain 12345","title":"qstat"},{"location":"Wiki_Export/Background/Batch_Processing/#qdel","text":"The qdel command lets you delete a job from the queue. You need to provide qdel with a job ID like so: qdel 12345 You can delete all your jobs with: qdel '*'","title":"qdel"},{"location":"Wiki_Export/Background/Batch_Processing/#more-information","text":"If you wish to learn about additional commands, please run the command \"man qstat\" and take note of the commands shown in the \"SEE ALSO\" section of the manual page. Exit the manual page and then run the \"man\" command on those. If you cannot find the information you need in the man pages, then contact us at rc-support@ucl.ac.uk for assistance.","title":"More Information"},{"location":"Wiki_Export/Background/Cluster_Computing/","text":"Cluster Computing at UCL \u00a7 UCL has a number of centrally-funded compute cluster facilities available, aimed at supporting all types of research at UCL. Legion , a mixed-use cluster hosted in UCL's Bloomsbury datacentres. Myriad , another mixed-use cluster hosted in UCL's portion of the Virtus datacentre in Slough. Grace , a cluster designed for medium-scale parallel workloads, hosted in UCL's portion of the Virtus datacentre in Slough. What is a cluster? \u00a7 A cluster is a large array of PCs or servers, referred to as nodes , networked together and often with a shared filesystem. Commonly in a shared cluster facility, a scheduler is used to take work from users and assign it to servers or groups of servers to be run as discrete jobs . Jobs can use more than one core or even more than one node simultaneously, communicating over special, faster types of network where available, to allow many cores to divide up the work. So is this the same thing as a supercomputer? \u00a7 Sort of. The term \"supercomputer\" nowadays usually refers to a large cluster designed to be able to run a single job in parallel over the whole machine, with an extremely fast network. In the past it was used as a catch-all term for a lot of computing installations more architecturally complex and power-hungry than an ordinary desktop computer or server. Why would a user choose to use a cluster? \u00a7 Clusters allow the use of many nodes simultaneously, without the user having to be present or to have a laptop or desktop computer in their office running all the time. This means that users can both run large parallel jobs and large numbers of serial jobs providing them with the ability to run jobs they cannot run locally, or get through work-loads that would be impractical on local resources. The clusters also have some nodes with more specialist hardware and some with extremely large quantities of RAM, allowing jobs that would be completely impossible on ordinary office machines. When should you not use a cluster? \u00a7 The vast majority of clusters run the Linux operating system rather than Windows. The central UCL clusters only run Linux, so if your applications only run on Windows, this service is not suitable for you. The available clusters are currently only x86_64-based (a.k.a. amd64, em64t), so if you need an alternative processor architecture (such as ARM or POWER), these are not suitable. Also, as these clusters are largely designed for work structured around using scripts, if your applications require you to enter commands while they're running, you will not be able to make full use of the service. (Some applications often look like they do but don't in practice, contact us if you're not sure.) What if I need more compute time/longer jobs/more storage? \u00a7 We recognise that researchers may sometimes require resources than the basic all-purpose allocation. Options for acquiring additional resources on a short or long term basis are described on the Additional Resource Requests page. Do I have to pay for any of this? \u00a7 The central UCL clusters are free at point of use for UCL researchers; you don't pay for the compute time or storage you use. Other compute resources you may have to pay for. How do I get help? \u00a7 Any questions about the central UCL clusters should go to the Research Computing Support Team at rc-support@ucl.ac.uk . The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided. Available 9:30am to 4:30pm, Monday to Friday, except on Bank Holidays and College Closures. We aim to provide you with a useful response within 24 hours. Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided. What if I need something totally different? \u00a7 Let us know your requirements and we may be able to suggest alternative computing facilities that you may be eligible to use. It will also allow us to take your needs into consideration for future acquisitions.","title":"Cluster Computing"},{"location":"Wiki_Export/Background/Cluster_Computing/#cluster-computing-at-ucl","text":"UCL has a number of centrally-funded compute cluster facilities available, aimed at supporting all types of research at UCL. Legion , a mixed-use cluster hosted in UCL's Bloomsbury datacentres. Myriad , another mixed-use cluster hosted in UCL's portion of the Virtus datacentre in Slough. Grace , a cluster designed for medium-scale parallel workloads, hosted in UCL's portion of the Virtus datacentre in Slough.","title":"Cluster Computing at UCL"},{"location":"Wiki_Export/Background/Cluster_Computing/#what-is-a-cluster","text":"A cluster is a large array of PCs or servers, referred to as nodes , networked together and often with a shared filesystem. Commonly in a shared cluster facility, a scheduler is used to take work from users and assign it to servers or groups of servers to be run as discrete jobs . Jobs can use more than one core or even more than one node simultaneously, communicating over special, faster types of network where available, to allow many cores to divide up the work.","title":"What is a cluster?"},{"location":"Wiki_Export/Background/Cluster_Computing/#so-is-this-the-same-thing-as-a-supercomputer","text":"Sort of. The term \"supercomputer\" nowadays usually refers to a large cluster designed to be able to run a single job in parallel over the whole machine, with an extremely fast network. In the past it was used as a catch-all term for a lot of computing installations more architecturally complex and power-hungry than an ordinary desktop computer or server.","title":"So is this the same thing as a supercomputer?"},{"location":"Wiki_Export/Background/Cluster_Computing/#why-would-a-user-choose-to-use-a-cluster","text":"Clusters allow the use of many nodes simultaneously, without the user having to be present or to have a laptop or desktop computer in their office running all the time. This means that users can both run large parallel jobs and large numbers of serial jobs providing them with the ability to run jobs they cannot run locally, or get through work-loads that would be impractical on local resources. The clusters also have some nodes with more specialist hardware and some with extremely large quantities of RAM, allowing jobs that would be completely impossible on ordinary office machines.","title":"Why would a user choose to use a cluster?"},{"location":"Wiki_Export/Background/Cluster_Computing/#when-should-you-not-use-a-cluster","text":"The vast majority of clusters run the Linux operating system rather than Windows. The central UCL clusters only run Linux, so if your applications only run on Windows, this service is not suitable for you. The available clusters are currently only x86_64-based (a.k.a. amd64, em64t), so if you need an alternative processor architecture (such as ARM or POWER), these are not suitable. Also, as these clusters are largely designed for work structured around using scripts, if your applications require you to enter commands while they're running, you will not be able to make full use of the service. (Some applications often look like they do but don't in practice, contact us if you're not sure.)","title":"When should you not use a cluster?"},{"location":"Wiki_Export/Background/Cluster_Computing/#what-if-i-need-more-compute-timelonger-jobsmore-storage","text":"We recognise that researchers may sometimes require resources than the basic all-purpose allocation. Options for acquiring additional resources on a short or long term basis are described on the Additional Resource Requests page.","title":"What if I need more compute time/longer jobs/more storage?"},{"location":"Wiki_Export/Background/Cluster_Computing/#do-i-have-to-pay-for-any-of-this","text":"The central UCL clusters are free at point of use for UCL researchers; you don't pay for the compute time or storage you use. Other compute resources you may have to pay for.","title":"Do I have to pay for any of this?"},{"location":"Wiki_Export/Background/Cluster_Computing/#how-do-i-get-help","text":"Any questions about the central UCL clusters should go to the Research Computing Support Team at rc-support@ucl.ac.uk . The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided. Available 9:30am to 4:30pm, Monday to Friday, except on Bank Holidays and College Closures. We aim to provide you with a useful response within 24 hours. Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided.","title":"How do I get help?"},{"location":"Wiki_Export/Background/Cluster_Computing/#what-if-i-need-something-totally-different","text":"Let us know your requirements and we may be able to suggest alternative computing facilities that you may be eligible to use. It will also allow us to take your needs into consideration for future acquisitions.","title":"What if I need something totally different?"},{"location":"Wiki_Export/Background/The_Scheduler/","text":"Error Article Removed Pending Rewrite","title":"How the scheduler works"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/","text":"Managing Data on Research Computing Systems \u00a7 This section contains information about storage hierarchy in RC systems and the ways in which you can move data in and out of them. These services should not be used to store data for which you have obligations under the UK Data Protection Act 1998. For further information and advice, please see: http://www.ico.org.uk/for_organisations/data_protection/the_guide At some point, you are going to need to copy input files on to our systems and copy your results back to your local machine or to a backup elsewhere. (It is good practice to always make your own backups). First we give generic instructions for doing so - do also check the machine-specific information on this page for details about things like dedicated transfer nodes, different file storage areas and quotas. Copying files using Linux or Mac OS X \u00a7 You can use the command-line utilities scp, sftp or rsync to copy your data about. Note that as our systems use Linux, you can use these instructions to copy data between systems (e.g. from Legion to Myriad) as well as from your own computer to a system. scp \u00a7 This will copy a data file from somewhere on your local machine to a specified location on the remote machine. scp <local_data_file> <remote_user_id>@<remote_hostname>:<remote_path> This will do the reverse, copying from the remote machine to your local machine. (Run from your local machine). scp <remote_user_id>@<remote_hostname>:<remote_path><remote_data_file> <local_path> To copy a whole directory with all its contents, use the -r option: scp -r <local_directory> <remote_user_id>@<remote_hostname>:<remote_path> sftp \u00a7 You can use sftp to log in to the remote machine, navigate through directories and use put and get to copy files from and to your local machine. lcd and lls are local equivalents of cd and ls so you can navigate through your local directories as you go. sftp <remote_user_id>@<remote_hostname> cd <remote_path> get <remote_file> lcd <local_path> put <local_file> rsync \u00a7 Rsync is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at the man pages as there are many options. Copying files using Windows and WinSCP \u00a7 WinSCP is a graphical client that you can use for scp or sftp. The login/create new session screen will open if this is the first time you are using WinSCP. You can choose SFTP or SCP as the file protocol. If you have an unstable connection with one, you may wish to try the other. SCP is probably generally better. Fill in the hostname of the machine you wish to connect to, your username and password. Click Save and give your settings a useful name. You'll then be shown your list of Stored sessions, which will have the one you just created. Select the session and click Login. Transferring files from outside the UCL firewall \u00a7 To transfer files when you are outside UCL's network, you can use the IS VPN , which will allow you to ssh directly into Legion or login05 as if you were inside the network, without having to go via Socrates first. If not using the VPN, you will need to do some form of ssh tunnelling - read on. Single-step login on Linux or Mac OS X using tunnelling \u00a7 Inside your ~/.ssh directory on your local machine, add the below to your config file (or create a file called config if you don't already have one). Generically, it should be of this form where can be anything you want to call this entry. Host <name> User <your.username> HostName <full.hostname> proxyCommand ssh -W <full.hostname>:22 <your.username>@socrates.ucl.ac.uk Here are some examples - you can have as many of these as you need in your config file. Host legion User <your.username> HostName legion.rc.ucl.ac.uk proxyCommand ssh -W legion.rc.ucl.ac.uk:22 <your.username>@socrates.ucl.ac.uk Host login05 User <your.username> HostName login05.external.legion.ucl.ac.uk proxyCommand ssh -W login05.external.legion.ucl.ac.uk:22 <your.username>@socrates.ucl.ac.uk Host aristotle User <your.username> HostName aristotle.rc.ucl.ac.uk proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 <your.username>@socrates.ucl.ac.uk You can now just do ssh legion or scp aristotle and you will go through Socrates. You'll be asked for login details twice since you're logging in to two machines. (Socrates uses central UCL credentials). Single-step login on Windows using tunnelling \u00a7 WinSCP can also set up SSH tunnels. Create a new session as before, and tick the Advanced options box in the bottom left corner. Select Connection > Tunnel from the left pane. Tick the Connect through SSH tunnel box and enter the hostname of the gateway you are tunnelling through, for example socrates.ucl.ac.uk Fill in your username and password for that host. (Central UCL ones for Socrates). Select Session from the left pane and fill in the hostname you want to end up on after the tunnel. Fill in your username and password for that host and set the file protocol to SCP. Save your settings with a useful name. Managing your quota \u00a7 To check your quota, run the command lquota . (Note: not currently available on Grace or Thomas - see below). Also useful is du , giving you information about your disk usage. For example, du -ch <dir> will give you a summary of the sizes of directory tree and subtrees, in human-readable sizes, with a total at the bottom. du -h --max-depth=1 will show you the totals for all top-level directories relative to where you are, plus the grand total. These can help you track down the locations of large amounts of data if you need to reduce your disk usage. To check your quota on Grace and Thomas, please run quota_check . This is part of the userscripts module which is loaded by default. Quotas for Lustre storage on Legion are automatically checked: if you use more than your quota for 14 days continuously, you will be prevented from submitting more jobs. We can increase quotas on-demand for small amounts, but for large amounts of storage (typically ~1TB or larger), a request must be submitted as described on the \"Additional Resource Requests\" page . Please contact rc-support@ucl.ac.uk for more information. Accessing Research Data Storage \u00a7 To access data stored in Research Data Storage, you can use scp to transfer data to/from the RC Services machine. Here is a guide on Connecting to Research Data Services . Legion-specific information \u00a7 To transfer your data to and from Legion in the most efficient manner, you must have a basic understanding of the bandwidth limitations of the machine. The entire system is limited by a 10Gb ethernet connection to UCL's network. If you find a significantly lower throughput, this may be caused by a bottleneck along the path between Legion and the machine you are trying to send the data to/from. Login and compute nodes are individually connected to the network via 1Gb/s links. This means that the maximum theoretical throughput that you can expect from any node in Legion is 125MB/s. Taking into account protocol overheads, you should see a maximum throughput of around 100MB/s on each node. For this reason we have to make the distinction between transferring large and modest amounts of data. Transferring modest amounts of data \u00a7 You can use Legion's login nodes to transfer modest amounts of data. Please bear in mind that there is no limit on the number of users that can be logged in to the Login nodes. Their activity may cause your data transfer rates to drop (as yours may affect theirs). If you need to transfer large amounts of data or need more reliable transfer rates, we recommend that you do so within a batch job, as described in the following section. If you are running scp from your local machine, use legion.rc.ucl.ac.uk as the hostname and you will transfer via the login nodes. Dedicated transfer node \u00a7 Legion provides a dedicated transfer node with ten gigabit network connections to the UCL network and to Legion. To access this node, log in via scp, sftp or ssh to: login05.external.legion.ucl.ac.uk Please note that you cannot submit or view the status of your compute jobs on this node - it is only available for data transfer. Legion's storage architecture and hierarchy \u00a7 Legion has three types of storage with distinct levels of performance, volatility and reliability: Home directories ($HOME) \u00a7 Smaller amount of storage Backed up Read-only access from compute nodes This storage has currently a hard quota of 50GB per user. It is the most reliable. But due to contention between several users it has very high performance variability. You can read and write to it from the login nodes, but only read access is granted from the compute nodes. The rationale for this stems from the fact that it cannot withstand large amounts of Input/Output such as that which happens within a large cluster like Legion. This is also to make sure that only important data is backed up as otherwise there would be excessive load on the backup system both in terms of performance and capacity. Shared scratch area ($HOME/Scratch) \u00a7 Larger storage Not backed up Writable by compute nodes The Scratch entry in your home directory is a symbolic link to the Lustre file system attached to Legion. This storage has very high throughput on individual files and allows separate processes on different machines to open the same file for I/O simultaneously. Due to its complexity, it performs very poorly when handling large amounts of small files. For example, running the \u201cfind\u201d command on a directory within $HOME/Scratch has much poorer performance than running in a directory stored under $HOME . Again, due to its complexity, it is prone to failure and may have to be completely reformatted if file system errors build up. Therefore, we give no guarantee that data stored on this file system is safe and strongly recommend that you save any important data as soon as possible to your $HOME directory via login nodes or any other external backup storage. Scratch and its underlying hardware are of vital importance for the good operation of the cluster in terms of I/O performance. Because this resource is scarce, the CRAG has enforced usage quotas, to be reviewed monthly. The use of Scratch is now subject to the following policies: All users will be granted an initial Scratch quota. Users are free to work within this quota but should note that files stored in their Scratch directory are NOT backed up and should therefore be considered \u2018at risk\u2019. Users may request increases in their Scratch quota by submitting a form to rc-support@ucl.ac.uk . They will be required to explain why they need an increase from a technical and computational point of view. The Scratch quota will be implemented as a soft quota so users will be able to temporarily exceed their quota for a short period. This period is set to 14 days by default. When a user exceeds their Scratch quota a warning is added to their Legion Message of the Day (MOTD) and an email is sent to their UCL email address. The warnings will display how much they are over quota, how long they have to reduce their usage and what will happen if they fail to reduce their usage. While a user continues to use more than their Scratch quota subsequent warning emails will be sent by the system every day. If a user reduces their usage of Scratch to below their quota within the 14 day grace period, no further action will be taken. At the end of the 14 day grace period if a user hasn't reduced their Scratch usage to below their quota, the system will stop the user from submitting any jobs until their Scratch usage is below their quota. In addition to user Scratch quotas there will be an overall limit on Lustre usage of 75% of total space, in order to ensure performance is maintained. If this limit is exceeded, ALL Legion users will be informed by email to their UCL email address of the risk to Lustre and requested to delete or move unwanted or currently unused files. Local node scratch area ( $TMPDIR ) \u00a7 Only exists during your job Fastest access as is local This storage resides on the hard drive installed in the compute nodes. It is only accessible temporarily throughout the duration of the job you have submitted and only within each of the compute nodes assigned to you. The path to this storage is set at run time in the $TMPDIR environment variable. This variable is set only within a shell generated by the SGE scheduler, and the path therein is unique to the node and job you are running. Once your job has completed the $TMPDIR directory is deleted on each node, so make sure that you have given enough wall clock time to allow data to be transferred back to your scratch area. Note that in parallel jobs running N slots (processes) only the main node ( slot 1 ) can be scripted as the remaining N -1 ones are just used to run compute processes without shell interaction - this means that your parallel program should only write to this local storage on process 1 (or 0, depending on the programming language). Users may automate the transfer of data from $TMPDIR to their scratch area by adding the directive #Local2Scratch to their job script. The amount of space available in $TMPDIR is controlled on a per node basis by the #$ -l tmpfs grid engine directive. This is to stop jobs interfering with each other by running out of disk space. If this directive is omitted from job scripts, then a default of 10 Gigabytes per node will be allocated to the job. For example to request 15 GB include the following in the job script: #$ -l tmpfs=15G The following diagram illustrates how these various levels of storage relate to login and compute nodes: Transferring files to or from Legion using a batch job \u00a7 Note: we recommend using the dedicated transfer node, as explained above, to perform large transfers. This method should only be used if the transfer node is unavailable for some reason. Using a batch job you can transfer multiple files or sections of a file simultaneously via each compute node directly onto Lustre, maximising the use of 10Gb/s link that connects Legion to the UCL network. You can use the scp or sftp commands on Legion to do file transfers between Legion and some remote machine of your choice, but both of these commands need to log in to the remote machine, and they need to do this WITHOUT any need for interactive input, i.e. without needing you to supply a password. The recommended method of doing this is to use RSA based authentication, i.e. using an SSH public/private key pair, generated by (for example) the Unix command ssh-keygen . Setting up the necessary Authentication Keys \u00a7 If you already have such a key pair for other purposes (e.g. for password-less login to Legion from your desktop machine), then all you need to do is to copy the private key file id_rsa into the ~/.ssh directory in your Legion home directory (making sure that it is readable only by you), and also copy the contents of the public key file id_rsa.pub into the file authorized_keys in the ~/.ssh directory on your remote machine. If you haven't generated an SSH key pair before, simply run this command on Legion: ssh-keygen -t rsa pressing return when it asks for passphrase. This will generate the key pair in the following two files in the directory .ssh in your Legion home directory: id_rsa (the private key) and id_rsa.pub (the public key) Finally, you need to copy the contents of the public key id_rsa.pub into the file authorized_keys (creating it first if necessary), in the .ssh directory under your home directory on your remote machine. Note that, in general, the private key is in the .ssh directory of the machine you are making the SSH connection FROM, and the public key is in the ~/.ssh directory of the machine you are connecting TO. You may well wish to connect between your remote machine and Legion in BOTH directions, e.g. to both SSH into Legion, and to allow Legion to do an scp/sftp file transfer to your remote machine: in this case you will have both public and private keys in the .ssh directories on both machines, thus allowing password-less connection in both directions. In the above description it is assumed that you are using a remote Unix machine; this method also works with a remote Windows machine, though the location of the .ssh directory is different, and may depend on your version of Windows. Using SCP or SFTP without interactive input in a batch job \u00a7 Having set up the SSH keys as described above, you can now use scp or sftp to do file transfer within a batch job. For instance, you could use an scp command like this: scp my_results <my_remote_userid>@<my_remote_hostname>:legion_files/my_results This will copy your Legion file my_results into a file of the same name in the directory legion_files on your remote machine <my_remote_hostname> . Note that the directory legion_files must already exist. Similarly, you could copy a data file from legion_files on your remote machine to Legion, but with one caviat - your $HOME directory in Legion is read-only, as viewed by the worker nodes! You must therefore copy your data to $HOME/Scratch : scp <my_remote_userid>@<my_remote_hostname>:legion_files/my_results $HOME/Scratch/my_results Either of these commands should be embedded in a simple job script like the one in the example below, which should be submitted using qsub (remember to supply your correct consortium and project for the job): 1 2 3 4 5 6 7 #!/bin/bash -l #$ -N Data_transfer #$ -l h_rt=1:0:0 #$ -l mem=1G #$ -P <your_project_name> #$ -wd /home/<your_user_id>/Scratch <scp_command> You can also use sftp to do a similar job, though in this case the situation is complicated by the fact that sftp normally takes its commands interactively. To get round this, supply the sftp commands with a file called sftp\\_script in your home directory, like this: cd legion_files put my_results exit and then supply this script name to the sftp command using the -b option, like this: sftp -b sftp_script <my_remote_userid>@<my_remote_hostname> This command will perform the same transfer as the first scp example above, and should be submitted as a batch job using a similar job script to the one shown. As usual, please contact research-computing@ucl.ac.uk if you need any assistance with doing this. Iridis-Specific Information \u00a7 The Iridis service was discontinued on 31 July 2015, and all the data it held in UCL user home directories was copied to Legion, into a read-only storage area only accessible from the Legion transfer node, login05: ``` /imports/iridis/ ``` This data was kept available until 18th April 2016.","title":"Managing Data on RC Systems"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#managing-data-on-research-computing-systems","text":"This section contains information about storage hierarchy in RC systems and the ways in which you can move data in and out of them. These services should not be used to store data for which you have obligations under the UK Data Protection Act 1998. For further information and advice, please see: http://www.ico.org.uk/for_organisations/data_protection/the_guide At some point, you are going to need to copy input files on to our systems and copy your results back to your local machine or to a backup elsewhere. (It is good practice to always make your own backups). First we give generic instructions for doing so - do also check the machine-specific information on this page for details about things like dedicated transfer nodes, different file storage areas and quotas.","title":"Managing Data on Research Computing Systems"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#copying-files-using-linux-or-mac-os-x","text":"You can use the command-line utilities scp, sftp or rsync to copy your data about. Note that as our systems use Linux, you can use these instructions to copy data between systems (e.g. from Legion to Myriad) as well as from your own computer to a system.","title":"Copying files using Linux or Mac OS X"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#scp","text":"This will copy a data file from somewhere on your local machine to a specified location on the remote machine. scp <local_data_file> <remote_user_id>@<remote_hostname>:<remote_path> This will do the reverse, copying from the remote machine to your local machine. (Run from your local machine). scp <remote_user_id>@<remote_hostname>:<remote_path><remote_data_file> <local_path> To copy a whole directory with all its contents, use the -r option: scp -r <local_directory> <remote_user_id>@<remote_hostname>:<remote_path>","title":"scp"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#sftp","text":"You can use sftp to log in to the remote machine, navigate through directories and use put and get to copy files from and to your local machine. lcd and lls are local equivalents of cd and ls so you can navigate through your local directories as you go. sftp <remote_user_id>@<remote_hostname> cd <remote_path> get <remote_file> lcd <local_path> put <local_file>","title":"sftp"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#rsync","text":"Rsync is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at the man pages as there are many options.","title":"rsync"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#copying-files-using-windows-and-winscp","text":"WinSCP is a graphical client that you can use for scp or sftp. The login/create new session screen will open if this is the first time you are using WinSCP. You can choose SFTP or SCP as the file protocol. If you have an unstable connection with one, you may wish to try the other. SCP is probably generally better. Fill in the hostname of the machine you wish to connect to, your username and password. Click Save and give your settings a useful name. You'll then be shown your list of Stored sessions, which will have the one you just created. Select the session and click Login.","title":"Copying files using Windows and WinSCP"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#transferring-files-from-outside-the-ucl-firewall","text":"To transfer files when you are outside UCL's network, you can use the IS VPN , which will allow you to ssh directly into Legion or login05 as if you were inside the network, without having to go via Socrates first. If not using the VPN, you will need to do some form of ssh tunnelling - read on.","title":"Transferring files from outside the UCL firewall"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#single-step-login-on-linux-or-mac-os-x-using-tunnelling","text":"Inside your ~/.ssh directory on your local machine, add the below to your config file (or create a file called config if you don't already have one). Generically, it should be of this form where can be anything you want to call this entry. Host <name> User <your.username> HostName <full.hostname> proxyCommand ssh -W <full.hostname>:22 <your.username>@socrates.ucl.ac.uk Here are some examples - you can have as many of these as you need in your config file. Host legion User <your.username> HostName legion.rc.ucl.ac.uk proxyCommand ssh -W legion.rc.ucl.ac.uk:22 <your.username>@socrates.ucl.ac.uk Host login05 User <your.username> HostName login05.external.legion.ucl.ac.uk proxyCommand ssh -W login05.external.legion.ucl.ac.uk:22 <your.username>@socrates.ucl.ac.uk Host aristotle User <your.username> HostName aristotle.rc.ucl.ac.uk proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 <your.username>@socrates.ucl.ac.uk You can now just do ssh legion or scp aristotle and you will go through Socrates. You'll be asked for login details twice since you're logging in to two machines. (Socrates uses central UCL credentials).","title":"Single-step login on Linux or Mac OS X using tunnelling"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#single-step-login-on-windows-using-tunnelling","text":"WinSCP can also set up SSH tunnels. Create a new session as before, and tick the Advanced options box in the bottom left corner. Select Connection > Tunnel from the left pane. Tick the Connect through SSH tunnel box and enter the hostname of the gateway you are tunnelling through, for example socrates.ucl.ac.uk Fill in your username and password for that host. (Central UCL ones for Socrates). Select Session from the left pane and fill in the hostname you want to end up on after the tunnel. Fill in your username and password for that host and set the file protocol to SCP. Save your settings with a useful name.","title":"Single-step login on Windows using tunnelling"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#managing-your-quota","text":"To check your quota, run the command lquota . (Note: not currently available on Grace or Thomas - see below). Also useful is du , giving you information about your disk usage. For example, du -ch <dir> will give you a summary of the sizes of directory tree and subtrees, in human-readable sizes, with a total at the bottom. du -h --max-depth=1 will show you the totals for all top-level directories relative to where you are, plus the grand total. These can help you track down the locations of large amounts of data if you need to reduce your disk usage. To check your quota on Grace and Thomas, please run quota_check . This is part of the userscripts module which is loaded by default. Quotas for Lustre storage on Legion are automatically checked: if you use more than your quota for 14 days continuously, you will be prevented from submitting more jobs. We can increase quotas on-demand for small amounts, but for large amounts of storage (typically ~1TB or larger), a request must be submitted as described on the \"Additional Resource Requests\" page . Please contact rc-support@ucl.ac.uk for more information.","title":"Managing your quota"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#accessing-research-data-storage","text":"To access data stored in Research Data Storage, you can use scp to transfer data to/from the RC Services machine. Here is a guide on Connecting to Research Data Services .","title":"Accessing Research Data Storage"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#legion-specific-information","text":"To transfer your data to and from Legion in the most efficient manner, you must have a basic understanding of the bandwidth limitations of the machine. The entire system is limited by a 10Gb ethernet connection to UCL's network. If you find a significantly lower throughput, this may be caused by a bottleneck along the path between Legion and the machine you are trying to send the data to/from. Login and compute nodes are individually connected to the network via 1Gb/s links. This means that the maximum theoretical throughput that you can expect from any node in Legion is 125MB/s. Taking into account protocol overheads, you should see a maximum throughput of around 100MB/s on each node. For this reason we have to make the distinction between transferring large and modest amounts of data.","title":"Legion-specific information"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#transferring-modest-amounts-of-data","text":"You can use Legion's login nodes to transfer modest amounts of data. Please bear in mind that there is no limit on the number of users that can be logged in to the Login nodes. Their activity may cause your data transfer rates to drop (as yours may affect theirs). If you need to transfer large amounts of data or need more reliable transfer rates, we recommend that you do so within a batch job, as described in the following section. If you are running scp from your local machine, use legion.rc.ucl.ac.uk as the hostname and you will transfer via the login nodes.","title":"Transferring modest amounts of data"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#dedicated-transfer-node","text":"Legion provides a dedicated transfer node with ten gigabit network connections to the UCL network and to Legion. To access this node, log in via scp, sftp or ssh to: login05.external.legion.ucl.ac.uk Please note that you cannot submit or view the status of your compute jobs on this node - it is only available for data transfer.","title":"Dedicated transfer node"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#legions-storage-architecture-and-hierarchy","text":"Legion has three types of storage with distinct levels of performance, volatility and reliability:","title":"Legion's storage architecture and hierarchy"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#home-directories-home","text":"Smaller amount of storage Backed up Read-only access from compute nodes This storage has currently a hard quota of 50GB per user. It is the most reliable. But due to contention between several users it has very high performance variability. You can read and write to it from the login nodes, but only read access is granted from the compute nodes. The rationale for this stems from the fact that it cannot withstand large amounts of Input/Output such as that which happens within a large cluster like Legion. This is also to make sure that only important data is backed up as otherwise there would be excessive load on the backup system both in terms of performance and capacity.","title":"Home directories ($HOME)"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#shared-scratch-area-homescratch","text":"Larger storage Not backed up Writable by compute nodes The Scratch entry in your home directory is a symbolic link to the Lustre file system attached to Legion. This storage has very high throughput on individual files and allows separate processes on different machines to open the same file for I/O simultaneously. Due to its complexity, it performs very poorly when handling large amounts of small files. For example, running the \u201cfind\u201d command on a directory within $HOME/Scratch has much poorer performance than running in a directory stored under $HOME . Again, due to its complexity, it is prone to failure and may have to be completely reformatted if file system errors build up. Therefore, we give no guarantee that data stored on this file system is safe and strongly recommend that you save any important data as soon as possible to your $HOME directory via login nodes or any other external backup storage. Scratch and its underlying hardware are of vital importance for the good operation of the cluster in terms of I/O performance. Because this resource is scarce, the CRAG has enforced usage quotas, to be reviewed monthly. The use of Scratch is now subject to the following policies: All users will be granted an initial Scratch quota. Users are free to work within this quota but should note that files stored in their Scratch directory are NOT backed up and should therefore be considered \u2018at risk\u2019. Users may request increases in their Scratch quota by submitting a form to rc-support@ucl.ac.uk . They will be required to explain why they need an increase from a technical and computational point of view. The Scratch quota will be implemented as a soft quota so users will be able to temporarily exceed their quota for a short period. This period is set to 14 days by default. When a user exceeds their Scratch quota a warning is added to their Legion Message of the Day (MOTD) and an email is sent to their UCL email address. The warnings will display how much they are over quota, how long they have to reduce their usage and what will happen if they fail to reduce their usage. While a user continues to use more than their Scratch quota subsequent warning emails will be sent by the system every day. If a user reduces their usage of Scratch to below their quota within the 14 day grace period, no further action will be taken. At the end of the 14 day grace period if a user hasn't reduced their Scratch usage to below their quota, the system will stop the user from submitting any jobs until their Scratch usage is below their quota. In addition to user Scratch quotas there will be an overall limit on Lustre usage of 75% of total space, in order to ensure performance is maintained. If this limit is exceeded, ALL Legion users will be informed by email to their UCL email address of the risk to Lustre and requested to delete or move unwanted or currently unused files.","title":"Shared scratch area ($HOME/Scratch)"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#local-node-scratch-area-tmpdir","text":"Only exists during your job Fastest access as is local This storage resides on the hard drive installed in the compute nodes. It is only accessible temporarily throughout the duration of the job you have submitted and only within each of the compute nodes assigned to you. The path to this storage is set at run time in the $TMPDIR environment variable. This variable is set only within a shell generated by the SGE scheduler, and the path therein is unique to the node and job you are running. Once your job has completed the $TMPDIR directory is deleted on each node, so make sure that you have given enough wall clock time to allow data to be transferred back to your scratch area. Note that in parallel jobs running N slots (processes) only the main node ( slot 1 ) can be scripted as the remaining N -1 ones are just used to run compute processes without shell interaction - this means that your parallel program should only write to this local storage on process 1 (or 0, depending on the programming language). Users may automate the transfer of data from $TMPDIR to their scratch area by adding the directive #Local2Scratch to their job script. The amount of space available in $TMPDIR is controlled on a per node basis by the #$ -l tmpfs grid engine directive. This is to stop jobs interfering with each other by running out of disk space. If this directive is omitted from job scripts, then a default of 10 Gigabytes per node will be allocated to the job. For example to request 15 GB include the following in the job script: #$ -l tmpfs=15G The following diagram illustrates how these various levels of storage relate to login and compute nodes:","title":"Local node scratch area ($TMPDIR)"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#transferring-files-to-or-from-legion-using-a-batch-job","text":"Note: we recommend using the dedicated transfer node, as explained above, to perform large transfers. This method should only be used if the transfer node is unavailable for some reason. Using a batch job you can transfer multiple files or sections of a file simultaneously via each compute node directly onto Lustre, maximising the use of 10Gb/s link that connects Legion to the UCL network. You can use the scp or sftp commands on Legion to do file transfers between Legion and some remote machine of your choice, but both of these commands need to log in to the remote machine, and they need to do this WITHOUT any need for interactive input, i.e. without needing you to supply a password. The recommended method of doing this is to use RSA based authentication, i.e. using an SSH public/private key pair, generated by (for example) the Unix command ssh-keygen .","title":"Transferring files to or from Legion using a batch job"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#setting-up-the-necessary-authentication-keys","text":"If you already have such a key pair for other purposes (e.g. for password-less login to Legion from your desktop machine), then all you need to do is to copy the private key file id_rsa into the ~/.ssh directory in your Legion home directory (making sure that it is readable only by you), and also copy the contents of the public key file id_rsa.pub into the file authorized_keys in the ~/.ssh directory on your remote machine. If you haven't generated an SSH key pair before, simply run this command on Legion: ssh-keygen -t rsa pressing return when it asks for passphrase. This will generate the key pair in the following two files in the directory .ssh in your Legion home directory: id_rsa (the private key) and id_rsa.pub (the public key) Finally, you need to copy the contents of the public key id_rsa.pub into the file authorized_keys (creating it first if necessary), in the .ssh directory under your home directory on your remote machine. Note that, in general, the private key is in the .ssh directory of the machine you are making the SSH connection FROM, and the public key is in the ~/.ssh directory of the machine you are connecting TO. You may well wish to connect between your remote machine and Legion in BOTH directions, e.g. to both SSH into Legion, and to allow Legion to do an scp/sftp file transfer to your remote machine: in this case you will have both public and private keys in the .ssh directories on both machines, thus allowing password-less connection in both directions. In the above description it is assumed that you are using a remote Unix machine; this method also works with a remote Windows machine, though the location of the .ssh directory is different, and may depend on your version of Windows.","title":"Setting up the necessary Authentication Keys"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#using-scp-or-sftp-without-interactive-input-in-a-batch-job","text":"Having set up the SSH keys as described above, you can now use scp or sftp to do file transfer within a batch job. For instance, you could use an scp command like this: scp my_results <my_remote_userid>@<my_remote_hostname>:legion_files/my_results This will copy your Legion file my_results into a file of the same name in the directory legion_files on your remote machine <my_remote_hostname> . Note that the directory legion_files must already exist. Similarly, you could copy a data file from legion_files on your remote machine to Legion, but with one caviat - your $HOME directory in Legion is read-only, as viewed by the worker nodes! You must therefore copy your data to $HOME/Scratch : scp <my_remote_userid>@<my_remote_hostname>:legion_files/my_results $HOME/Scratch/my_results Either of these commands should be embedded in a simple job script like the one in the example below, which should be submitted using qsub (remember to supply your correct consortium and project for the job): 1 2 3 4 5 6 7 #!/bin/bash -l #$ -N Data_transfer #$ -l h_rt=1:0:0 #$ -l mem=1G #$ -P <your_project_name> #$ -wd /home/<your_user_id>/Scratch <scp_command> You can also use sftp to do a similar job, though in this case the situation is complicated by the fact that sftp normally takes its commands interactively. To get round this, supply the sftp commands with a file called sftp\\_script in your home directory, like this: cd legion_files put my_results exit and then supply this script name to the sftp command using the -b option, like this: sftp -b sftp_script <my_remote_userid>@<my_remote_hostname> This command will perform the same transfer as the first scp example above, and should be submitted as a batch job using a similar job script to the one shown. As usual, please contact research-computing@ucl.ac.uk if you need any assistance with doing this.","title":"Using SCP or SFTP without interactive input in a batch job"},{"location":"Wiki_Export/needs_work/Managing_Data_on_RC_Systems/#iridis-specific-information","text":"The Iridis service was discontinued on 31 July 2015, and all the data it held in UCL user home directories was copied to Legion, into a read-only storage area only accessible from the Legion transfer node, login05: ``` /imports/iridis/ ``` This data was kept available until 18th April 2016.","title":"Iridis-Specific Information"},{"location":"Wiki_Export/needs_work/Resource_Allocation/","text":"For information about requesting additional resources see Additional Resource Requests . Governance \u00a7 Resource allocation policy for Research Computing services is determined by the CRAG , informed by the user community. The CRAG reports to the Research Computing Governance Group . Resource Allocation on Legion \u00a7 Allocation of computational resources on the Legion cluster is based on a combination of job class, wait time and fairshare. Resource allocation is based on four features of job: Memory size Core count Licenses Wall clock time Legion has nodes of several different types, listed below. The tmpfs is the maximum size of $TMPDIR that can be requested. Type Cores per node RAM per node Connectivity Nodes Total Cores tmpfs T 32 1511GB Ethernet 6 192 1536G U 16 64GB Infiniband 160 2560 792G V 12 + 2 GPU 48GB Ethernet 8 96 + 16GPU 358G X 12 24GB Infiniband 144 1728 173G Y 12 24GB Ethernet 99 1188 406G Z 12 48GB Ethernet 4 48 173G S 16 + 2 MIC 64GB Infiniband 10 160 + 20 MIC 1536G O 16 64GB Infiniband 36 576 792G P 12 + 1 K40c GPU 8GB Ethernet 1 12 + 1 K40c GPU 112G Q 32 512GB Ethernet 1 32 1024G Nodes of type W are the original nodes (now retired), whilst X, Y and Z are the new nodes added during the Legion III upgrade. Nodes of type X are for running parallel jobs, nodes of type Y are used for running jobs which can be run within one node (less than 12 cores) and nodes of type Z are used for jobs which require more memory. Types T, U and V were added later. Type T are for jobs with a high memory requirement. Type V are only for jobs which request a GPU. Type S are only for jobs which request a mic. The largest job that the general population of users can run on nodes of types U, S, O and Q will vary as they are mostly paid for by specific research groups. When a job is submitted it is evaluated and automatically assigned to one of several classes, based on information in the job submission script: In addition, as before, scheduling is based on the job type of the job. There are three job types: Multi-node jobs: These jobs require more than one node of the type they have been assigned to. Short, single-node jobs: These have a wall-clock time less than or equal to X minutes and fit within a single node of the type they have been assigned to. Long, single-node jobs: These have a wall-clock time greater than X and fit within a single node of the type they have been assigned to. X is under constant evaluation: Assumptions should not be made as to the value of X at any given time. Assignment rules \u00a7 The policy defined by the CRAG for scheduling jobs is based on the eight rules below: Resource requests that cannot be satisfied will cause the job to be rejected at submit time. The number of nodes a job will use will be determined independently for each node class. When this is combined with run time requirements, may cause the jobs to not be eligible to run on a particular node class. Single node jobs will share the node they run on with other single node jobs to the limit of available resources. Long Single Node jobs are banned from node classes X and U. Multi node jobs are banned from node classes Y and Z. Only jobs that request a GPU can use nodes of type V. Only jobs that request a MIC can use nodes of type S. Nodes of type T have a scheduling policy like Y/Z for jobs that use > 64GB RAM. Other jobs are limited to 1hr. (Eg smp jobs of 32 cores need to request > 2G per core). Jobs requiring Licenses will run on those nodes where they consume the fewest licenses. Users can specify which node classes their jobs will run on, provided that they do not contradict the policy set above. Any other resource matching will be done automatically by the scheduler to soonest available resource. Wallclock times \u00a7 Specific information related to wall-clock times and where jobs will run is summarised in the table below. Wall clock time X Y Z T U V S \\<=15 mins \\<=1 node \\<=1 node \\<=1 node \\<=1 node \\<=1 node* \\<=1 node* \\<=10 nodes* \\<=12 hours 2-72 nodes \\<=1 node \\<=1 node \\<=1 node* \\<=1-36 nodes* \\<=1 node* \\<=10 nodes* \\<=1 day 2-42 nodes \\<=1 node \\<=1 node \\<=1 node* 2-25 nodes* \\<=1 node* \\<=1 node* \\<=2 days 2-21 nodes \\<=1 node \\<=1 node \\<=1 node* 2-16 nodes* \\<=1 node* \\<=1 node* \\<=3 days 0 1 core 1 core 1 core* 0* 1 core* 0 * marks combinations that have other restrictions as described in the rules above. The priority of jobs is set as follows: \\<=15 min and \\<=1 node and eligible for X are set the lowest priority because it is expected that these jobs will obtain resources via backfill. All other jobs have priority set inversely proportional to the wall-clock time. With the exception of the jobs in point 1, there is no relationship between priority and job size. Fair-share and wait times are weighted in the priority calculation. In addition to the priority assigned based on job classes, jobs will also derive priority from fair share; jobs that have been waiting a long time, or have been submitted by a user and/or project that has not otherwise consumed many resources recently, will also acquire a higher priority. Note that despite these priority assignments it may take longer to assign resources for large jobs than for small ones. However, the higher priority assigned to large jobs should prevent smaller jobs from delaying them. Jobs that request 64 nodes or a little less may be delayed by the requirement to run within a computational unit. Estimating resources needed by your job \u00a7 It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. Run your program as: /usr/bin/time --verbose myprogram myargs where myprogram myargs is however you normally run your program, with whatever options you pass to it. When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is maxrss (maximum resident set size) which roughly tells you the largest amount of memory it used. Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible - if you increase it too much you may end up with a job that cannot be submitted. You can also look at nodesforjob $jobID when a job is running to see a snapshot of the memory, swap and load on the nodes your job is running on. (But if your job is sharing nodes it will show you the total resources in use, not just those used by your job). Bear in mind that memory use can increase over time as your job runs. Memory requests must be integers \u00a7 SoGE's memory specifiers are integers followed by a multiplier letter. Valid multiplier letters are k, K, m, M, g, G, t, and T, where k means multiply the value by 1000, K multiply by 1024, m multiply by 1000\u00d71000, M multiply by 1024\u00d71024, g multiply by 1000\u00d71000\u00d71000, G multiply by 1024\u00d71024\u00d71024, t multiply by 1000\u00d71000\u00d71000\u00d71000, and T multiply by 1024\u00d71024\u00d71024\u00d71024. If no multiplier is present, the value is just counted in bytes. These are valid: #$ -l mem=2500M #$ -l mem=1G #$ -l mem=1T but you cannot ask for 1.5G. Resource Allocation on Grace \u00a7 Grace is intended for parallel multi-node jobs requesting a minimum of 32 cores. All nodes on Grace have 16 cores and 64G RAM. The maximum tmpfs that can be requested is 100G. Jobs of less than 32 cores \u00a7 Jobs of less than 32 cores will only run on two of the compute nodes and the maximum wallclock for those is 12hrs, intended for testing purposes. If people submit many jobs of that size, the queue for those two nodes will be long. These workloads should be run on Myriad . Wallclock times \u00a7 Cores Max wallclock 32-256 48hrs 257-512 24hrs 513-10912 12hrs You may have a very long queue time if you try to use the maximum job size... Priority access requests \u00a7 Requests to run jobs outside the above limits should be addressed to rc-support@ucl.ac.uk for review (see Priority Access ). A clear justification for the request must be included; where requests are made to run jobs for longer than 3 days, it is expected: the code to be run has been optimised for the cluster you are running it on the code to be run cannot do checkpoint/restart without major modifications.","title":"Resource Allocation"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#governance","text":"Resource allocation policy for Research Computing services is determined by the CRAG , informed by the user community. The CRAG reports to the Research Computing Governance Group .","title":"Governance"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#resource-allocation-on-legion","text":"Allocation of computational resources on the Legion cluster is based on a combination of job class, wait time and fairshare. Resource allocation is based on four features of job: Memory size Core count Licenses Wall clock time Legion has nodes of several different types, listed below. The tmpfs is the maximum size of $TMPDIR that can be requested. Type Cores per node RAM per node Connectivity Nodes Total Cores tmpfs T 32 1511GB Ethernet 6 192 1536G U 16 64GB Infiniband 160 2560 792G V 12 + 2 GPU 48GB Ethernet 8 96 + 16GPU 358G X 12 24GB Infiniband 144 1728 173G Y 12 24GB Ethernet 99 1188 406G Z 12 48GB Ethernet 4 48 173G S 16 + 2 MIC 64GB Infiniband 10 160 + 20 MIC 1536G O 16 64GB Infiniband 36 576 792G P 12 + 1 K40c GPU 8GB Ethernet 1 12 + 1 K40c GPU 112G Q 32 512GB Ethernet 1 32 1024G Nodes of type W are the original nodes (now retired), whilst X, Y and Z are the new nodes added during the Legion III upgrade. Nodes of type X are for running parallel jobs, nodes of type Y are used for running jobs which can be run within one node (less than 12 cores) and nodes of type Z are used for jobs which require more memory. Types T, U and V were added later. Type T are for jobs with a high memory requirement. Type V are only for jobs which request a GPU. Type S are only for jobs which request a mic. The largest job that the general population of users can run on nodes of types U, S, O and Q will vary as they are mostly paid for by specific research groups. When a job is submitted it is evaluated and automatically assigned to one of several classes, based on information in the job submission script: In addition, as before, scheduling is based on the job type of the job. There are three job types: Multi-node jobs: These jobs require more than one node of the type they have been assigned to. Short, single-node jobs: These have a wall-clock time less than or equal to X minutes and fit within a single node of the type they have been assigned to. Long, single-node jobs: These have a wall-clock time greater than X and fit within a single node of the type they have been assigned to. X is under constant evaluation: Assumptions should not be made as to the value of X at any given time.","title":"Resource Allocation on Legion"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#assignment-rules","text":"The policy defined by the CRAG for scheduling jobs is based on the eight rules below: Resource requests that cannot be satisfied will cause the job to be rejected at submit time. The number of nodes a job will use will be determined independently for each node class. When this is combined with run time requirements, may cause the jobs to not be eligible to run on a particular node class. Single node jobs will share the node they run on with other single node jobs to the limit of available resources. Long Single Node jobs are banned from node classes X and U. Multi node jobs are banned from node classes Y and Z. Only jobs that request a GPU can use nodes of type V. Only jobs that request a MIC can use nodes of type S. Nodes of type T have a scheduling policy like Y/Z for jobs that use > 64GB RAM. Other jobs are limited to 1hr. (Eg smp jobs of 32 cores need to request > 2G per core). Jobs requiring Licenses will run on those nodes where they consume the fewest licenses. Users can specify which node classes their jobs will run on, provided that they do not contradict the policy set above. Any other resource matching will be done automatically by the scheduler to soonest available resource.","title":"Assignment rules"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#wallclock-times","text":"Specific information related to wall-clock times and where jobs will run is summarised in the table below. Wall clock time X Y Z T U V S \\<=15 mins \\<=1 node \\<=1 node \\<=1 node \\<=1 node \\<=1 node* \\<=1 node* \\<=10 nodes* \\<=12 hours 2-72 nodes \\<=1 node \\<=1 node \\<=1 node* \\<=1-36 nodes* \\<=1 node* \\<=10 nodes* \\<=1 day 2-42 nodes \\<=1 node \\<=1 node \\<=1 node* 2-25 nodes* \\<=1 node* \\<=1 node* \\<=2 days 2-21 nodes \\<=1 node \\<=1 node \\<=1 node* 2-16 nodes* \\<=1 node* \\<=1 node* \\<=3 days 0 1 core 1 core 1 core* 0* 1 core* 0 * marks combinations that have other restrictions as described in the rules above. The priority of jobs is set as follows: \\<=15 min and \\<=1 node and eligible for X are set the lowest priority because it is expected that these jobs will obtain resources via backfill. All other jobs have priority set inversely proportional to the wall-clock time. With the exception of the jobs in point 1, there is no relationship between priority and job size. Fair-share and wait times are weighted in the priority calculation. In addition to the priority assigned based on job classes, jobs will also derive priority from fair share; jobs that have been waiting a long time, or have been submitted by a user and/or project that has not otherwise consumed many resources recently, will also acquire a higher priority. Note that despite these priority assignments it may take longer to assign resources for large jobs than for small ones. However, the higher priority assigned to large jobs should prevent smaller jobs from delaying them. Jobs that request 64 nodes or a little less may be delayed by the requirement to run within a computational unit.","title":"Wallclock times"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#estimating-resources-needed-by-your-job","text":"It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. Run your program as: /usr/bin/time --verbose myprogram myargs where myprogram myargs is however you normally run your program, with whatever options you pass to it. When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is maxrss (maximum resident set size) which roughly tells you the largest amount of memory it used. Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible - if you increase it too much you may end up with a job that cannot be submitted. You can also look at nodesforjob $jobID when a job is running to see a snapshot of the memory, swap and load on the nodes your job is running on. (But if your job is sharing nodes it will show you the total resources in use, not just those used by your job). Bear in mind that memory use can increase over time as your job runs.","title":"Estimating resources needed by your job"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#memory-requests-must-be-integers","text":"SoGE's memory specifiers are integers followed by a multiplier letter. Valid multiplier letters are k, K, m, M, g, G, t, and T, where k means multiply the value by 1000, K multiply by 1024, m multiply by 1000\u00d71000, M multiply by 1024\u00d71024, g multiply by 1000\u00d71000\u00d71000, G multiply by 1024\u00d71024\u00d71024, t multiply by 1000\u00d71000\u00d71000\u00d71000, and T multiply by 1024\u00d71024\u00d71024\u00d71024. If no multiplier is present, the value is just counted in bytes. These are valid: #$ -l mem=2500M #$ -l mem=1G #$ -l mem=1T but you cannot ask for 1.5G.","title":"Memory requests must be integers"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#resource-allocation-on-grace","text":"Grace is intended for parallel multi-node jobs requesting a minimum of 32 cores. All nodes on Grace have 16 cores and 64G RAM. The maximum tmpfs that can be requested is 100G.","title":"Resource Allocation on Grace"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#jobs-of-less-than-32-cores","text":"Jobs of less than 32 cores will only run on two of the compute nodes and the maximum wallclock for those is 12hrs, intended for testing purposes. If people submit many jobs of that size, the queue for those two nodes will be long. These workloads should be run on Myriad .","title":"Jobs of less than 32 cores"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#wallclock-times_1","text":"Cores Max wallclock 32-256 48hrs 257-512 24hrs 513-10912 12hrs You may have a very long queue time if you try to use the maximum job size...","title":"Wallclock times"},{"location":"Wiki_Export/needs_work/Resource_Allocation/#priority-access-requests","text":"Requests to run jobs outside the above limits should be addressed to rc-support@ucl.ac.uk for review (see Priority Access ). A clear justification for the request must be included; where requests are made to run jobs for longer than 3 days, it is expected: the code to be run has been optimised for the cluster you are running it on the code to be run cannot do checkpoint/restart without major modifications.","title":"Priority access requests"},{"location":"Wiki_Export/needs_work/Running_Hybrid_OpenMP__MPI_Code/","text":"If you wish to submit a job that combines MPI and OpenMP parallelisation then you have a number of challenges you need to think about. First of all, you may need to use an MPI library that is thread-safe - that does not keep MPI state in shared memory between processor threads. Support for this in OpenMPI is still in development, and Intel MPI (the default on our systems) is recommended for this instead. This guide will give you are short walk-through of the process of writing, building and running a simple hybrid code on Legion. Set up modules \u00a7 The default modules are correct - in case you have others loaded, these are what you need: module unload compilers mpi module load compilers/intel/2015/update2 module load mpi/intel/2015/update3/intel You can check the MPI you have loaded by running mpif90 -v You should see something similar to the output below: mpif90 for the Intel(R) MPI Library 5.0 Update 3 for Linux* Copyright(C) 2003-2015, Intel Corporation. All rights reserved. ifort version 15.0.2 Compile the code \u00a7 To compile the code, you need to use the mpif90 compiler wrapper (or the C equivalent for your own C code) and pass it the -openmp option to enable the processing of OpenMP directives. Run: mpif90 -o hybrid -openmp hybrid.f90 This should produce a binary called \"hybrid\" in your current working directory. Submit the Job \u00a7 You will need to request the total number of cores you wish to use, and either set $OMP_NUM_THREADS for the number of OpenMP threads yourself, or allow it to be worked out automatically by setting it to OMP_NUM_THREADS=$(ppn) . That will set $OMP_NUM_THREADS to $NSLOTS/$NHOSTS , so you can use threads within a node and MPI between nodes and don't need to know in advance what size of node you are running on. GERun will then run $NSLOTS/$OMP_NUM_THREADS processes, round-robin allocated (if supported by the MPI). Therefore, if you want to use 24 cores on the type X nodes, with one MPI process per node and 12 threads per process you would request the example below. Note that if you are using multiple nodes and ppn , you get exclusive access to those nodes, so if you ask for 2.5 nodes-worth of cores you will end up with more threads on the last node than you thought you had. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash -l # Batch script to run a hybrid parallel job under SGE with Intel MPI. #$ -S /bin/bash # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N MadIntelHybrid # Select the MPI parallel environment and 24 cores. #$ -pe mpi 24 # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your_UCL_id/scratch/output/ # Automatically set threads to processes per node: if on X nodes = 12 OMP threads export OMP_NUM_THREADS=$(ppn) # Run our MPI job with the default modules. Gerun is a wrapper script for mpirun. gerun $HOME/src/madscience/madhybrid If you want to specify a specific number of OMP threads, you would alter the relevant lines above to this: # Run 12 MPI processes, each spawning 2 threads #$ -pe mpi 24 export OMP_NUM_THREADS=2 gerun your_binary","title":"Running Hybrid OpenMP/MPI Code"},{"location":"Wiki_Export/needs_work/Running_Hybrid_OpenMP__MPI_Code/#set-up-modules","text":"The default modules are correct - in case you have others loaded, these are what you need: module unload compilers mpi module load compilers/intel/2015/update2 module load mpi/intel/2015/update3/intel You can check the MPI you have loaded by running mpif90 -v You should see something similar to the output below: mpif90 for the Intel(R) MPI Library 5.0 Update 3 for Linux* Copyright(C) 2003-2015, Intel Corporation. All rights reserved. ifort version 15.0.2","title":"Set up modules"},{"location":"Wiki_Export/needs_work/Running_Hybrid_OpenMP__MPI_Code/#compile-the-code","text":"To compile the code, you need to use the mpif90 compiler wrapper (or the C equivalent for your own C code) and pass it the -openmp option to enable the processing of OpenMP directives. Run: mpif90 -o hybrid -openmp hybrid.f90 This should produce a binary called \"hybrid\" in your current working directory.","title":"Compile the code"},{"location":"Wiki_Export/needs_work/Running_Hybrid_OpenMP__MPI_Code/#submit-the-job","text":"You will need to request the total number of cores you wish to use, and either set $OMP_NUM_THREADS for the number of OpenMP threads yourself, or allow it to be worked out automatically by setting it to OMP_NUM_THREADS=$(ppn) . That will set $OMP_NUM_THREADS to $NSLOTS/$NHOSTS , so you can use threads within a node and MPI between nodes and don't need to know in advance what size of node you are running on. GERun will then run $NSLOTS/$OMP_NUM_THREADS processes, round-robin allocated (if supported by the MPI). Therefore, if you want to use 24 cores on the type X nodes, with one MPI process per node and 12 threads per process you would request the example below. Note that if you are using multiple nodes and ppn , you get exclusive access to those nodes, so if you ask for 2.5 nodes-worth of cores you will end up with more threads on the last node than you thought you had. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash -l # Batch script to run a hybrid parallel job under SGE with Intel MPI. #$ -S /bin/bash # Request ten minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:10:0 # Request 1 gigabyte of RAM (must be an integer) #$ -l mem=1G # Request 15 gigabyte of TMPDIR space per node (default is 10 GB) #$ -l tmpfs=15G # Set the name of the job. #$ -N MadIntelHybrid # Select the MPI parallel environment and 24 cores. #$ -pe mpi 24 # Set the working directory to somewhere in your scratch space. #$ -wd /home/<your_UCL_id/scratch/output/ # Automatically set threads to processes per node: if on X nodes = 12 OMP threads export OMP_NUM_THREADS=$(ppn) # Run our MPI job with the default modules. Gerun is a wrapper script for mpirun. gerun $HOME/src/madscience/madhybrid If you want to specify a specific number of OMP threads, you would alter the relevant lines above to this: # Run 12 MPI processes, each spawning 2 threads #$ -pe mpi 24 export OMP_NUM_THREADS=2 gerun your_binary","title":"Submit the Job"},{"location":"Wiki_Export/needs_work/Summary_of_Legion_changes/","text":"Host key warning \u00a7 The host keys for these login nodes and for legion.rc.ucl.ac.uk have changed, so when you try to log in you may get a warning from ssh saying that this has happened. You will need to remove the old keys from the known hosts list. Remove old host keys \u00a7 On Linux you can remove the old keys with: `ssh-keygen -R login09.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.59` `ssh-keygen -R login08.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.58` `ssh-keygen -R login07.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.57` `ssh-keygen -R login06.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.56` `ssh-keygen -R legion.rc.ucl.ac.uk` On Socrates you will probably need to edit your ~/.ssh/known_hosts file manually and delete the line for legion. (pico and vi are available text editors on Socrates). Using WinSCP the warning will look like this, and you will have the option to update the key. ``` 'Server's host key does not match the one that WinSCP has in cache.' # Modules There are some fairly significant changes to the modules available and of course newer versions of packages than were available under the old system. To see all the modules: module avail If you have the Legion default modules loaded in your .bashrc, then you should have the new default modules loaded automatically. module list Currently Loaded Modulefiles: 1) gcc-libs/4.9.2 7) subversion/1.8.13 13) rcps-core/1.0.0 2) cmake/3.2.1 8) screen/4.2.1 14) compilers/intel/2015/update2 3) flex/2.5.39 9) gerun 15) mpi/intel/2015/update3/intel 4) git/2.3.5 10) nano/2.4.2 16) default-modules 5) apr/1.5.2 11) nedit/5.6-aug15 6) apr-util/1.5.4 12) dos2unix/7.3 If you have \u201cmodule load\u201d commands in your .bashrc, you'll have to update them to reflect the changes to the module names/versions, otherwise you will see error messages. You can check our progress in installing all applications on the github repository for the module files here: <https://github.com/UCL-RITS/rcps-modulefiles> And the scripts that build the packages here: <https://github.com/UCL-RITS/rcps-buildscripts> # Jobscript differences ## Parallel environments for shared memory threads or MPI The way you request threads has changed: instead of using `#$ -l thr=4`, you would put this in your jobscript: # Request 4 threads #$ -pe smp 4 If you are using MPI, then there is only one parallel environment: # Request 4 MPI processes #$ -pe mpi 4 (`-pe mpi` is an alias for `-pe qlc` so you can use either and they are equivalent). ## RAM requested by shared memory jobs As a result of the change above, threaded jobs now also request RAM per core like MPI jobs do, rather than requesting the total amount. For example, asking for 4 threads and 12G RAM will give you a total of 48G RAM and not 12G as it was before. Check your memory requirements as you will greatly reduce the places your jobs can run if you leave them too high. ## Mixed-mode OpenMP and MPI This has changed significantly. You will now request the total number of cores you wish to use, and either set OMP\\_NUM\\_THREADS yourself, or allow it to be worked out automatically. # Run 12 MPI processes, each spawning 2 threads #$ -pe qlc 24 export OMP_NUM_THREADS=2 gerun your_binary The below will automatically set OMP\\_NUM\\_THREADS to $NSLOTS/$NHOSTS, so you will use threads within a node and MPI between nodes and don't need to know in advance what size of node you are running on. Gerun will then run $NSLOTS/$OMP\\_NUM\\_THREADS processes, round-robin allocated (if supported by the MPI). #$ -pe qlc 24 export OMP_NUM_THREADS=$(ppn) gerun your_binary ``` For example, if that runs on 2 x 12-core nodes, you'll get 2 MPI processes, each using 12 threads. Python \u00a7 There are python2/recommended and python3/recommended bundles. These use a virtualenv and have pip set up for you. They both have numpy and scipy available. See also Compiling#Python for how to install your own packages. To see what is already installed, the Python-shared list shows what is installed for both Python2 and 3, while the Python2 list and Python3 list show what is only installed for one or the other. (There may also be prereqs that aren't listed explicitly - pip will tell you if something is already installed as long as you have the bundle loaded).","title":"Summary of Legion changes"},{"location":"Wiki_Export/needs_work/Summary_of_Legion_changes/#host-key-warning","text":"The host keys for these login nodes and for legion.rc.ucl.ac.uk have changed, so when you try to log in you may get a warning from ssh saying that this has happened. You will need to remove the old keys from the known hosts list.","title":"Host key warning"},{"location":"Wiki_Export/needs_work/Summary_of_Legion_changes/#remove-old-host-keys","text":"On Linux you can remove the old keys with: `ssh-keygen -R login09.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.59` `ssh-keygen -R login08.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.58` `ssh-keygen -R login07.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.57` `ssh-keygen -R login06.external.legion.ucl.ac.uk` `ssh-keygen -R 193.60.225.56` `ssh-keygen -R legion.rc.ucl.ac.uk` On Socrates you will probably need to edit your ~/.ssh/known_hosts file manually and delete the line for legion. (pico and vi are available text editors on Socrates). Using WinSCP the warning will look like this, and you will have the option to update the key. ``` 'Server's host key does not match the one that WinSCP has in cache.' # Modules There are some fairly significant changes to the modules available and of course newer versions of packages than were available under the old system. To see all the modules: module avail If you have the Legion default modules loaded in your .bashrc, then you should have the new default modules loaded automatically. module list Currently Loaded Modulefiles: 1) gcc-libs/4.9.2 7) subversion/1.8.13 13) rcps-core/1.0.0 2) cmake/3.2.1 8) screen/4.2.1 14) compilers/intel/2015/update2 3) flex/2.5.39 9) gerun 15) mpi/intel/2015/update3/intel 4) git/2.3.5 10) nano/2.4.2 16) default-modules 5) apr/1.5.2 11) nedit/5.6-aug15 6) apr-util/1.5.4 12) dos2unix/7.3 If you have \u201cmodule load\u201d commands in your .bashrc, you'll have to update them to reflect the changes to the module names/versions, otherwise you will see error messages. You can check our progress in installing all applications on the github repository for the module files here: <https://github.com/UCL-RITS/rcps-modulefiles> And the scripts that build the packages here: <https://github.com/UCL-RITS/rcps-buildscripts> # Jobscript differences ## Parallel environments for shared memory threads or MPI The way you request threads has changed: instead of using `#$ -l thr=4`, you would put this in your jobscript: # Request 4 threads #$ -pe smp 4 If you are using MPI, then there is only one parallel environment: # Request 4 MPI processes #$ -pe mpi 4 (`-pe mpi` is an alias for `-pe qlc` so you can use either and they are equivalent). ## RAM requested by shared memory jobs As a result of the change above, threaded jobs now also request RAM per core like MPI jobs do, rather than requesting the total amount. For example, asking for 4 threads and 12G RAM will give you a total of 48G RAM and not 12G as it was before. Check your memory requirements as you will greatly reduce the places your jobs can run if you leave them too high. ## Mixed-mode OpenMP and MPI This has changed significantly. You will now request the total number of cores you wish to use, and either set OMP\\_NUM\\_THREADS yourself, or allow it to be worked out automatically. # Run 12 MPI processes, each spawning 2 threads #$ -pe qlc 24 export OMP_NUM_THREADS=2 gerun your_binary The below will automatically set OMP\\_NUM\\_THREADS to $NSLOTS/$NHOSTS, so you will use threads within a node and MPI between nodes and don't need to know in advance what size of node you are running on. Gerun will then run $NSLOTS/$OMP\\_NUM\\_THREADS processes, round-robin allocated (if supported by the MPI). #$ -pe qlc 24 export OMP_NUM_THREADS=$(ppn) gerun your_binary ``` For example, if that runs on 2 x 12-core nodes, you'll get 2 MPI processes, each using 12 threads.","title":"Remove old host keys"},{"location":"Wiki_Export/needs_work/Summary_of_Legion_changes/#python","text":"There are python2/recommended and python3/recommended bundles. These use a virtualenv and have pip set up for you. They both have numpy and scipy available. See also Compiling#Python for how to install your own packages. To see what is already installed, the Python-shared list shows what is installed for both Python2 and 3, while the Python2 list and Python3 list show what is only installed for one or the other. (There may also be prereqs that aren't listed explicitly - pip will tell you if something is already installed as long as you have the bundle loaded).","title":"Python"},{"location":"Wiki_Export/software/done/ANSYS/","text":"ANSYS Software \u00a7 UCL has licenses for ANSYS/CFX and ANSYS/Fluent, and we try to keep the copies on our clusters up-to-date. Before either application can be run, the user needs to go though a number of set up steps. These are detailed here. The ANSYS module needs to be loaded by issuing the command: module load ansys/14.5.7 The first time this is done, users should run the shell script setup_cfx.sh to configure licensing and HP-MPI options on a login node: setup_cfx.sh Running this script is required regardless of whether you are running ANSYS/CFX or ANSYS/Fluent. ANSYS/CFX and ANSYS/Fluent are intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer (up to two hours) on the User Test Nodes. Interactive work can be done using the ANSYS interactive tools provided you have X-windows functionality enabled though your ssh connection. See our User Guide for more information about enabling X-windows functionality and the User Test nodes. Our current licence allows up to ten ANSYS/CFX and ANSYS/Fluent jobs running at one time using no more than 64 cores in addition to cores on each job's head node. ANSYS/CFX Job Submission \u00a7 On Legion, there are a limited number of licenses (10 jobs, additional 64 cores) available for running CFX and Fluent jobs and in order to make sure that jobs only run if there are licenses available, it is necessary for users to request ANSYS licenses with their jobs, by adding \"-ac app=cfx\" to their job submission. In addition, because CFX handles its own parallelisation, a number of complex options need to be passed in job scripts to make it run correctly. Single node run \u00a7 Here is an example runscript for running cfx5solve multi-threaded on a given .def file including comments. Example Multi-threaded ANSYS/CFX Runscript \u00a7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run cfx5solve on the StaticMixer.def example # file, single node multi-threaded (4 threads), # 1. Force bash as the executing shell. #$ -S /bin/bash # 2. Request 15 munutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:15:0 # 3. Request 1 gigabyte of RAM. #$ -l mem=1G # 4. Set the name of the job. #$ -N StaticMixer_thread_4 # 5. Select 4 threads. #$ -l thr=4 # 6. Select the project that this job will run under. #$ -P <your project name> # 7. Request ANSYS licences #$ -ac app=cfx # 8. Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-14.5.7 #$ -wd /home/<your userid>/Scratch/cfxtests-14.5.7 # 9. Load the ANSYS module to set up your environment module load ansys/14.5.7 # 10. Copy the .def file into the working (current) directory cp /home/<your userid>/cfx_examples/StaticMixer.def . # 11. Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"15 [min]\" -def StaticMixer.def -par-local -partition $OMP_NUM_THREADS This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-StaticMixer-thr.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/cfxtests-14.5.7 SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. Replace the .def file in 10 and 11 by your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-thr.sh Output files will be saved in the job's working directory. Multi node run \u00a7 Here is an example runscript for running cfx5solve on more than one node (using MPI) on a given .def file including comments. Example Multi-node/MPI ANSYS/CFX Runscript \u00a7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run cfx5solve on the StaticMixer.def example # file, distributed parallel (36 cores), # Request one hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:00:0 # Request 2 gigabyte of RAM. #$ -l mem=2G # Set the name of the job. #$ -N StaticMixer_P_dist_36 # Select the QLogic parallel environment (qlc) and 36 processors. #$ -pe qlc 36 # Select the project that this job will run under. #$ -P <your project name> # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-14.5.7 #$ -wd /home/<your userid>/Scratch/cfxtests-14.5.7 # Load the ANSYS module to set up your environment module load ansys/14.5.7 # Copy the .def file into the working (current) directory cp /home/<your userid>/cfx_examples/StaticMixer.def . # SGE puts the machine file in $TMPDIR/machines. Use this to generate the # string CFX_NODES needed by cfx5solve export CFX_NODES=`cfxnodes $TMPDIR/machines` # Force infinipath export MPI_IC_ORDER=PSM # Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"60 [min]\" -def StaticMixer.def -start-method \"Platform MPI Distributed Parallel\" -par-dist $CFX_NODES This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-StaticMixer-par.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/cfxtests-14.5.7 SGE directives and may need to change the memory, wallclock time, number of MPI Processors (item 5) and job name directives as well. Replace the .def file in 10 and 13 by your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-par.sh Output files will be saved in the job's working directory. ANSYS/Fluent Job Submission \u00a7 On Legion, there are a limited number of licenses (10 jobs, additional 64 cores) available for running CFX and Fluent jobs and in order to make sure that jobs only run if there are licenses available, it is necessary for users to request ANSYS licenses with their jobs, by adding -ac app=cfx to their job submission. In addition, because Fluent handles its own parallelisation, a number of complex options need to be passed in job scripts to make it run correctly. Serial Run \u00a7 Here is an example runscript for running Fluent in serial mode (1 core) with comments. Example Serial ANSYS/Fluent Runscript \u00a7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run ANSYS/fluent in serial mode # (1 core). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabyte of RAM. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_ser1 # Select the project that this job will run under. #$ -P <your project ID> # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-14.5.7 #$ -wd /home/<your userid>/Scratch/fluent-tests-14.5.7 # Load the ANSYS module to set up your environment module load ansys/14.5.7 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 2D single precision (-g no GUI). For double precision use # 2ddp. For 3D use 3d or 3ddp. fluent 2d -g < test-1.in This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-ANSYS-fluent-ser.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/fluent-tests-14.5.7 SGE directives and may need to change the memory, wallclock time, and job name directives as well. Replace the .cas and .in files in 9 and 10 by your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-ser.sh Output files will be saved in the job's working directory. Parallel Run \u00a7 Here is an example runscript for running Fluent in parallel potentially across more than one node. Example Parallel (MPI) ANSYS/Fluent Runscript \u00a7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run ANSYS/fluent distributed parallel # (8 cores). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabyte of RAM. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_par8 # Select the QLogic parallel environment (qlc) and 8 processors. #$ -pe qlc 8 # Select the project that this job will run under. #$ -P <your project ID> # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-14.5.7 #$ -wd /home/<your userid>/Scratch/fluent-tests-14.5.7 # Load the ANSYS module to set up your environment module load ansys/14.5.7 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 3D single precision (-g no GUI). For double precision use # 3ddp. For 2D use 2d or 2ddp. # Do not change -t, -mpi, -pinfiniband and -cnf options. fluent 3d -t$NSLOTS -mpi=pcmpi -pinfiniband -cnf=$TMPDIR/machines -g < test-1.in This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-ANSYS-fluent-par.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/fluent-tests-14.5.7 SGE directives and may need to change the memory, wallclock time, number of MPI Processors (item 5) and job name directives as well. Replace the .cas and .in files in 10 and 11 by your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-ser.sh Output files will be saved in the job's working directory.","title":"ANSYS Software"},{"location":"Wiki_Export/software/done/ANSYS/#ansys-software","text":"UCL has licenses for ANSYS/CFX and ANSYS/Fluent, and we try to keep the copies on our clusters up-to-date. Before either application can be run, the user needs to go though a number of set up steps. These are detailed here. The ANSYS module needs to be loaded by issuing the command: module load ansys/14.5.7 The first time this is done, users should run the shell script setup_cfx.sh to configure licensing and HP-MPI options on a login node: setup_cfx.sh Running this script is required regardless of whether you are running ANSYS/CFX or ANSYS/Fluent. ANSYS/CFX and ANSYS/Fluent are intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer (up to two hours) on the User Test Nodes. Interactive work can be done using the ANSYS interactive tools provided you have X-windows functionality enabled though your ssh connection. See our User Guide for more information about enabling X-windows functionality and the User Test nodes. Our current licence allows up to ten ANSYS/CFX and ANSYS/Fluent jobs running at one time using no more than 64 cores in addition to cores on each job's head node.","title":"ANSYS Software"},{"location":"Wiki_Export/software/done/ANSYS/#ansyscfx-job-submission","text":"On Legion, there are a limited number of licenses (10 jobs, additional 64 cores) available for running CFX and Fluent jobs and in order to make sure that jobs only run if there are licenses available, it is necessary for users to request ANSYS licenses with their jobs, by adding \"-ac app=cfx\" to their job submission. In addition, because CFX handles its own parallelisation, a number of complex options need to be passed in job scripts to make it run correctly.","title":"ANSYS/CFX Job Submission"},{"location":"Wiki_Export/software/done/ANSYS/#single-node-run","text":"Here is an example runscript for running cfx5solve multi-threaded on a given .def file including comments.","title":"Single node run"},{"location":"Wiki_Export/software/done/ANSYS/#example-multi-threaded-ansyscfx-runscript","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run cfx5solve on the StaticMixer.def example # file, single node multi-threaded (4 threads), # 1. Force bash as the executing shell. #$ -S /bin/bash # 2. Request 15 munutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:15:0 # 3. Request 1 gigabyte of RAM. #$ -l mem=1G # 4. Set the name of the job. #$ -N StaticMixer_thread_4 # 5. Select 4 threads. #$ -l thr=4 # 6. Select the project that this job will run under. #$ -P <your project name> # 7. Request ANSYS licences #$ -ac app=cfx # 8. Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-14.5.7 #$ -wd /home/<your userid>/Scratch/cfxtests-14.5.7 # 9. Load the ANSYS module to set up your environment module load ansys/14.5.7 # 10. Copy the .def file into the working (current) directory cp /home/<your userid>/cfx_examples/StaticMixer.def . # 11. Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"15 [min]\" -def StaticMixer.def -par-local -partition $OMP_NUM_THREADS This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-StaticMixer-thr.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/cfxtests-14.5.7 SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. Replace the .def file in 10 and 11 by your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-thr.sh Output files will be saved in the job's working directory.","title":"Example Multi-threaded ANSYS/CFX Runscript"},{"location":"Wiki_Export/software/done/ANSYS/#multi-node-run","text":"Here is an example runscript for running cfx5solve on more than one node (using MPI) on a given .def file including comments.","title":"Multi node run"},{"location":"Wiki_Export/software/done/ANSYS/#example-multi-nodempi-ansyscfx-runscript","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run cfx5solve on the StaticMixer.def example # file, distributed parallel (36 cores), # Request one hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:00:0 # Request 2 gigabyte of RAM. #$ -l mem=2G # Set the name of the job. #$ -N StaticMixer_P_dist_36 # Select the QLogic parallel environment (qlc) and 36 processors. #$ -pe qlc 36 # Select the project that this job will run under. #$ -P <your project name> # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory cfxtests-14.5.7 #$ -wd /home/<your userid>/Scratch/cfxtests-14.5.7 # Load the ANSYS module to set up your environment module load ansys/14.5.7 # Copy the .def file into the working (current) directory cp /home/<your userid>/cfx_examples/StaticMixer.def . # SGE puts the machine file in $TMPDIR/machines. Use this to generate the # string CFX_NODES needed by cfx5solve export CFX_NODES=`cfxnodes $TMPDIR/machines` # Force infinipath export MPI_IC_ORDER=PSM # Run cfx5solve - Note: -max-elapsed-time needs to be set to the same # time as defined by 2 above. cfx5solve -max-elapsed-time \"60 [min]\" -def StaticMixer.def -start-method \"Platform MPI Distributed Parallel\" -par-dist $CFX_NODES This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-StaticMixer-par.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/cfxtests-14.5.7 SGE directives and may need to change the memory, wallclock time, number of MPI Processors (item 5) and job name directives as well. Replace the .def file in 10 and 13 by your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg: qsub run-StaticMixer-par.sh Output files will be saved in the job's working directory.","title":"Example Multi-node/MPI ANSYS/CFX Runscript"},{"location":"Wiki_Export/software/done/ANSYS/#ansysfluent-job-submission","text":"On Legion, there are a limited number of licenses (10 jobs, additional 64 cores) available for running CFX and Fluent jobs and in order to make sure that jobs only run if there are licenses available, it is necessary for users to request ANSYS licenses with their jobs, by adding -ac app=cfx to their job submission. In addition, because Fluent handles its own parallelisation, a number of complex options need to be passed in job scripts to make it run correctly.","title":"ANSYS/Fluent Job Submission"},{"location":"Wiki_Export/software/done/ANSYS/#serial-run","text":"Here is an example runscript for running Fluent in serial mode (1 core) with comments.","title":"Serial Run"},{"location":"Wiki_Export/software/done/ANSYS/#example-serial-ansysfluent-runscript","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run ANSYS/fluent in serial mode # (1 core). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabyte of RAM. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_ser1 # Select the project that this job will run under. #$ -P <your project ID> # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-14.5.7 #$ -wd /home/<your userid>/Scratch/fluent-tests-14.5.7 # Load the ANSYS module to set up your environment module load ansys/14.5.7 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 2D single precision (-g no GUI). For double precision use # 2ddp. For 3D use 3d or 3ddp. fluent 2d -g < test-1.in This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-ANSYS-fluent-ser.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/fluent-tests-14.5.7 SGE directives and may need to change the memory, wallclock time, and job name directives as well. Replace the .cas and .in files in 9 and 10 by your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-ser.sh Output files will be saved in the job's working directory.","title":"Example Serial ANSYS/Fluent Runscript"},{"location":"Wiki_Export/software/done/ANSYS/#parallel-run","text":"Here is an example runscript for running Fluent in parallel potentially across more than one node.","title":"Parallel Run"},{"location":"Wiki_Export/software/done/ANSYS/#example-parallel-mpi-ansysfluent-runscript","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/bin/bash -l # ANSYS 14.5.7: Batch script to run ANSYS/fluent distributed parallel # (8 cores). # Request 2 hours of wallclock time (format hours:minutes:seconds). #$ -l h_rt=2:0:0 # Request 2 gigabyte of RAM. #$ -l mem=2G # Set the name of the job. #$ -N Fluent_par8 # Select the QLogic parallel environment (qlc) and 8 processors. #$ -pe qlc 8 # Select the project that this job will run under. #$ -P <your project ID> # Request ANSYS licences #$ -ac app=cfx # Set the working directory to somewhere in your scratch space. In this # case the subdirectory fluent-tests-14.5.7 #$ -wd /home/<your userid>/Scratch/fluent-tests-14.5.7 # Load the ANSYS module to set up your environment module load ansys/14.5.7 # Copy Fluent input files into the working (current) directory cp <path to your input files>/test-1.cas . cp <path to your input files>/test-1.in . # Run fluent in 3D single precision (-g no GUI). For double precision use # 3ddp. For 2D use 2d or 2ddp. # Do not change -t, -mpi, -pinfiniband and -cnf options. fluent 3d -t$NSLOTS -mpi=pcmpi -pinfiniband -cnf=$TMPDIR/machines -g < test-1.in This runscript is available on Legion in the file: /shared/ucl/apps/ansys/14.5.7/ucl/share/run-ANSYS-fluent-par.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_name> and -wd /home/<your_UCL_id>/Scratch/fluent-tests-14.5.7 SGE directives and may need to change the memory, wallclock time, number of MPI Processors (item 5) and job name directives as well. Replace the .cas and .in files in 10 and 11 by your ones. The simplest form of qsub command can be used to submit the job eg: qsub run-ANSYS-fluent-ser.sh Output files will be saved in the job's working directory.","title":"Example Parallel (MPI) ANSYS/Fluent Runscript"},{"location":"Wiki_Export/software/done/ATLAS/","text":"ATLAS \u00a7 ATLAS (Automatically Tuned Linear Algebra Software) provides self-tuning implementations of BLAS and LAPACK. We have versions of ATLAS available that were built with various different compilers. Linking ATLAS 3.10.2 \u00a7 There are ATLAS modules for GCC 4.9.2 and the Intel 2015 compilers. The modules add the relevant lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH. You can see what those are with module show followed by the module name. ATLAS website Dynamic linking \u00a7 There is one combined library each for serial and threaded ATLAS (in most circumstances you probably want the serial version). Serial: -L${ATLASROOT}/lib -lsatlas Threaded: -L${ATLASROOT}/lib -ltatlas Static linking \u00a7 There are multiple libraries. Serial: -L${ATLASROOT}/lib -llapack -lf77blas -lcblas -latlas Threaded: -L${ATLASROOT}/lib -llapack -lptf77blas -lptcblas -latlas Runtime errors \u00a7 If you get a runtime error saying that libgfortran.so cannot be found, you need to add -lgfortran to your link line. The Intel equivalent is -lifcore . You can run module show <compiler module> with the compiler you are using to see where the libraries are located.","title":"ATLAS"},{"location":"Wiki_Export/software/done/ATLAS/#atlas","text":"ATLAS (Automatically Tuned Linear Algebra Software) provides self-tuning implementations of BLAS and LAPACK. We have versions of ATLAS available that were built with various different compilers.","title":"ATLAS"},{"location":"Wiki_Export/software/done/ATLAS/#linking-atlas-3102","text":"There are ATLAS modules for GCC 4.9.2 and the Intel 2015 compilers. The modules add the relevant lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH. You can see what those are with module show followed by the module name. ATLAS website","title":"Linking ATLAS 3.10.2"},{"location":"Wiki_Export/software/done/ATLAS/#dynamic-linking","text":"There is one combined library each for serial and threaded ATLAS (in most circumstances you probably want the serial version). Serial: -L${ATLASROOT}/lib -lsatlas Threaded: -L${ATLASROOT}/lib -ltatlas","title":"Dynamic linking"},{"location":"Wiki_Export/software/done/ATLAS/#static-linking","text":"There are multiple libraries. Serial: -L${ATLASROOT}/lib -llapack -lf77blas -lcblas -latlas Threaded: -L${ATLASROOT}/lib -llapack -lptf77blas -lptcblas -latlas","title":"Static linking"},{"location":"Wiki_Export/software/done/ATLAS/#runtime-errors","text":"If you get a runtime error saying that libgfortran.so cannot be found, you need to add -lgfortran to your link line. The Intel equivalent is -lifcore . You can run module show <compiler module> with the compiler you are using to see where the libraries are located.","title":"Runtime errors"},{"location":"Wiki_Export/software/done/BEAST/","text":"BEAST \u00a7 BEAST is an application for Bayesian MCMC analysis of molecular sequences orientated towards rooted, time-measured phylogenies inferred using strict or relaxed molecular clock models. It can be used as a method of reconstructing phylogenies but is also a framework for testing evolutionary hypotheses without conditioning on a single tree topology. The installation on Legion includes the extra applications Tracer and FigTree. Currently BEAST on Legion doesn't work with the BEAGLE high-performance library. BEAST is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. To use BEAST, Tracer and FigTree you need to load the following modules before running any of the applications: module load java/1.6.0_32 module load beast/1.7.4 Here is an example run script for submitting batch jobs to the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash -l # Batch script to run an OpenMP threaded BEAST job on Legion with the upgraded # software stack under SGE. # This version works with the modules environment upgraded in Feb 2012. # BEAST Version 1.7.4 # Request 15 minutes of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=0:15:0 # Request 1 gigabyte of RAM. Change this to suit your requirements. #$ -l mem=1G # Set the name of the job. You can change this if you wish. #$ -N BEAST_job_1 # Select 4 threads (the most possible on Legion is 12). NOTE: BEAST currently # can only use 1 thread per partition. #$ -l thr=4 # Select the project that this job will run under. # Find <your_project_id> by running the command \"groups\" #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. This is # a necessary step with the upgraded software stack as compute nodes cannot # write to $HOME. # # NOTE: this directory must exist. # # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/BEAST # Your work *must* be done in $TMPDIR cd $TMPDIR # Load correct modules for BEAST module load java/1.6.0_32 module load beast/1.7.4 # Copy input XML file to TMPDIR and run BEAST cp $infile . beast_infile=`basename $infile` echo \"Running BEAST with $OMP_NUM_THREADS threads ...\" beast -threads $OMP_NUM_THREADS $beast_infile # Preferably, tar-up (archive) all output files onto the shared scratch area # this will include the R_output file above. tar zcvf $HOME/Scratch/BEAST/files_from_job_$JOB_ID $TMPDIR A copy of this runscript is available on Legion in: /shared/ucl/apps/BEAST/run-beast.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_id> and -wd /home/<your_UCL_id>/Scratch/BEAST grid engine directives. You may also need to change the -l thr=4 , -l mem=1G and -l h_rt=0:15:0 directives. Submit the script using a qsub command like: qsub -v infile=`pwd`/gopher.xml run-beast.sh replacing gopher.xml with your BEAST XML file. Output will be written to $TMPDIR and so will need to be copied back to your ~/Scratch directory.","title":"BEAST"},{"location":"Wiki_Export/software/done/BEAST/#beast","text":"BEAST is an application for Bayesian MCMC analysis of molecular sequences orientated towards rooted, time-measured phylogenies inferred using strict or relaxed molecular clock models. It can be used as a method of reconstructing phylogenies but is also a framework for testing evolutionary hypotheses without conditioning on a single tree topology. The installation on Legion includes the extra applications Tracer and FigTree. Currently BEAST on Legion doesn't work with the BEAGLE high-performance library. BEAST is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. To use BEAST, Tracer and FigTree you need to load the following modules before running any of the applications: module load java/1.6.0_32 module load beast/1.7.4 Here is an example run script for submitting batch jobs to the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash -l # Batch script to run an OpenMP threaded BEAST job on Legion with the upgraded # software stack under SGE. # This version works with the modules environment upgraded in Feb 2012. # BEAST Version 1.7.4 # Request 15 minutes of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=0:15:0 # Request 1 gigabyte of RAM. Change this to suit your requirements. #$ -l mem=1G # Set the name of the job. You can change this if you wish. #$ -N BEAST_job_1 # Select 4 threads (the most possible on Legion is 12). NOTE: BEAST currently # can only use 1 thread per partition. #$ -l thr=4 # Select the project that this job will run under. # Find <your_project_id> by running the command \"groups\" #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. This is # a necessary step with the upgraded software stack as compute nodes cannot # write to $HOME. # # NOTE: this directory must exist. # # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/BEAST # Your work *must* be done in $TMPDIR cd $TMPDIR # Load correct modules for BEAST module load java/1.6.0_32 module load beast/1.7.4 # Copy input XML file to TMPDIR and run BEAST cp $infile . beast_infile=`basename $infile` echo \"Running BEAST with $OMP_NUM_THREADS threads ...\" beast -threads $OMP_NUM_THREADS $beast_infile # Preferably, tar-up (archive) all output files onto the shared scratch area # this will include the R_output file above. tar zcvf $HOME/Scratch/BEAST/files_from_job_$JOB_ID $TMPDIR A copy of this runscript is available on Legion in: /shared/ucl/apps/BEAST/run-beast.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_id> and -wd /home/<your_UCL_id>/Scratch/BEAST grid engine directives. You may also need to change the -l thr=4 , -l mem=1G and -l h_rt=0:15:0 directives. Submit the script using a qsub command like: qsub -v infile=`pwd`/gopher.xml run-beast.sh replacing gopher.xml with your BEAST XML file. Output will be written to $TMPDIR and so will need to be copied back to your ~/Scratch directory.","title":"BEAST"},{"location":"Wiki_Export/software/done/CASTEP/","text":"CASTEP \u00a7 CASTEP is a full-featured materials modelling code based on a first-principles quantum mechanical description of electrons and nuclei. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra. CASTEP is developed by members of the CASTEP Developers Group (CDG) who are currently all UK based academics. Access to CASTEP is enabled by a module file and being a member of the appropriate reserved application group. Please email rc-support AT ucl.ac.uk to get your userid added to the CASTEP group. On Legion CASTEP is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. To use CASTEP you need to load the module: module load castep/17.21/intel-2017","title":"CASTEP"},{"location":"Wiki_Export/software/done/CASTEP/#castep","text":"CASTEP is a full-featured materials modelling code based on a first-principles quantum mechanical description of electrons and nuclei. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra. CASTEP is developed by members of the CASTEP Developers Group (CDG) who are currently all UK based academics. Access to CASTEP is enabled by a module file and being a member of the appropriate reserved application group. Please email rc-support AT ucl.ac.uk to get your userid added to the CASTEP group. On Legion CASTEP is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. To use CASTEP you need to load the module: module load castep/17.21/intel-2017","title":"CASTEP"},{"location":"Wiki_Export/software/done/FSL/","text":"FSL \u00a7 The FMRIB Software Library is a set of tools for analysis and visualization of FMRI, MRI and DTI brain imaging data. It is developed by the FMRIB Analysis Group of the Nuffield Department of Clinical Neurosciences at the University of Oxford. Versions 5.0.1 and 4.1.9 are available on our clusters. FSL is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. Output can be displayed using the slices program. Before you can use FSL you need to load the FSL module and call FSL's set up script. The procedure is slightly different for each version. For version 4.1.9: module unload compilers/intel/11.1/072 module load compilers/gnu/4.4.0 module load fsl/4.1.9/gnu.4.4.0 source $FSLDIR/etc/fslconf/fsl.sh For version 5.0.1: module unload compilers/intel/11.1/072 module load compilers/gnu/4.6.3 module load fsl/5.0.1/gnu.4.6.3 source $FSLDIR/etc/fslconf/fsl.sh FSL 5 has been compiled using GCC 4.6.3 so that it can be used in combination with R. FSL programs can now be run in command line mode for example: susan structural.nii.gz 2000 2 3 1 0 structural_susan.nii.gz With FSL version 5 some programs ( e.g. FEAT) can submit jobs directly to the cluster and will do this by default. For other programs you will need a job script. Here is an example job script for version 5 to submit a batch job to the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash -l # Batch script to run FSL 5 example jobs on Legion # Request 1 hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:0:0 # Request 4 gigabyte of RAM. #$ -l mem=4G # Note: some FSL programs are multi-threaded eg FEAT and you will need to # include a -l thr=<number of threads> directive as well. # Set the name of the job. #$ -N FSL_job1 # Select the project that this job will run under. # Find <your_project_id> by running the command \"groups\" #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. This is # a necessary step with the upgraded software stack as compute nodes cannot # write to $HOME. # # Note: this directory MUST exist before your job starts! # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/FSL_output # Setup FSL runtime environment module unload compilers/intel/11.1/072 module load compilers/gnu/4.6.3 module load fsl/5.0.1/gnu.4.6.3 source $FSLDIR/etc/fslconf/fsl.sh # Your work *must* be done in $TMPDIR cd $TMPDIR # Run job - A simple example using the BET command on the supplied example # data. # Copy input files to $TMPDIR cp /shared/ucl/apps/FSL/5.0.1/feeds/data/structural.nii.gz $TMPDIR echo \"\" echo \"Running: bet structural.nii.gz structural_brain.nii\" echo \"\" time bet structural.nii.gz structural_brain.nii # Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME/Scratch/FSL/files_from_job_$JOB_ID $TMPDIR # Make sure you have given enough time for the copy to complete! This is available on Legion in: /shared/ucl/apps/FSL/5.0.1/run-fsl.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_id> and -wd /home/<your_UCL_id>/Scratch/FSL_output SGE directives. You will also need to change the FSL command in the example and may need to change the memory, wallclock time and job name directives as well. The script can be submitted using the simplest form of the qsub command, i.e.: qsub run-fsl.sh Output will be written to $TMPDIR and so will need to be copied back to your ~/Scratch directory - step 10 in the job script. A version of the job script for FSL version 4 is available in: /shared/ucl/apps/FSL/4.1.9/run-fsl.sh","title":"FSL"},{"location":"Wiki_Export/software/done/FSL/#fsl","text":"The FMRIB Software Library is a set of tools for analysis and visualization of FMRI, MRI and DTI brain imaging data. It is developed by the FMRIB Analysis Group of the Nuffield Department of Clinical Neurosciences at the University of Oxford. Versions 5.0.1 and 4.1.9 are available on our clusters. FSL is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. Output can be displayed using the slices program. Before you can use FSL you need to load the FSL module and call FSL's set up script. The procedure is slightly different for each version. For version 4.1.9: module unload compilers/intel/11.1/072 module load compilers/gnu/4.4.0 module load fsl/4.1.9/gnu.4.4.0 source $FSLDIR/etc/fslconf/fsl.sh For version 5.0.1: module unload compilers/intel/11.1/072 module load compilers/gnu/4.6.3 module load fsl/5.0.1/gnu.4.6.3 source $FSLDIR/etc/fslconf/fsl.sh FSL 5 has been compiled using GCC 4.6.3 so that it can be used in combination with R. FSL programs can now be run in command line mode for example: susan structural.nii.gz 2000 2 3 1 0 structural_susan.nii.gz With FSL version 5 some programs ( e.g. FEAT) can submit jobs directly to the cluster and will do this by default. For other programs you will need a job script. Here is an example job script for version 5 to submit a batch job to the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash -l # Batch script to run FSL 5 example jobs on Legion # Request 1 hour of wallclock time (format hours:minutes:seconds). #$ -l h_rt=1:0:0 # Request 4 gigabyte of RAM. #$ -l mem=4G # Note: some FSL programs are multi-threaded eg FEAT and you will need to # include a -l thr=<number of threads> directive as well. # Set the name of the job. #$ -N FSL_job1 # Select the project that this job will run under. # Find <your_project_id> by running the command \"groups\" #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. This is # a necessary step with the upgraded software stack as compute nodes cannot # write to $HOME. # # Note: this directory MUST exist before your job starts! # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/FSL_output # Setup FSL runtime environment module unload compilers/intel/11.1/072 module load compilers/gnu/4.6.3 module load fsl/5.0.1/gnu.4.6.3 source $FSLDIR/etc/fslconf/fsl.sh # Your work *must* be done in $TMPDIR cd $TMPDIR # Run job - A simple example using the BET command on the supplied example # data. # Copy input files to $TMPDIR cp /shared/ucl/apps/FSL/5.0.1/feeds/data/structural.nii.gz $TMPDIR echo \"\" echo \"Running: bet structural.nii.gz structural_brain.nii\" echo \"\" time bet structural.nii.gz structural_brain.nii # Preferably, tar-up (archive) all output files onto the shared scratch area tar zcvf $HOME/Scratch/FSL/files_from_job_$JOB_ID $TMPDIR # Make sure you have given enough time for the copy to complete! This is available on Legion in: /shared/ucl/apps/FSL/5.0.1/run-fsl.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_id> and -wd /home/<your_UCL_id>/Scratch/FSL_output SGE directives. You will also need to change the FSL command in the example and may need to change the memory, wallclock time and job name directives as well. The script can be submitted using the simplest form of the qsub command, i.e.: qsub run-fsl.sh Output will be written to $TMPDIR and so will need to be copied back to your ~/Scratch directory - step 10 in the job script. A version of the job script for FSL version 4 is available in: /shared/ucl/apps/FSL/4.1.9/run-fsl.sh","title":"FSL"},{"location":"Wiki_Export/software/done/FreeSurfer/","text":"FreeSurfer \u00a7 FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data. Version 5.3.0 is available on Legion (an older version 5.1.0 is also available). FreeSurfer s intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. Before you can use FreeSurfer you need to load the FreeSurfer module and set an environment variable to point to your subjects directory: module load freesurfer/5.3.0 export SUBJECTS_DIR=~/Scratch/<some-directory> This directory needs to be in your Scratch space so the compute nodes have write access to it. The latest version of FreeSurfer supports OpenMP. Here is an example runscript to submit a batch job to the cluster using 4 threads: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #!/bin/bash -l # Batch script to run a serial FreeSurfer job on Legion with the upgraded # software stack under SGE. This version works with the modules # environment upgraded in Feb 2012. # FreeSurfer Version 5.3.0 # Nov 2013 # This version of FreeSurfer supports OpenMP # Request 1 hour of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=1:00:0 # Request 15 gigabyte of RAM for the whole job. Change this to suit your # requirements. #$ -l mem=15G # Set the name of the job. You can change this if you wish. #$ -N FreeSurfer_job_1m # Select 4 threads (the most possible on the majority of Legion is 12). #$ -l thr=4 # Select the project that this job will run under. # Find <your_project_id> by running the command \"groups\" #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. This is # a necessary step with the upgraded software stack as compute nodes cannot # write to your $HOME. # # NOTE: this directory must exist. # # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/FreeSurfer_output # Your work *must* be done in $TMPDIR cd $TMPDIR # Load FreeSurfer module and point to your subjects directory. module load freesurfer/5.3.0 export SUBJECTS_DIR=~/Scratch/PADDINGTON # Run FreeSurfer programs - replace with your command(s) #time recon-all -i $SUBJECTS_DIR/30432.n.nii -subjid 30432 -autorecon1 -cw256 time recon-all -subjid 30432 -autorecon1 -cw256 This is available on Legion in: /shared/ucl/apps/FreeSurfer/run-freesurfer.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_id> and -wd /home/<your_UCL_id>/Scratch/FreeSurfer_output SGE directives and set the SUBJECTS_DIR variable to the path to your subjects directory. You will also need to change the FreeSurfer command in the example and may need to change the memory, wallclock time, number of threads and job name directives as well. The script can be submitted using the simplest form of the qsub command ie: qsub run-freesurfer.sh Output files and logs will be written to your subjects directory.","title":"FreeSurfer"},{"location":"Wiki_Export/software/done/FreeSurfer/#freesurfer","text":"FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data. Version 5.3.0 is available on Legion (an older version 5.1.0 is also available). FreeSurfer s intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. Before you can use FreeSurfer you need to load the FreeSurfer module and set an environment variable to point to your subjects directory: module load freesurfer/5.3.0 export SUBJECTS_DIR=~/Scratch/<some-directory> This directory needs to be in your Scratch space so the compute nodes have write access to it. The latest version of FreeSurfer supports OpenMP. Here is an example runscript to submit a batch job to the cluster using 4 threads: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #!/bin/bash -l # Batch script to run a serial FreeSurfer job on Legion with the upgraded # software stack under SGE. This version works with the modules # environment upgraded in Feb 2012. # FreeSurfer Version 5.3.0 # Nov 2013 # This version of FreeSurfer supports OpenMP # Request 1 hour of wallclock time (format hours:minutes:seconds). # Change this to suit your requirements. #$ -l h_rt=1:00:0 # Request 15 gigabyte of RAM for the whole job. Change this to suit your # requirements. #$ -l mem=15G # Set the name of the job. You can change this if you wish. #$ -N FreeSurfer_job_1m # Select 4 threads (the most possible on the majority of Legion is 12). #$ -l thr=4 # Select the project that this job will run under. # Find <your_project_id> by running the command \"groups\" #$ -P <your_project_id> # Set the working directory to somewhere in your scratch space. This is # a necessary step with the upgraded software stack as compute nodes cannot # write to your $HOME. # # NOTE: this directory must exist. # # Replace \"<your_UCL_id>\" with your UCL user ID :) #$ -wd /home/<your_UCL_id>/Scratch/FreeSurfer_output # Your work *must* be done in $TMPDIR cd $TMPDIR # Load FreeSurfer module and point to your subjects directory. module load freesurfer/5.3.0 export SUBJECTS_DIR=~/Scratch/PADDINGTON # Run FreeSurfer programs - replace with your command(s) #time recon-all -i $SUBJECTS_DIR/30432.n.nii -subjid 30432 -autorecon1 -cw256 time recon-all -subjid 30432 -autorecon1 -cw256 This is available on Legion in: /shared/ucl/apps/FreeSurfer/run-freesurfer.sh Please copy if you wish and edit it to suit your jobs. You will need to change the -P <your_project_id> and -wd /home/<your_UCL_id>/Scratch/FreeSurfer_output SGE directives and set the SUBJECTS_DIR variable to the path to your subjects directory. You will also need to change the FreeSurfer command in the example and may need to change the memory, wallclock time, number of threads and job name directives as well. The script can be submitted using the simplest form of the qsub command ie: qsub run-freesurfer.sh Output files and logs will be written to your subjects directory.","title":"FreeSurfer"},{"location":"Wiki_Export/software/done/MKL/","text":"Intel's Math Kernel Library \u00a7 Intel MKL implements CDFT, BLACS and ScaLAPACK and provides Fortran 95 interfaces for BLAS and LAPACK. MKL is now part of the default Intel compiler module. (Type module avail compilers/intel to see the current versions of Intel modules). module load compilers/intel/2015/update2 module show on the Intel compiler module will display that $MKLROOT is set. Easy linking of MKL \u00a7 If you can, try to use -mkl as a compiler flag - if that works, it should get all the correct libraries linked in the right order. Some build systems do not work with this however. Intel MKL link line advisor \u00a7 It can be complicated to get the correct link line for MKL, so Intel has provided a tool which will give you the link line with the libraries in the right order. https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor Pick the version of MKL you are using (for the Intel 2015 compiler it should be Intel(R) Parallel Studio XE 2015 Composer Edition), and these options: OS: Linux Xeon Phi: None (unless you're explicitly using one) Pick your compiler. BLAS and LAPACK are Fortran95 interfaces, to select them pick a Fortran compiler. Architecture: Intel 64 You can choose what type of linking you prefer. Dynamic linking means the libraries are linked at runtime and use the .so library, while static means they are linked at compile time and use the .a library. The Single Dynamic Library for later MKL versions will mean MKL will do clever things to work out which parts of it you are using. Interface layer: ILP64 (64-bit integer) You probably want sequential threading in most cases. Select Intel MPI if required. You'll get something like this: # Link Flags ${MKLROOT}/lib/intel64/libmkl_blas95_ilp64 ${MKLROOT}/lib/intel64/libmkl_lapack95_ilp64 \\ -L${MKLROOT}/lib/intel64 -lmkl_scalapack_ilp64 -lmkl_cdft_core -lmkl_intel_ilp64 \\ -lmkl_core -lmkl_sequential -lmkl_blacs_intelmpi_ilp64 -lpthread -lm # Compiler options -i8 -I${MKLROOT}/include/intel64/ilp64 -I${MKLROOT}/include Corrections \u00a7 It is a good idea to double check the library locations given by the tool are correct: do an ls and make sure the directory exists and contains the libraries. In the past there have been slight path differences between tool and install for some versions.","title":"MKL"},{"location":"Wiki_Export/software/done/MKL/#intels-math-kernel-library","text":"Intel MKL implements CDFT, BLACS and ScaLAPACK and provides Fortran 95 interfaces for BLAS and LAPACK. MKL is now part of the default Intel compiler module. (Type module avail compilers/intel to see the current versions of Intel modules). module load compilers/intel/2015/update2 module show on the Intel compiler module will display that $MKLROOT is set.","title":"Intel's Math Kernel Library"},{"location":"Wiki_Export/software/done/MKL/#easy-linking-of-mkl","text":"If you can, try to use -mkl as a compiler flag - if that works, it should get all the correct libraries linked in the right order. Some build systems do not work with this however.","title":"Easy linking of MKL"},{"location":"Wiki_Export/software/done/MKL/#intel-mkl-link-line-advisor","text":"It can be complicated to get the correct link line for MKL, so Intel has provided a tool which will give you the link line with the libraries in the right order. https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor Pick the version of MKL you are using (for the Intel 2015 compiler it should be Intel(R) Parallel Studio XE 2015 Composer Edition), and these options: OS: Linux Xeon Phi: None (unless you're explicitly using one) Pick your compiler. BLAS and LAPACK are Fortran95 interfaces, to select them pick a Fortran compiler. Architecture: Intel 64 You can choose what type of linking you prefer. Dynamic linking means the libraries are linked at runtime and use the .so library, while static means they are linked at compile time and use the .a library. The Single Dynamic Library for later MKL versions will mean MKL will do clever things to work out which parts of it you are using. Interface layer: ILP64 (64-bit integer) You probably want sequential threading in most cases. Select Intel MPI if required. You'll get something like this: # Link Flags ${MKLROOT}/lib/intel64/libmkl_blas95_ilp64 ${MKLROOT}/lib/intel64/libmkl_lapack95_ilp64 \\ -L${MKLROOT}/lib/intel64 -lmkl_scalapack_ilp64 -lmkl_cdft_core -lmkl_intel_ilp64 \\ -lmkl_core -lmkl_sequential -lmkl_blacs_intelmpi_ilp64 -lpthread -lm # Compiler options -i8 -I${MKLROOT}/include/intel64/ilp64 -I${MKLROOT}/include","title":"Intel MKL link line advisor"},{"location":"Wiki_Export/software/done/MKL/#corrections","text":"It is a good idea to double check the library locations given by the tool are correct: do an ls and make sure the directory exists and contains the libraries. In the past there have been slight path differences between tool and install for some versions.","title":"Corrections"},{"location":"Wiki_Export/software/done/OpenBLAS/","text":"OpenBLAS \u00a7 OpenBLAS provides CBLAS, BLAS and LAPACK. There are versions compiled for the Intel and GNU compilers. With the default modules loaded: module load openblas/0.2.14/intel-2015-update2 module show on the OpenBLAS module will display that $OPENBLASROOT is set, so you can use this in your link line if the library is not picked up automatically. Linking OpenBLAS \u00a7 Our OpenBLAS modules now contain symlinks for libblas and liblapack that both point to libopenblas. This means that the default -lblas -llapack link options, that some programs try to use when building, will work. This is how you would normally link OpenBLAS: -L${OPENBLASROOT}/lib -lopenblas If code you are compiling requires separate entries for BLAS and LAPACK, set them both to -lopenblas . OpenBLAS and OpenMP warning \u00a7 If you are running a threaded program and get this warning: OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option. Then tell OpenBLAS to use only one thread by adding this to your jobscript (this overrides $OMP_NUM_THREADS for OpenBLAS): export OPENBLAS_NUM_THREADS = 1 If it is your code, you can also set it with the function void openblas_set_num_threads ( int num_threads );","title":"OpenBLAS"},{"location":"Wiki_Export/software/done/OpenBLAS/#openblas","text":"OpenBLAS provides CBLAS, BLAS and LAPACK. There are versions compiled for the Intel and GNU compilers. With the default modules loaded: module load openblas/0.2.14/intel-2015-update2 module show on the OpenBLAS module will display that $OPENBLASROOT is set, so you can use this in your link line if the library is not picked up automatically.","title":"OpenBLAS"},{"location":"Wiki_Export/software/done/OpenBLAS/#linking-openblas","text":"Our OpenBLAS modules now contain symlinks for libblas and liblapack that both point to libopenblas. This means that the default -lblas -llapack link options, that some programs try to use when building, will work. This is how you would normally link OpenBLAS: -L${OPENBLASROOT}/lib -lopenblas If code you are compiling requires separate entries for BLAS and LAPACK, set them both to -lopenblas .","title":"Linking OpenBLAS"},{"location":"Wiki_Export/software/done/OpenBLAS/#openblas-and-openmp-warning","text":"If you are running a threaded program and get this warning: OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option. Then tell OpenBLAS to use only one thread by adding this to your jobscript (this overrides $OMP_NUM_THREADS for OpenBLAS): export OPENBLAS_NUM_THREADS = 1 If it is your code, you can also set it with the function void openblas_set_num_threads ( int num_threads );","title":"OpenBLAS and OpenMP warning"},{"location":"Wiki_Export/software/done/Quantum_Espresso/","text":"Quantum Espresso \u00a7 If you're already acquainted with Quantum Espresso, running it on Legion should be relatively straightforward. The script below demonstrates how to run pw.x , but should be adaptable to any of the other binaries. We have attempted to compile all the relevant components of Quantum Espresso, but if there's a particular component missing in our build that you need, let us know and we'll try to add it. (There are a lot of bits.) This script assumes that you are somewhere in the Scratch area already -- it doesn't change directory, so if you attempt to run it from somewhere in the normal home directories, the job will go into the Eqw state with an error saying that the space isn't writable. Simply submit the script from the same directory as your Quantum Espresso input files, and change the input and output file names if necessary. You'll also need to change the project name to your project (and take out the \\<> brackets). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash -l # 1. Force bash as the executing shell. #$ -S /bin/bash # 2. Request thirty minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:30:0 # 3. Request 1 gigabyte of RAM per core. #$ -l mem=1G # 4. Set the name of the job. #$ -N QE # 5. Select the QLogic parallel environment (qlc) and 16 cores. #$ -pe qlc 16 # 6. Select the project that this job will run under. #$ -P <your project> # 7. Set the working directory to the current working directory when the job is submitted. #$ -cwd # The autoload module unloads and loads all the required modules. # Careful when using this with any other modules. # Take a look at what it's doing with 'module show quantumespresso/5.0.2/autoload' if you're not sure. module load quantumespresso/5.0.2/autoload # Set the path here to where ever you keep your pseudopotentials. export ESPRESSO_PSEUDO=$HOME/qe-psp # Note - the -in argument here for input files avoids any problems with # redirection to MPI applications that can arise. gerun pw.x -in input.in >output.out","title":"Quantum Espresso"},{"location":"Wiki_Export/software/done/Quantum_Espresso/#quantum-espresso","text":"If you're already acquainted with Quantum Espresso, running it on Legion should be relatively straightforward. The script below demonstrates how to run pw.x , but should be adaptable to any of the other binaries. We have attempted to compile all the relevant components of Quantum Espresso, but if there's a particular component missing in our build that you need, let us know and we'll try to add it. (There are a lot of bits.) This script assumes that you are somewhere in the Scratch area already -- it doesn't change directory, so if you attempt to run it from somewhere in the normal home directories, the job will go into the Eqw state with an error saying that the space isn't writable. Simply submit the script from the same directory as your Quantum Espresso input files, and change the input and output file names if necessary. You'll also need to change the project name to your project (and take out the \\<> brackets). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash -l # 1. Force bash as the executing shell. #$ -S /bin/bash # 2. Request thirty minutes of wallclock time (format hours:minutes:seconds). #$ -l h_rt=0:30:0 # 3. Request 1 gigabyte of RAM per core. #$ -l mem=1G # 4. Set the name of the job. #$ -N QE # 5. Select the QLogic parallel environment (qlc) and 16 cores. #$ -pe qlc 16 # 6. Select the project that this job will run under. #$ -P <your project> # 7. Set the working directory to the current working directory when the job is submitted. #$ -cwd # The autoload module unloads and loads all the required modules. # Careful when using this with any other modules. # Take a look at what it's doing with 'module show quantumespresso/5.0.2/autoload' if you're not sure. module load quantumespresso/5.0.2/autoload # Set the path here to where ever you keep your pseudopotentials. export ESPRESSO_PSEUDO=$HOME/qe-psp # Note - the -in argument here for input files avoids any problems with # redirection to MPI applications that can arise. gerun pw.x -in input.in >output.out","title":"Quantum Espresso"},{"location":"Wiki_Export/software/done/R_packages_available_on_Legion/","text":"R and Bioconductor Packages installed on RC Systems \u00a7 Note Please consider replacing this with instructions on how to retrieve this list, instead of the list itself. This is a list of the R add-on packages available with R 3.4.0 and Bioconductor 3.5 on Legion, Grace, Thomas and Aristotle. Package Description abc Tools for Approximate Bayesian Computation (ABC) abc.data Data Only: Tools for Approximate Bayesian Computation (ABC) abind Combine Multidimensional Arrays acepack ACE and AVAS for Selecting Multiple Regression Transformations adapt adapt -- multidimensional numerical integration ade4 Analysis of Ecological Data : Exploratory and Euclidean Methods in Environmental Sciences adegenet Exploratory Analysis of Genetic and Genomic Data ADGofTest Anderson-Darling GoF test affxparser Affymetrix File Parsing SDK affy Methods for Affymetrix Oligonucleotide Arrays affydata Affymetrix Data for Demonstration Purpose affyio Tools for parsing Affymetrix data files affylmGUI GUI for limma package with Affymetrix microarrays affyPLM Methods for fitting probe-level models affyQCReport QC Report Generation for affyBatch objects akima Interpolation of Irregularly and Regularly Spaced Data annaffy Annotation tools for Affymetrix biological metadata annmap Genome annotation and visualisation package pertaining to Affymetrix arrays and NGS analysis. annotate Annotation for microarrays AnnotationDbi Annotation Database Interface AnnotationForge Code for Building Annotation Database Packages AnnotationHub Client to access AnnotationHub resources ape Analyses of Phylogenetics and Evolution aroma.affymetrix Analysis of Large Affymetrix Microarray Data Sets aroma.apd A Probe-Level Data File Format Used by 'aroma.affymetrix' [deprecated] aroma.core Core Methods and Classes Used by 'aroma.*' Packages Part of the Aroma Framework assertthat Easy pre and post assertions. backports Reimplementations of Functions Introduced Since R-3.0.0 base The R Base Package base64 Base64 Encoder and Decoder base64enc Tools for base64 encoding BaSTA Age-Specific Survival Analysis from Incomplete Capture-Recapture/Recovery Data BatchJobs Batch Computing with R bayesplot Plotting for Bayesian Models BBmisc Miscellaneous Helper Functions for B. Bischl beadarray Quality assessment and low-level analysis for Illumina BeadArray data beadarrayExampleData Example data for the beadarray package BeadDataPackR Compression of Illumina BeadArray data beanplot Visualization via Beanplots (like Boxplot/Stripchart/Violin Plot) BH Boost C++ Header Files BiasedUrn Biased Urn Model Distributions Biobase Biobase: Base functions for Bioconductor BiocGenerics S4 generic functions for Bioconductor BiocInstaller Install/Update Bioconductor, CRAN, and github Packages BiocParallel Bioconductor facilities for parallel evaluation biomaRt Interface to BioMart databases (e.g. Ensembl, COSMIC, Wormbase and Gramene) Biostrings String objects representing biological sequences, and matching algorithms biovizBase Basic graphic utilities for visualization of genomic data. bitops Bitwise Operations blockmodeling An R package for Generalized and classical blockmodeling of valued networks boot Bootstrap Functions (Originally by Angelo Canty for S) brew Templating Framework for Report Generation BSgenome Infrastructure for Biostrings-based genome data packages and support for efficient SNP representation BSgenome.Hsapiens.UCSC.hg19 Full genome sequences for Homo sapiens (UCSC version hg19) bsseq Analyze, manage and store bisulfite sequencing data bumphunter Bump Hunter car Companion to Applied Regression caret Classification and Regression Training Category Category Analysis caTools Tools: moving window statistics, GIF, Base64, ROC AUC, etc. ChAMP Chip Analysis Methylation Pipeline for Illumina HumanMethylation450 and EPIC ChAMPdata Data Packages for ChAMP package checkmate Fast and Versatile Argument Checks class Functions for Classification cluster \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. cmprsk Subdistribution Analysis of Competing Risks coda Output Analysis and Diagnostics for MCMC codetools Code Analysis Tools for R colorRamps Builds color tables colorspace Color Space Manipulation colourpicker A Colour Picker Widget for Shiny Apps, RStudio, R-markdown, and 'htmlwidgets' combinat combinatorics utilities compiler The R Compiler Package copula Multivariate Dependence with Copulas copynumber Segmentation of single- and multi-track copy number data by penalized least squares regression. corpcor Efficient Estimation of Covariance and (Partial) Correlation corrplot Visualization of a Correlation Matrix crayon Colored Terminal Output curl A Modern and Flexible Web Client for R data.table Extension of Data.frame datasets The R Datasets Package DBI R Database Interface deldir Delaunay Triangulation and Dirichlet (Voronoi) Tessellation dendextend Extending R's Dendrogram Functionality DEoptimR Differential Evolution Optimization in Pure R DESeq Differential gene expression analysis based on the negative binomial distribution DESeq2 Differential gene expression analysis based on the negative binomial distribution Design Design Package DEXSeq Inference of differential exon usage in RNA-Seq dichromat Color Schemes for Dichromats digest Create Compact Hash Digests of R Objects diptest Hartigan's Dip Test Statistic for Unimodality - Corrected DMRcate Methylation array and sequencing spatial analysis methods DMRcatedata Data Package for DMRcate package DNAcopy DNA copy number data analysis doMC Foreach Parallel Adaptor for 'parallel' doParallel Foreach Parallel Adaptor for the 'parallel' Package doRNG Generic Reproducible Parallel Backend for foreach Loops dplyr A Grammar of Data Manipulation DSS Dispersion shrinakge for sequencing data. DT A Wrapper of the JavaScript Library 'DataTables' dygraphs Interface to 'Dygraphs' Interactive Time Series Charting Library dynamicTreeCut Methods for Detection of Clusters in Hierarchical Clustering Dendrograms DynDoc Dynamic document tools e1071 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien easyRNASeq Count summarization and normalization for RNA-Seq data EBSeq An R package for gene and isoform differential expression analysis of RNA-seq data edgeR Empirical Analysis of Digital Gene Expression Data in R effects Effect Displays for Linear, Generalized Linear, and Other Models ellipse Functions for drawing ellipses and ellipse-like confidence regions ensembldb Utilities to create and use an Ensembl based annotation database Epi A Package for Statistical Analysis in Epidemiology etm Empirical Transition Matrix evaluate Parsing and Evaluation Tools that Provide More Details than the Default expm Matrix Exponential FactoMineR Multivariate Exploratory Data Analysis and Data Mining fail File Abstraction Interface Layer (FAIL) fastcluster Fast Hierarchical Clustering Routines for R and Python fastICA FastICA Algorithms to perform ICA and Projection Pursuit fastmatch Fast match() function FDb.InfiniumMethylation.hg19 Annotation package for Illumina Infinium DNA methylation probes fdrtool Estimation of (Local) False Discovery Rates and Higher Criticism FEM Identification of Functional Epigenetic Modules fields Tools for Spatial Data flashClust Implementation of optimal hierarchical clustering flexmix Flexible Mixture Modeling foreach Provides Foreach Looping Construct for R foreign Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka, dBase, ... Formula Extended Model Formulas fpc Flexible Procedures for Clustering futile.logger A Logging Utility for R futile.options Futile options management future A Future API for R gam Generalized Additive Models gamlss Generalised Additive Models for Location Scale and Shape gamlss.data GAMLSS Data gamlss.dist Distributions to be Used for GAMLSS Modelling gamlss.mx Fitting Mixture Distributions with GAMLSS gamlss.nl Fitting non linear parametric GAMLSS models gcrma Background Adjustment Using Sequence Information gdata Various R Programming Tools for Data Manipulation GenABEL genome-wide SNP association analysis GenABEL.data Package contains data which is used by GenABEL example and test functions genefilter genefilter: methods for filtering genes from high-throughput experiments geneLenDataBase Lengths of mRNA transcripts for a number of genomes GeneNet Modeling and Inferring Gene Networks geneplotter Graphics related functions for Bioconductor genetics Population Genetics GenomeGraphs Plotting genomic information from Ensembl GenomeInfoDb Utilities for manipulating chromosome and other 'seqname' identifiers genomeIntervals Operations on genomic intervals GenomicAlignments Representation and manipulation of short genomic alignments GenomicFeatures Tools for making and manipulating transcript centric annotations GenomicRanges Representation and manipulation of genomic intervals and variables defined along a genome GEOquery Get data from NCBI Gene Expression Omnibus (GEO) GGally Extension to 'ggplot2' ggplot2 Create Elegant Data Visualisations Using the Grammar of Graphics ggvis Interactive Grammar of Graphics glmnet Lasso and Elastic-Net Regularized Generalized Linear Models globals Identify Global Objects in R Expressions gmodels Various R Programming Tools for Model Fitting GO.db A set of annotation maps describing the entire Gene Ontology goseq Gene Ontology analyser for RNA-seq and other length biased data GOstats Tools for manipulating GO and microarrays. gplots Various R Programming Tools for Plotting Data graph graph: A package to handle graph data structures graphics The R Graphics Package grDevices The R Graphics Devices and Support for Colours and Fonts grid The Grid Graphics Package gridBase Integration of base and grid graphics gridExtra Miscellaneous Functions for \"Grid\" Graphics GSEABase Gene set enrichment data structures and methods gsl wrapper for the Gnu Scientific Library gsmoothr Smoothing tools gtable Arrange 'Grobs' in Tables gtools Various R Programming Tools Gviz Plotting data and annotation information along genomic coordinates HAC Estimation, Simulation and Visualization of Hierarchical Archimedean Copulae (HAC) haplo.stats Statistical Analysis of Haplotypes with Traits and Covariates when Linkage Phase is Ambiguous hexbin Hexagonal Binning Routines hgu95av2.db Affymetrix Human Genome U95 Set annotation data (chip hgu95av2) HI Simulation from distributions supported by nested hyperplanes highr Syntax Highlighting for R Source Code Hmisc Harrell Miscellaneous HotDeckImputation Hot Deck Imputation Methods for Missing Data htmlTable Advanced Tables for Markdown/HTML htmltools Tools for HTML htmlwidgets HTML Widgets for R httpuv HTTP and WebSocket Server Library httr Tools for Working with URLs and HTTP hugene10stprobeset.db Affymetrix hugene10 annotation data (chip hugene10stprobeset) hugene10sttranscriptcluster.db Affymetrix hugene10 annotation data (chip hugene10sttranscriptcluster) hwriter HTML Writer - Outputs R objects in HTML format igraph Network Analysis and Visualization Illumina450ProbeVariants.db Annotation Package combining variant data from 1000 Genomes Project for Illumina HumanMethylation450 Bead Chip probes IlluminaHumanMethylation450kanno.ilmn12.hg19 Annotation for Illumina's 450k methylation arrays IlluminaHumanMethylation450kmanifest Annotation for Illumina's 450k methylation arrays IlluminaHumanMethylationEPICanno.ilm10b2.hg19 Annotation for Illumina's EPIC methylation arrays IlluminaHumanMethylationEPICmanifest Manifest for Illumina's EPIC methylation arrays illuminaHumanv4.db Illumina HumanHT12v4 annotation data (chip illuminaHumanv4) illuminaio Parsing Illumina Microarray Output Files impute impute: Imputation for microarray data INLA Functions which Allow to Perform Full Bayesian Analysis of Latent Gaussian Models using Integrated Nested Laplace Approximations inline Functions to Inline C, C++, Fortran Function Calls from R interactiveDisplayBase Base package for enabling powerful shiny web displays of Bioconductor objects intervals Tools for Working with Points and Intervals IRanges Infrastructure for manipulating intervals on sequences irlba Fast Truncated SVD, PCA and Symmetric Eigendecomposition for Large Dense and Sparse Matrices isva Independent Surrogate Variable Analysis iterators Provides Iterator Construct for R jsonlite A Robust, High Performance JSON Parser and Generator for R KEGG.db A set of annotation maps for KEGG kernlab Kernel-Based Machine Learning Lab KernSmooth Functions for Kernel Smoothing Supporting Wand & Jones (1995) knitr A General-Purpose Package for Dynamic Report Generation in R labeling Axis Labeling lambda.r Modeling Data with Functional Programming lattice Trellis Graphics for R latticeExtra Extra Graphical Utilities Based on Lattice lazyeval Lazy (Non-Standard) Evaluation leaps regression subset selection LearnBayes Functions for Learning Bayesian Inference limma Linear Models for Microarray Data listenv Environments Behaving (Almost) as Lists lme4 Linear Mixed-Effects Models using 'Eigen' and S4 locfit Local Regression, Likelihood and Density Estimation. longitudinal Analysis of Multiple Time Course Data loo Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models lpSolve Interface to 'Lp_solve' v. 5.5 to Solve Linear/Integer Programs LSD Lots of Superior Depictions lumi BeadArray Specific Methods for Illumina Methylation and Expression Microarrays made4 Multivariate analysis of microarray data using ADE4 magrittr A Forward-Pipe Operator for R maps Draw Geographical Maps maptree Mapping, pruning, and graphing tree models markdown 'Markdown' Rendering for R marray Exploratory analysis for two-color spotted microarray data MASS Support Functions and Datasets for Venables and Ripley's MASS Matrix Sparse and Dense Matrix Classes and Methods MatrixModels Modelling with Sparse And Dense Matrices matrixStats Functions that Apply to Rows and Columns of Matrices (and to Vectors) mclust Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation mcmc Markov Chain Monte Carlo memoise Memoisation of Functions metafor Meta-Analysis Package for R methods Formal Methods and Classes methylumi Handle Illumina methylation data mgcv Mixed GAM Computation Vehicle with GCV/AIC/REML Smoothness Estimation mime Map Filenames to MIME Types minfi Analyze Illumina Infinium DNA methylation arrays miniUI Shiny UI Widgets for Small Screens minqa Derivative-free optimization algorithms by quadratic approximation missMethyl Analysing Illumina HumanMethylation BeadChip Data mitools Tools for multiple imputation of missing data mlr Machine Learning in R mnormt The Multivariate Normal and t Distributions ModelMetrics Rapid Calculation of Model Metrics modeltools Tools and Classes for Statistical Models msm Multi-State Markov and Hidden Markov Models in Continuous Time mstate Data Preparation, Estimation and Prediction in Multi-State Models multcomp Simultaneous Inference in General Parametric Models multtest Resampling-based multiple hypothesis testing munsell Utilities for Using Munsell Colours mvtnorm Multivariate Normal and t Distributions NetworkAnalysis Statistical inference on populations of weighted or unweighted networks. nleqslv Solve Systems of Nonlinear Equations nlme Linear and Nonlinear Mixed Effects Models nloptr R interface to NLopt NMF Algorithms and Framework for Nonnegative Matrix Factorization (NMF) nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models nnls The Lawson-Hanson algorithm for non-negative least squares (NNLS) nor1mix Normal (1-d) Mixture Models (S3 Classes and Methods) numDeriv Accurate Numerical Derivatives nutshell Data for \"R in a Nutshell\" nutshell.audioscrobbler Audioscrobbler data for \"R in a Nutshell\" nutshell.bbdb Baseball Database for \"R in a Nutshell\" OPE Outer-product emulator openssl Toolkit for Encryption, Signatures and Certificates Based on OpenSSL org.Hs.eg.db Genome wide annotation for Human packrat A Dependency Management System for Projects and their R Package Dependencies parallel Support for Parallel computation in R parallelMap Unified Interface to Parallelization Back-Ends ParamHelpers Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning parcor Regularized estimation of partial correlation matrices pbkrtest Parametric Bootstrap and Kenward Roger Based Methods for Mixed Model Comparison pcaPP Robust PCA by Projection Pursuit pegas Population and Evolutionary Genetics Analysis System permute Functions for Generating Restricted Permutations of Data phangorn Phylogenetic Analysis in R pixmap Bitmap Images (``Pixel Maps'') pkgmaker Package development utilities PKI Public Key Infrastucture for R Based on the X.509 Standard plogr The 'plog' C++ Logging Library plotly Create Interactive Web Graphics via 'plotly.js' pls Partial Least Squares and Principal Component Regression plyr Tools for Splitting, Applying and Combining Data polspline Polynomial Spline Routines poweRlaw Analysis of Heavy Tailed Distributions ppls Penalized Partial Least Squares prabclus Functions for Clustering of Presence-Absence, Abundance and Multilocus Genetic Data pracma Practical Numerical Math Functions praise Praise Users preprocessCore A collection of pre-processing functions prettyunits Pretty, Human Readable Formatting of Quantities progress Terminal Progress Bars proto Prototype Object-Based Programming PSCBS Analysis of Parent-Specific DNA Copy Numbers pspline Penalized Smoothing Splines psych Procedures for Psychological, Psychometric, and Personality Research purrr Functional Programming Tools quadprog Functions to solve Quadratic Programming Problems. quantreg Quantile Regression qvalue Q-value estimation for false discovery rate control R.cache Fast and Light-Weight Caching (Memoization) of Objects and Results to Speed Up Computations R.devices Unified Handling of Graphics Devices R.filesets Easy Handling of and Access to Files Organized in Structured Directories R.huge Methods for Accessing Huge Amounts of Data [deprecated] R.methodsS3 S3 Methods Simplified R.oo R Object-Oriented Programming with or without References R.rsp Dynamic Generation of Scientific Reports R.utils Various Programming Utilities R2HTML HTML Exportation for R Objects R2jags Using R to Run 'JAGS' R2WinBUGS Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS' R6 Classes with Reference Semantics randomForest Breiman and Cutler's Random Forests for Classification and Regression RBGL An interface to the BOOST graph library RColorBrewer ColorBrewer Palettes Rcpp Seamless R and C++ Integration RcppArmadillo 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra Library RcppEigen 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra Library RcppGSL 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices RCurl General Network (HTTP/FTP/...) Client Interface for R RefFreeEWAS EWAS using Reference-Free DNA Methylation Mixture Deconvolution registry Infrastructure for R Package Registries relaimpo Relative importance of regressors in linear models Repitools Epigenomic tools reshape Flexibly Reshape Data reshape2 Flexibly Reshape Data: A Reboot of the Reshape Package rgdal Bindings for the Geospatial Data Abstraction Library rgeos Interface to Geometry Engine - Open Source (GEOS) rgl 3D Visualization Using OpenGL Rglpk R/GNU Linear Programming Kit Interface ridge Ridge Regression with automatic selection of the penalty parameter Ringo R Investigation of ChIP-chip Oligoarrays rjags Bayesian Graphical Models using MCMC RJSONIO Serialize R objects to JSON, JavaScript Object Notation rlecuyer R Interface to RNG with Multiple Streams Rmpi Interface (Wrapper) to MPI (Message-Passing Interface) rms Regression Modeling Strategies RMySQL Database Interface and 'MySQL' Driver for R RNetCDF Interface to NetCDF Datasets rngtools Utility functions for working with Random Number Generators robustbase Basic Robust Statistics ROC utilities for ROC, with uarray focus rpart Recursive Partitioning and Regression Trees RPMM Recursively Partitioned Mixture Model Rsamtools Binary alignment (BAM), FASTA, variant call (BCF), and tabix file import rsconnect Deployment Interface for R Markdown Documents and Shiny Applications Rsolnp General Non-Linear Optimization RSQLite 'SQLite' Interface for R rstan R Interface to Stan rstanarm Bayesian Applied Regression Modeling via Stan rstantools Tools for Developing R Packages Interfacing with 'Stan' rstudioapi Safely Access the RStudio API Rsubread Subread sequence alignment for R rtracklayer R interface to genome browsers and their annotation tracks ruv Detect and Remove Unwanted Variation using Negative Controls S4Vectors S4 implementation of vectors and lists saemix Stochastic Approximation Expectation Maximization (SAEM) algorithm sampling Survey Sampling sandwich Robust Covariance Matrix Estimators scales Scale Functions for Visualization scatterplot3d 3D Scatter Plot schoolmath Functions and datasets for math used in school segmented Regression Models with Breakpoints/Changepoints Estimation sendmailR send email using R seqinr Biological Sequences Retrieval and Analysis seqLogo Sequence logos for DNA sequence alignments shiny Web Application Framework for R shinyjs Easily Improve the User Experience of Your Shiny Apps in Seconds shinystan Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models shinythemes Themes for Shiny ShortRead FASTQ input and manipulation siggenes Multiple testing using SAM and Efron's empirical Bayes approaches simpleaffy Very simple high level analysis of Affymetrix data slam Sparse Lightweight Arrays and Matrices sn The Skew-Normal and Skew-t Distributions snow Simple Network of Workstations snowfall Easier cluster computing (based on snow). softImpute Matrix Completion via Iterative Soft-Thresholded SVD sourcetools Tools for Reading, Tokenizing and Parsing R Code sp Classes and Methods for Spatial Data spam SPArse Matrix SparseM Sparse Linear Algebra spatial Functions for Kriging and Point Pattern Analysis spdep Spatial Dependence: Weighting Schemes, Statistics and Models splines Regression Spline Functions and Classes stabledist Stable Distribution Functions StanHeaders C++ Header Files for Stan statmod Statistical Modeling stats The R Stats Package stats4 Statistical Functions using S4 Classes stringdist Approximate String Matching and String Distance Functions stringi Character String Processing Facilities stringr Simple, Consistent Wrappers for Common String Operations SummarizedExperiment SummarizedExperiment container survey Analysis of Complex Survey Samples survival Survival Analysis sva Surrogate Variable Analysis tcltk Tcl/Tk Interface testthat Unit Testing for R tgp Bayesian Treed Gaussian Process Models TH.data TH's Data Archive threejs Interactive 3D Scatter Plots, Networks and Globes tibble Simple Data Frames tidyr Easily Tidy Data with spread() and gather() Functions tkrplot TK Rplot tools Tools for Package Development trimcluster Cluster analysis with trimming truncnorm Truncated normal distribution tseries Time Series Analysis and Computational Finance TxDb.Hsapiens.UCSC.hg19.knownGene Annotation package for TxDb object(s) utils The R Utils Package VariantAnnotation Annotation of Genetic Variants vegan Community Ecology Package VGAM Vector Generalized Linear and Additive Models viridis Default Color Maps from 'matplotlib' viridisLite Default Color Maps from 'matplotlib' (Lite Version) vsn Variance stabilization and calibration for microarray data wateRmelon Illumina 450 methylation array normalization and metrics WGCNA Weighted Correlation Network Analysis whisker for R, logicless templating widgetTools Creates an interactive tcltk widget XML Tools for Parsing and Generating XML Within R and S-Plus xps Processing and Analysis of Affymetrix Oligonucleotide Arrays including Exon Arrays, Whole Genome Arrays and Plate Arrays xtable Export Tables to LaTeX or HTML xts eXtensible Time Series XVector Representation and manpulation of external sequences yaml Methods to Convert R Data to YAML and Back zlibbioc An R packaged zlib-1.2.5 zoo S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)","title":"R packages available on Legion"},{"location":"Wiki_Export/software/done/R_packages_available_on_Legion/#r-and-bioconductor-packages-installed-on-rc-systems","text":"Note Please consider replacing this with instructions on how to retrieve this list, instead of the list itself. This is a list of the R add-on packages available with R 3.4.0 and Bioconductor 3.5 on Legion, Grace, Thomas and Aristotle. Package Description abc Tools for Approximate Bayesian Computation (ABC) abc.data Data Only: Tools for Approximate Bayesian Computation (ABC) abind Combine Multidimensional Arrays acepack ACE and AVAS for Selecting Multiple Regression Transformations adapt adapt -- multidimensional numerical integration ade4 Analysis of Ecological Data : Exploratory and Euclidean Methods in Environmental Sciences adegenet Exploratory Analysis of Genetic and Genomic Data ADGofTest Anderson-Darling GoF test affxparser Affymetrix File Parsing SDK affy Methods for Affymetrix Oligonucleotide Arrays affydata Affymetrix Data for Demonstration Purpose affyio Tools for parsing Affymetrix data files affylmGUI GUI for limma package with Affymetrix microarrays affyPLM Methods for fitting probe-level models affyQCReport QC Report Generation for affyBatch objects akima Interpolation of Irregularly and Regularly Spaced Data annaffy Annotation tools for Affymetrix biological metadata annmap Genome annotation and visualisation package pertaining to Affymetrix arrays and NGS analysis. annotate Annotation for microarrays AnnotationDbi Annotation Database Interface AnnotationForge Code for Building Annotation Database Packages AnnotationHub Client to access AnnotationHub resources ape Analyses of Phylogenetics and Evolution aroma.affymetrix Analysis of Large Affymetrix Microarray Data Sets aroma.apd A Probe-Level Data File Format Used by 'aroma.affymetrix' [deprecated] aroma.core Core Methods and Classes Used by 'aroma.*' Packages Part of the Aroma Framework assertthat Easy pre and post assertions. backports Reimplementations of Functions Introduced Since R-3.0.0 base The R Base Package base64 Base64 Encoder and Decoder base64enc Tools for base64 encoding BaSTA Age-Specific Survival Analysis from Incomplete Capture-Recapture/Recovery Data BatchJobs Batch Computing with R bayesplot Plotting for Bayesian Models BBmisc Miscellaneous Helper Functions for B. Bischl beadarray Quality assessment and low-level analysis for Illumina BeadArray data beadarrayExampleData Example data for the beadarray package BeadDataPackR Compression of Illumina BeadArray data beanplot Visualization via Beanplots (like Boxplot/Stripchart/Violin Plot) BH Boost C++ Header Files BiasedUrn Biased Urn Model Distributions Biobase Biobase: Base functions for Bioconductor BiocGenerics S4 generic functions for Bioconductor BiocInstaller Install/Update Bioconductor, CRAN, and github Packages BiocParallel Bioconductor facilities for parallel evaluation biomaRt Interface to BioMart databases (e.g. Ensembl, COSMIC, Wormbase and Gramene) Biostrings String objects representing biological sequences, and matching algorithms biovizBase Basic graphic utilities for visualization of genomic data. bitops Bitwise Operations blockmodeling An R package for Generalized and classical blockmodeling of valued networks boot Bootstrap Functions (Originally by Angelo Canty for S) brew Templating Framework for Report Generation BSgenome Infrastructure for Biostrings-based genome data packages and support for efficient SNP representation BSgenome.Hsapiens.UCSC.hg19 Full genome sequences for Homo sapiens (UCSC version hg19) bsseq Analyze, manage and store bisulfite sequencing data bumphunter Bump Hunter car Companion to Applied Regression caret Classification and Regression Training Category Category Analysis caTools Tools: moving window statistics, GIF, Base64, ROC AUC, etc. ChAMP Chip Analysis Methylation Pipeline for Illumina HumanMethylation450 and EPIC ChAMPdata Data Packages for ChAMP package checkmate Fast and Versatile Argument Checks class Functions for Classification cluster \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. cmprsk Subdistribution Analysis of Competing Risks coda Output Analysis and Diagnostics for MCMC codetools Code Analysis Tools for R colorRamps Builds color tables colorspace Color Space Manipulation colourpicker A Colour Picker Widget for Shiny Apps, RStudio, R-markdown, and 'htmlwidgets' combinat combinatorics utilities compiler The R Compiler Package copula Multivariate Dependence with Copulas copynumber Segmentation of single- and multi-track copy number data by penalized least squares regression. corpcor Efficient Estimation of Covariance and (Partial) Correlation corrplot Visualization of a Correlation Matrix crayon Colored Terminal Output curl A Modern and Flexible Web Client for R data.table Extension of Data.frame datasets The R Datasets Package DBI R Database Interface deldir Delaunay Triangulation and Dirichlet (Voronoi) Tessellation dendextend Extending R's Dendrogram Functionality DEoptimR Differential Evolution Optimization in Pure R DESeq Differential gene expression analysis based on the negative binomial distribution DESeq2 Differential gene expression analysis based on the negative binomial distribution Design Design Package DEXSeq Inference of differential exon usage in RNA-Seq dichromat Color Schemes for Dichromats digest Create Compact Hash Digests of R Objects diptest Hartigan's Dip Test Statistic for Unimodality - Corrected DMRcate Methylation array and sequencing spatial analysis methods DMRcatedata Data Package for DMRcate package DNAcopy DNA copy number data analysis doMC Foreach Parallel Adaptor for 'parallel' doParallel Foreach Parallel Adaptor for the 'parallel' Package doRNG Generic Reproducible Parallel Backend for foreach Loops dplyr A Grammar of Data Manipulation DSS Dispersion shrinakge for sequencing data. DT A Wrapper of the JavaScript Library 'DataTables' dygraphs Interface to 'Dygraphs' Interactive Time Series Charting Library dynamicTreeCut Methods for Detection of Clusters in Hierarchical Clustering Dendrograms DynDoc Dynamic document tools e1071 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien easyRNASeq Count summarization and normalization for RNA-Seq data EBSeq An R package for gene and isoform differential expression analysis of RNA-seq data edgeR Empirical Analysis of Digital Gene Expression Data in R effects Effect Displays for Linear, Generalized Linear, and Other Models ellipse Functions for drawing ellipses and ellipse-like confidence regions ensembldb Utilities to create and use an Ensembl based annotation database Epi A Package for Statistical Analysis in Epidemiology etm Empirical Transition Matrix evaluate Parsing and Evaluation Tools that Provide More Details than the Default expm Matrix Exponential FactoMineR Multivariate Exploratory Data Analysis and Data Mining fail File Abstraction Interface Layer (FAIL) fastcluster Fast Hierarchical Clustering Routines for R and Python fastICA FastICA Algorithms to perform ICA and Projection Pursuit fastmatch Fast match() function FDb.InfiniumMethylation.hg19 Annotation package for Illumina Infinium DNA methylation probes fdrtool Estimation of (Local) False Discovery Rates and Higher Criticism FEM Identification of Functional Epigenetic Modules fields Tools for Spatial Data flashClust Implementation of optimal hierarchical clustering flexmix Flexible Mixture Modeling foreach Provides Foreach Looping Construct for R foreign Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka, dBase, ... Formula Extended Model Formulas fpc Flexible Procedures for Clustering futile.logger A Logging Utility for R futile.options Futile options management future A Future API for R gam Generalized Additive Models gamlss Generalised Additive Models for Location Scale and Shape gamlss.data GAMLSS Data gamlss.dist Distributions to be Used for GAMLSS Modelling gamlss.mx Fitting Mixture Distributions with GAMLSS gamlss.nl Fitting non linear parametric GAMLSS models gcrma Background Adjustment Using Sequence Information gdata Various R Programming Tools for Data Manipulation GenABEL genome-wide SNP association analysis GenABEL.data Package contains data which is used by GenABEL example and test functions genefilter genefilter: methods for filtering genes from high-throughput experiments geneLenDataBase Lengths of mRNA transcripts for a number of genomes GeneNet Modeling and Inferring Gene Networks geneplotter Graphics related functions for Bioconductor genetics Population Genetics GenomeGraphs Plotting genomic information from Ensembl GenomeInfoDb Utilities for manipulating chromosome and other 'seqname' identifiers genomeIntervals Operations on genomic intervals GenomicAlignments Representation and manipulation of short genomic alignments GenomicFeatures Tools for making and manipulating transcript centric annotations GenomicRanges Representation and manipulation of genomic intervals and variables defined along a genome GEOquery Get data from NCBI Gene Expression Omnibus (GEO) GGally Extension to 'ggplot2' ggplot2 Create Elegant Data Visualisations Using the Grammar of Graphics ggvis Interactive Grammar of Graphics glmnet Lasso and Elastic-Net Regularized Generalized Linear Models globals Identify Global Objects in R Expressions gmodels Various R Programming Tools for Model Fitting GO.db A set of annotation maps describing the entire Gene Ontology goseq Gene Ontology analyser for RNA-seq and other length biased data GOstats Tools for manipulating GO and microarrays. gplots Various R Programming Tools for Plotting Data graph graph: A package to handle graph data structures graphics The R Graphics Package grDevices The R Graphics Devices and Support for Colours and Fonts grid The Grid Graphics Package gridBase Integration of base and grid graphics gridExtra Miscellaneous Functions for \"Grid\" Graphics GSEABase Gene set enrichment data structures and methods gsl wrapper for the Gnu Scientific Library gsmoothr Smoothing tools gtable Arrange 'Grobs' in Tables gtools Various R Programming Tools Gviz Plotting data and annotation information along genomic coordinates HAC Estimation, Simulation and Visualization of Hierarchical Archimedean Copulae (HAC) haplo.stats Statistical Analysis of Haplotypes with Traits and Covariates when Linkage Phase is Ambiguous hexbin Hexagonal Binning Routines hgu95av2.db Affymetrix Human Genome U95 Set annotation data (chip hgu95av2) HI Simulation from distributions supported by nested hyperplanes highr Syntax Highlighting for R Source Code Hmisc Harrell Miscellaneous HotDeckImputation Hot Deck Imputation Methods for Missing Data htmlTable Advanced Tables for Markdown/HTML htmltools Tools for HTML htmlwidgets HTML Widgets for R httpuv HTTP and WebSocket Server Library httr Tools for Working with URLs and HTTP hugene10stprobeset.db Affymetrix hugene10 annotation data (chip hugene10stprobeset) hugene10sttranscriptcluster.db Affymetrix hugene10 annotation data (chip hugene10sttranscriptcluster) hwriter HTML Writer - Outputs R objects in HTML format igraph Network Analysis and Visualization Illumina450ProbeVariants.db Annotation Package combining variant data from 1000 Genomes Project for Illumina HumanMethylation450 Bead Chip probes IlluminaHumanMethylation450kanno.ilmn12.hg19 Annotation for Illumina's 450k methylation arrays IlluminaHumanMethylation450kmanifest Annotation for Illumina's 450k methylation arrays IlluminaHumanMethylationEPICanno.ilm10b2.hg19 Annotation for Illumina's EPIC methylation arrays IlluminaHumanMethylationEPICmanifest Manifest for Illumina's EPIC methylation arrays illuminaHumanv4.db Illumina HumanHT12v4 annotation data (chip illuminaHumanv4) illuminaio Parsing Illumina Microarray Output Files impute impute: Imputation for microarray data INLA Functions which Allow to Perform Full Bayesian Analysis of Latent Gaussian Models using Integrated Nested Laplace Approximations inline Functions to Inline C, C++, Fortran Function Calls from R interactiveDisplayBase Base package for enabling powerful shiny web displays of Bioconductor objects intervals Tools for Working with Points and Intervals IRanges Infrastructure for manipulating intervals on sequences irlba Fast Truncated SVD, PCA and Symmetric Eigendecomposition for Large Dense and Sparse Matrices isva Independent Surrogate Variable Analysis iterators Provides Iterator Construct for R jsonlite A Robust, High Performance JSON Parser and Generator for R KEGG.db A set of annotation maps for KEGG kernlab Kernel-Based Machine Learning Lab KernSmooth Functions for Kernel Smoothing Supporting Wand & Jones (1995) knitr A General-Purpose Package for Dynamic Report Generation in R labeling Axis Labeling lambda.r Modeling Data with Functional Programming lattice Trellis Graphics for R latticeExtra Extra Graphical Utilities Based on Lattice lazyeval Lazy (Non-Standard) Evaluation leaps regression subset selection LearnBayes Functions for Learning Bayesian Inference limma Linear Models for Microarray Data listenv Environments Behaving (Almost) as Lists lme4 Linear Mixed-Effects Models using 'Eigen' and S4 locfit Local Regression, Likelihood and Density Estimation. longitudinal Analysis of Multiple Time Course Data loo Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models lpSolve Interface to 'Lp_solve' v. 5.5 to Solve Linear/Integer Programs LSD Lots of Superior Depictions lumi BeadArray Specific Methods for Illumina Methylation and Expression Microarrays made4 Multivariate analysis of microarray data using ADE4 magrittr A Forward-Pipe Operator for R maps Draw Geographical Maps maptree Mapping, pruning, and graphing tree models markdown 'Markdown' Rendering for R marray Exploratory analysis for two-color spotted microarray data MASS Support Functions and Datasets for Venables and Ripley's MASS Matrix Sparse and Dense Matrix Classes and Methods MatrixModels Modelling with Sparse And Dense Matrices matrixStats Functions that Apply to Rows and Columns of Matrices (and to Vectors) mclust Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation mcmc Markov Chain Monte Carlo memoise Memoisation of Functions metafor Meta-Analysis Package for R methods Formal Methods and Classes methylumi Handle Illumina methylation data mgcv Mixed GAM Computation Vehicle with GCV/AIC/REML Smoothness Estimation mime Map Filenames to MIME Types minfi Analyze Illumina Infinium DNA methylation arrays miniUI Shiny UI Widgets for Small Screens minqa Derivative-free optimization algorithms by quadratic approximation missMethyl Analysing Illumina HumanMethylation BeadChip Data mitools Tools for multiple imputation of missing data mlr Machine Learning in R mnormt The Multivariate Normal and t Distributions ModelMetrics Rapid Calculation of Model Metrics modeltools Tools and Classes for Statistical Models msm Multi-State Markov and Hidden Markov Models in Continuous Time mstate Data Preparation, Estimation and Prediction in Multi-State Models multcomp Simultaneous Inference in General Parametric Models multtest Resampling-based multiple hypothesis testing munsell Utilities for Using Munsell Colours mvtnorm Multivariate Normal and t Distributions NetworkAnalysis Statistical inference on populations of weighted or unweighted networks. nleqslv Solve Systems of Nonlinear Equations nlme Linear and Nonlinear Mixed Effects Models nloptr R interface to NLopt NMF Algorithms and Framework for Nonnegative Matrix Factorization (NMF) nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models nnls The Lawson-Hanson algorithm for non-negative least squares (NNLS) nor1mix Normal (1-d) Mixture Models (S3 Classes and Methods) numDeriv Accurate Numerical Derivatives nutshell Data for \"R in a Nutshell\" nutshell.audioscrobbler Audioscrobbler data for \"R in a Nutshell\" nutshell.bbdb Baseball Database for \"R in a Nutshell\" OPE Outer-product emulator openssl Toolkit for Encryption, Signatures and Certificates Based on OpenSSL org.Hs.eg.db Genome wide annotation for Human packrat A Dependency Management System for Projects and their R Package Dependencies parallel Support for Parallel computation in R parallelMap Unified Interface to Parallelization Back-Ends ParamHelpers Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning parcor Regularized estimation of partial correlation matrices pbkrtest Parametric Bootstrap and Kenward Roger Based Methods for Mixed Model Comparison pcaPP Robust PCA by Projection Pursuit pegas Population and Evolutionary Genetics Analysis System permute Functions for Generating Restricted Permutations of Data phangorn Phylogenetic Analysis in R pixmap Bitmap Images (``Pixel Maps'') pkgmaker Package development utilities PKI Public Key Infrastucture for R Based on the X.509 Standard plogr The 'plog' C++ Logging Library plotly Create Interactive Web Graphics via 'plotly.js' pls Partial Least Squares and Principal Component Regression plyr Tools for Splitting, Applying and Combining Data polspline Polynomial Spline Routines poweRlaw Analysis of Heavy Tailed Distributions ppls Penalized Partial Least Squares prabclus Functions for Clustering of Presence-Absence, Abundance and Multilocus Genetic Data pracma Practical Numerical Math Functions praise Praise Users preprocessCore A collection of pre-processing functions prettyunits Pretty, Human Readable Formatting of Quantities progress Terminal Progress Bars proto Prototype Object-Based Programming PSCBS Analysis of Parent-Specific DNA Copy Numbers pspline Penalized Smoothing Splines psych Procedures for Psychological, Psychometric, and Personality Research purrr Functional Programming Tools quadprog Functions to solve Quadratic Programming Problems. quantreg Quantile Regression qvalue Q-value estimation for false discovery rate control R.cache Fast and Light-Weight Caching (Memoization) of Objects and Results to Speed Up Computations R.devices Unified Handling of Graphics Devices R.filesets Easy Handling of and Access to Files Organized in Structured Directories R.huge Methods for Accessing Huge Amounts of Data [deprecated] R.methodsS3 S3 Methods Simplified R.oo R Object-Oriented Programming with or without References R.rsp Dynamic Generation of Scientific Reports R.utils Various Programming Utilities R2HTML HTML Exportation for R Objects R2jags Using R to Run 'JAGS' R2WinBUGS Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS' R6 Classes with Reference Semantics randomForest Breiman and Cutler's Random Forests for Classification and Regression RBGL An interface to the BOOST graph library RColorBrewer ColorBrewer Palettes Rcpp Seamless R and C++ Integration RcppArmadillo 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra Library RcppEigen 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra Library RcppGSL 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices RCurl General Network (HTTP/FTP/...) Client Interface for R RefFreeEWAS EWAS using Reference-Free DNA Methylation Mixture Deconvolution registry Infrastructure for R Package Registries relaimpo Relative importance of regressors in linear models Repitools Epigenomic tools reshape Flexibly Reshape Data reshape2 Flexibly Reshape Data: A Reboot of the Reshape Package rgdal Bindings for the Geospatial Data Abstraction Library rgeos Interface to Geometry Engine - Open Source (GEOS) rgl 3D Visualization Using OpenGL Rglpk R/GNU Linear Programming Kit Interface ridge Ridge Regression with automatic selection of the penalty parameter Ringo R Investigation of ChIP-chip Oligoarrays rjags Bayesian Graphical Models using MCMC RJSONIO Serialize R objects to JSON, JavaScript Object Notation rlecuyer R Interface to RNG with Multiple Streams Rmpi Interface (Wrapper) to MPI (Message-Passing Interface) rms Regression Modeling Strategies RMySQL Database Interface and 'MySQL' Driver for R RNetCDF Interface to NetCDF Datasets rngtools Utility functions for working with Random Number Generators robustbase Basic Robust Statistics ROC utilities for ROC, with uarray focus rpart Recursive Partitioning and Regression Trees RPMM Recursively Partitioned Mixture Model Rsamtools Binary alignment (BAM), FASTA, variant call (BCF), and tabix file import rsconnect Deployment Interface for R Markdown Documents and Shiny Applications Rsolnp General Non-Linear Optimization RSQLite 'SQLite' Interface for R rstan R Interface to Stan rstanarm Bayesian Applied Regression Modeling via Stan rstantools Tools for Developing R Packages Interfacing with 'Stan' rstudioapi Safely Access the RStudio API Rsubread Subread sequence alignment for R rtracklayer R interface to genome browsers and their annotation tracks ruv Detect and Remove Unwanted Variation using Negative Controls S4Vectors S4 implementation of vectors and lists saemix Stochastic Approximation Expectation Maximization (SAEM) algorithm sampling Survey Sampling sandwich Robust Covariance Matrix Estimators scales Scale Functions for Visualization scatterplot3d 3D Scatter Plot schoolmath Functions and datasets for math used in school segmented Regression Models with Breakpoints/Changepoints Estimation sendmailR send email using R seqinr Biological Sequences Retrieval and Analysis seqLogo Sequence logos for DNA sequence alignments shiny Web Application Framework for R shinyjs Easily Improve the User Experience of Your Shiny Apps in Seconds shinystan Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models shinythemes Themes for Shiny ShortRead FASTQ input and manipulation siggenes Multiple testing using SAM and Efron's empirical Bayes approaches simpleaffy Very simple high level analysis of Affymetrix data slam Sparse Lightweight Arrays and Matrices sn The Skew-Normal and Skew-t Distributions snow Simple Network of Workstations snowfall Easier cluster computing (based on snow). softImpute Matrix Completion via Iterative Soft-Thresholded SVD sourcetools Tools for Reading, Tokenizing and Parsing R Code sp Classes and Methods for Spatial Data spam SPArse Matrix SparseM Sparse Linear Algebra spatial Functions for Kriging and Point Pattern Analysis spdep Spatial Dependence: Weighting Schemes, Statistics and Models splines Regression Spline Functions and Classes stabledist Stable Distribution Functions StanHeaders C++ Header Files for Stan statmod Statistical Modeling stats The R Stats Package stats4 Statistical Functions using S4 Classes stringdist Approximate String Matching and String Distance Functions stringi Character String Processing Facilities stringr Simple, Consistent Wrappers for Common String Operations SummarizedExperiment SummarizedExperiment container survey Analysis of Complex Survey Samples survival Survival Analysis sva Surrogate Variable Analysis tcltk Tcl/Tk Interface testthat Unit Testing for R tgp Bayesian Treed Gaussian Process Models TH.data TH's Data Archive threejs Interactive 3D Scatter Plots, Networks and Globes tibble Simple Data Frames tidyr Easily Tidy Data with spread() and gather() Functions tkrplot TK Rplot tools Tools for Package Development trimcluster Cluster analysis with trimming truncnorm Truncated normal distribution tseries Time Series Analysis and Computational Finance TxDb.Hsapiens.UCSC.hg19.knownGene Annotation package for TxDb object(s) utils The R Utils Package VariantAnnotation Annotation of Genetic Variants vegan Community Ecology Package VGAM Vector Generalized Linear and Additive Models viridis Default Color Maps from 'matplotlib' viridisLite Default Color Maps from 'matplotlib' (Lite Version) vsn Variance stabilization and calibration for microarray data wateRmelon Illumina 450 methylation array normalization and metrics WGCNA Weighted Correlation Network Analysis whisker for R, logicless templating widgetTools Creates an interactive tcltk widget XML Tools for Parsing and Generating XML Within R and S-Plus xps Processing and Analysis of Affymetrix Oligonucleotide Arrays including Exon Arrays, Whole Genome Arrays and Plate Arrays xtable Export Tables to LaTeX or HTML xts eXtensible Time Series XVector Representation and manpulation of external sequences yaml Methods to Convert R Data to YAML and Back zlibbioc An R packaged zlib-1.2.5 zoo S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)","title":"R and Bioconductor Packages installed on RC Systems"},{"location":"Wiki_Export/software/unedited/Running_CP2K_on_Legion/","text":"2.2.426 \u00a7 The module for this version is contained in the chemistry modules, so you will need to load the module set that contains it before it is visible in the modules list: ``` module load chemistry-modules CP2K should then be visible as: module avail [.....] ----- /shared/ucl/depts/chemistry/modulefiles ----- cp2k/2.2.426/openmpi/gnu.4.6.3/cp2k ### Submitter An automatic submitter is available for this version, available by loading the submission scripts module from the chemistry set: ``` `module load chemistry-modules` `module load submission-scripts` ``` The alias \"submitters\" will then list the submitters available. The \"cp2k.submit\" submitter takes up to 5 arguments, and any omitted will be asked for interactively: cp2k.submit \u00abinput_file\u00bb \u00abcores\u00bb \u00abmaximum_run_time\u00bb \u00abmemory_per_core\u00bb \u00abjob_name\u00bb So, for example: cp2k.submit water.inp 8 2:00:00 4G mywatermolecule ``` would request a job running CP2K with the input file \"water.inp\", on 8 cores, with a maximum runtime of 2 hours, with 4 gigabytes of memory per core, and a job name of \"mywatermolecule\". Job script \u00a7 As with the previous versions, some extra arguments to the OpenMPI mpirun command are necessary. An example job script is below: ``` ```#!/bin/bash -l``` ```# setting up the environment for SGE:``` ```#$ -S /bin/bash -l``` ```#$ -cwd``` ```#$ -N CP2K-Job``` ```# Set your project name here:``` ```#$ -P Project_Name ``` ```# If necessary, alter the maximum quantity of memory here:``` ```#$ -l mem=2G``` ```# Alter the maximum run time here (hours:minutes:seconds)``` ```#$ -l h_rt=10:00:00``` ```# Alter the number of processors here:``` ```#$ -pe openmpi 1 ``` ```# loading the correct modules``` ```module add chemistry-modules``` ```module add cp2k/2.2.426/openmpi/gnu.4.6.3/cp2k ``` ```#Modify this to name your input file``` ```InputFile=ACP2KFile.inp``` ```OutputFile=${InputFile%\\.inp}.log``` ```mpirun --mca btl ^tcp -n $NSLOTS cp2k.popt $InputFile > $OutputFile``` ``` branch 2_1 and trunk \u00a7 To use these versions of CP2K on Legion, you will have to make considerable changes to your user environment, including using a test build of OpenMPI specifically designed for this purpose. You also need to pick which CP2K package you wish to use: trunk, or branch 2_1 - the jobscript below selects branch 2_1 - if you require trunk, you will need to change the module specification. Job script \u00a7 In order to use this special version of OpenMPI, you need to make a few changes to the normal OpenMPI job script. An example job script is shown below: ``` ```#!/bin/bash -l``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=0:30:0``` ```# 3. Request 1 gigabyte of RAM.``` ```#$ -l mem=1G``` ```# 4. Set the name of the job.``` ```#$ -N cp2k-branch-openmpi-gcc440-test-dev``` ```# 5. Select the OpenMPI parallel environment and 8 processors.``` ```#$ -pe openmpi 8``` ```# 6. Select the project that this job will run under.``` ```#$ -P ``` ```# 7. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```#$ -wd /home/``` ```/scratch/``` ```# 8. Run our MPI job. ``` ```module add compilers/gnu/4.4.0``` ```module add mpi/openmpi/1.4.1/gnu.4.4.0``` ```module add fftw/2.1.5/double/gnu.4.4.0``` ```module add atlas/3.8.3/gnu.4.4.0``` ```module add blacs/patch03/gnu.4.4.0``` ```module add scalapack/1.8.0/gnu.4.4.0``` ```module add cp2k/2_1-branch/openmpi/gnu.4.4.0``` ```# Delete OpenMPI PE SSH wrapper``` ```rm $TMPDIR/ssh``` ```# Need to add in --prefix $MPI_HOME as not using system OpenMPI``` ```mpirun --prefix $MPI_HOME -machinefile $TMPDIR/machines -np $NSLOTS `which cp2k.popt` C.inp``` ``` The main differences between this script and the normal OpenMPI scripts is that we have to delete an SSH wrapper from $TMPDIR/ssh and insert --prefix $MPI_HOME into our mpirun command. Note that we've also forced the loading of the correct modules in the job script. This assumes you have an input file called C.inp in your working directory. Amend this script as necessary. If you put the modules into your job script as above, you should not need to have them in your .bashrc, but it's probably worth doing so just to be on the safe side, unless doing so causes clashes with other programs you want to use. Category:Bash script pages Category:Legion Category:Legion Applications Category:Legion Software","title":"Running CP2K on Legion"},{"location":"Wiki_Export/software/unedited/Running_CP2K_on_Legion/#22426","text":"The module for this version is contained in the chemistry modules, so you will need to load the module set that contains it before it is visible in the modules list: ``` module load chemistry-modules CP2K should then be visible as: module avail [.....] ----- /shared/ucl/depts/chemistry/modulefiles ----- cp2k/2.2.426/openmpi/gnu.4.6.3/cp2k ### Submitter An automatic submitter is available for this version, available by loading the submission scripts module from the chemistry set: ``` `module load chemistry-modules` `module load submission-scripts` ``` The alias \"submitters\" will then list the submitters available. The \"cp2k.submit\" submitter takes up to 5 arguments, and any omitted will be asked for interactively: cp2k.submit \u00abinput_file\u00bb \u00abcores\u00bb \u00abmaximum_run_time\u00bb \u00abmemory_per_core\u00bb \u00abjob_name\u00bb So, for example: cp2k.submit water.inp 8 2:00:00 4G mywatermolecule ``` would request a job running CP2K with the input file \"water.inp\", on 8 cores, with a maximum runtime of 2 hours, with 4 gigabytes of memory per core, and a job name of \"mywatermolecule\".","title":"2.2.426"},{"location":"Wiki_Export/software/unedited/Running_CP2K_on_Legion/#job-script","text":"As with the previous versions, some extra arguments to the OpenMPI mpirun command are necessary. An example job script is below: ``` ```#!/bin/bash -l``` ```# setting up the environment for SGE:``` ```#$ -S /bin/bash -l``` ```#$ -cwd``` ```#$ -N CP2K-Job``` ```# Set your project name here:``` ```#$ -P Project_Name ``` ```# If necessary, alter the maximum quantity of memory here:``` ```#$ -l mem=2G``` ```# Alter the maximum run time here (hours:minutes:seconds)``` ```#$ -l h_rt=10:00:00``` ```# Alter the number of processors here:``` ```#$ -pe openmpi 1 ``` ```# loading the correct modules``` ```module add chemistry-modules``` ```module add cp2k/2.2.426/openmpi/gnu.4.6.3/cp2k ``` ```#Modify this to name your input file``` ```InputFile=ACP2KFile.inp``` ```OutputFile=${InputFile%\\.inp}.log``` ```mpirun --mca btl ^tcp -n $NSLOTS cp2k.popt $InputFile > $OutputFile``` ```","title":"Job script"},{"location":"Wiki_Export/software/unedited/Running_CP2K_on_Legion/#branch-2_1-and-trunk","text":"To use these versions of CP2K on Legion, you will have to make considerable changes to your user environment, including using a test build of OpenMPI specifically designed for this purpose. You also need to pick which CP2K package you wish to use: trunk, or branch 2_1 - the jobscript below selects branch 2_1 - if you require trunk, you will need to change the module specification.","title":"branch 2_1 and trunk"},{"location":"Wiki_Export/software/unedited/Running_CP2K_on_Legion/#job-script_1","text":"In order to use this special version of OpenMPI, you need to make a few changes to the normal OpenMPI job script. An example job script is shown below: ``` ```#!/bin/bash -l``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=0:30:0``` ```# 3. Request 1 gigabyte of RAM.``` ```#$ -l mem=1G``` ```# 4. Set the name of the job.``` ```#$ -N cp2k-branch-openmpi-gcc440-test-dev``` ```# 5. Select the OpenMPI parallel environment and 8 processors.``` ```#$ -pe openmpi 8``` ```# 6. Select the project that this job will run under.``` ```#$ -P ``` ```# 7. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```#$ -wd /home/``` ```/scratch/``` ```# 8. Run our MPI job. ``` ```module add compilers/gnu/4.4.0``` ```module add mpi/openmpi/1.4.1/gnu.4.4.0``` ```module add fftw/2.1.5/double/gnu.4.4.0``` ```module add atlas/3.8.3/gnu.4.4.0``` ```module add blacs/patch03/gnu.4.4.0``` ```module add scalapack/1.8.0/gnu.4.4.0``` ```module add cp2k/2_1-branch/openmpi/gnu.4.4.0``` ```# Delete OpenMPI PE SSH wrapper``` ```rm $TMPDIR/ssh``` ```# Need to add in --prefix $MPI_HOME as not using system OpenMPI``` ```mpirun --prefix $MPI_HOME -machinefile $TMPDIR/machines -np $NSLOTS `which cp2k.popt` C.inp``` ``` The main differences between this script and the normal OpenMPI scripts is that we have to delete an SSH wrapper from $TMPDIR/ssh and insert --prefix $MPI_HOME into our mpirun command. Note that we've also forced the loading of the correct modules in the job script. This assumes you have an input file called C.inp in your working directory. Amend this script as necessary. If you put the modules into your job script as above, you should not need to have them in your .bashrc, but it's probably worth doing so just to be on the safe side, unless doing so causes clashes with other programs you want to use. Category:Bash script pages Category:Legion Category:Legion Applications Category:Legion Software","title":"Job script"},{"location":"Wiki_Export/software/unedited/Running_DL_Poly_4_on_Legion/","text":"A build of DL_Poly 4 is available to users on Legion. It has been built with the default modules environment (Intel compilers and QLogic MPI). Please read and understand the DL_Poly 4 license . Please read this as it restricts what you may do with the software. In order to use the central builds of DL_Poly 4, you will need to be added to the appropriate UNIX user group. Please forward your registration confirmation e-mail to rc-support@ucl.ac.uk to be added to the group. DL_Poly looks for its input files in the directory which its executable has been placed. Since this is not practical in a multi-user environment, it is necessary for a user to copy the DL_Poly executable to their working directory before running it. The script below does this as part of the job, as well as creating a job-specific working directory. In addition, many of the scripts provided with the DL_Poly distribution that live in the execute subdirectory have had the execute bit removed so that they cannot run (they would not work anyway as users do not have permission to write to this directory). Example submission script \u00a7 You will need to modify the contents of section 7 so that it points to the directory with your input files in it, as well as correctly setting the project in section 6. This script will create a sub-directory of that directory called _ with your data and output in it. More advanced users may wish to create their own scripts and work-flows around the DL_Poly binary. This binary may be found in /shared/ucl/apps/dl_poly/4/4.01/execute/DLPOLY.Z on Legion. ``` ```#!/bin/bash -i``` ```# Batch script to run an MPI DL_POLY job on Legion with the upgraded ``` ```# software stack under SGE with QLogic MPI.``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request thirty minutes of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=0:30:0``` ```# 3. Request 1 gigabyte of RAM.``` ```#$ -l mem=1G``` ```# 4. Set the name of the job.``` ```#$ -N DL_POLY``` ```# 5. Select the QLogic parallel environment (qlc) and 16 processors.``` ```#$ -pe qlc 16``` ```# 6. Select the project that this job will run under.``` ```#$ -P ``` ```# 7. Set the working directory to somewhere in your scratch space. This ``` ```# is a necessary step with the upgraded software stack as compute nodes ``` ```# cannot write to $HOME. This should be set to the directory where ``` ```# your DL_Poly input files are.``` ```#$ -wd /home/``` ```/scratch/output/``` ```# 8. Now we need to set up and run our DL_Poly job. DL_Poly is a bit ``` ```# odd, so we need to move files about to make things run.``` ```dl_poly_work_dir=$SGE_O_WORKDIR/${JOB_NAME}_${JOB_ID}``` ```dl_poly_executable=/shared/ucl/apps/dl_poly/4/4.01/execute/DLPOLY.Z``` ```# Make a working directory.``` ```mkdir $dl_poly_work_dir``` ```# Copy DL_Poly input files to temporary directory.``` ```for a in CONTROL FIELD CONFIG TABLE TABEAM``` ```do``` ``` if [ -f $SGE_O_WORKDIR/$a ]; then``` ``` echo Copying $SGE_O_WORKDIR/$a to $dl_poly_work_dir/$a``` ``` cp $SGE_O_WORKDIR/$a $dl_poly_work_dir/$a``` ``` else``` ``` echo No $SGE_O_WORKDIR/$a found.``` ``` fi``` ```done``` ```# Copy DL_Poly executable to temporary working directory.``` ```echo Copying DL_Poly executable to $dl_poly_work_dir``` ```cp $dl_poly_executable $dl_poly_work_dir``` ```# Run it.``` ```cd $dl_poly_work_dir``` ```echo Running $dl_poly_work_dir/DLPOLY.Z.``` ```mpirun -m $TMPDIR/machines -np $NSLOTS $dl_poly_work_dir/DLPOLY.Z``` ```# Delete executable.``` ```rm $dl_poly_work_dir/DLPOLY.Z``` ``` Category:Bash script pages Category:Legion","title":"Running DL Poly 4 on Legion"},{"location":"Wiki_Export/software/unedited/Running_DL_Poly_4_on_Legion/#example-submission-script","text":"You will need to modify the contents of section 7 so that it points to the directory with your input files in it, as well as correctly setting the project in section 6. This script will create a sub-directory of that directory called _ with your data and output in it. More advanced users may wish to create their own scripts and work-flows around the DL_Poly binary. This binary may be found in /shared/ucl/apps/dl_poly/4/4.01/execute/DLPOLY.Z on Legion. ``` ```#!/bin/bash -i``` ```# Batch script to run an MPI DL_POLY job on Legion with the upgraded ``` ```# software stack under SGE with QLogic MPI.``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request thirty minutes of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=0:30:0``` ```# 3. Request 1 gigabyte of RAM.``` ```#$ -l mem=1G``` ```# 4. Set the name of the job.``` ```#$ -N DL_POLY``` ```# 5. Select the QLogic parallel environment (qlc) and 16 processors.``` ```#$ -pe qlc 16``` ```# 6. Select the project that this job will run under.``` ```#$ -P ``` ```# 7. Set the working directory to somewhere in your scratch space. This ``` ```# is a necessary step with the upgraded software stack as compute nodes ``` ```# cannot write to $HOME. This should be set to the directory where ``` ```# your DL_Poly input files are.``` ```#$ -wd /home/``` ```/scratch/output/``` ```# 8. Now we need to set up and run our DL_Poly job. DL_Poly is a bit ``` ```# odd, so we need to move files about to make things run.``` ```dl_poly_work_dir=$SGE_O_WORKDIR/${JOB_NAME}_${JOB_ID}``` ```dl_poly_executable=/shared/ucl/apps/dl_poly/4/4.01/execute/DLPOLY.Z``` ```# Make a working directory.``` ```mkdir $dl_poly_work_dir``` ```# Copy DL_Poly input files to temporary directory.``` ```for a in CONTROL FIELD CONFIG TABLE TABEAM``` ```do``` ``` if [ -f $SGE_O_WORKDIR/$a ]; then``` ``` echo Copying $SGE_O_WORKDIR/$a to $dl_poly_work_dir/$a``` ``` cp $SGE_O_WORKDIR/$a $dl_poly_work_dir/$a``` ``` else``` ``` echo No $SGE_O_WORKDIR/$a found.``` ``` fi``` ```done``` ```# Copy DL_Poly executable to temporary working directory.``` ```echo Copying DL_Poly executable to $dl_poly_work_dir``` ```cp $dl_poly_executable $dl_poly_work_dir``` ```# Run it.``` ```cd $dl_poly_work_dir``` ```echo Running $dl_poly_work_dir/DLPOLY.Z.``` ```mpirun -m $TMPDIR/machines -np $NSLOTS $dl_poly_work_dir/DLPOLY.Z``` ```# Delete executable.``` ```rm $dl_poly_work_dir/DLPOLY.Z``` ``` Category:Bash script pages Category:Legion","title":"Example submission script"},{"location":"Wiki_Export/software/unedited/Running_GROMACS_on_Legion/","text":"GROMACS on Legion was built with FFTW, the OpenMPI library and the GNU compilers. It is therefore strongly recommended that you have these modules loaded when running it: sge/6.2u3 compilers/gnu/4.6.3 mpi/openmpi/1.4.5/gnu.4.6.3 gromacs/4.6.1/openmpi/gnu.4.6.3 Which version you require depends on the problem you wish to solve. For both single and double precisions version builds, serial binaries and an MPI binary for mdrun (mdrun_mpi) are provided. Double precision binaries have a _d suffix (so mdrun_d, mdrun_mpi_d etc). The MPI binaries should be run in the OpenMPI (openmpi) parallel environment with the OpenMPI implementation. Job Script \u00a7 ``` ```#!/bin/bash -l``` ```# Batch script to run an MPI parallel GROMACS job on Legion with the upgraded software``` ```# stack under SGE with OpenMPI.``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=0:10:0``` ```# 3. Request 1 gigabyte of RAM per process.``` ```#$ -l mem=1G``` ```# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)``` ```#$ -l tmpfs=15G``` ```# 5. Set the name of the job.``` ```#$ -N GROMACS_1_16_OpenMPI``` ```# 6. Select the OpenMPI parallel environment and 16 processes.``` ```#$ -pe openmpi 16``` ```# 7. Select the project that this job will run under.``` ```# Find ``` ``` by running the command \"groups\"``` ```#$ -P ``` ```# 8. Set the working directory of the job to the current directory``` ```# containing your input files.``` ```# This *has* to be somewhere in your Scratch space, or else your``` ```# job will go into the Eqw state.``` ```#$ -cwd``` ```module unload compilers/intel/11.1/072``` ```module unload mpi/qlogic/1.2.7/intel``` ```module unload mkl/10.2.5/035``` ```module load compilers/gnu/4.6.3``` ```module load mpi/openmpi/1.4.5/gnu.4.6.3``` ```module load gromacs/4.6.1/openmpi/gnu.4.6.3``` ```# Run GROMACS - replace with mdrun command line suitable for your job!``` ```gerun mdrun_mpi -v -stepout 10000``` ``` Category:Bash script pages Category:Legion","title":"Running GROMACS on Legion"},{"location":"Wiki_Export/software/unedited/Running_GROMACS_on_Legion/#job-script","text":"``` ```#!/bin/bash -l``` ```# Batch script to run an MPI parallel GROMACS job on Legion with the upgraded software``` ```# stack under SGE with OpenMPI.``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=0:10:0``` ```# 3. Request 1 gigabyte of RAM per process.``` ```#$ -l mem=1G``` ```# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)``` ```#$ -l tmpfs=15G``` ```# 5. Set the name of the job.``` ```#$ -N GROMACS_1_16_OpenMPI``` ```# 6. Select the OpenMPI parallel environment and 16 processes.``` ```#$ -pe openmpi 16``` ```# 7. Select the project that this job will run under.``` ```# Find ``` ``` by running the command \"groups\"``` ```#$ -P ``` ```# 8. Set the working directory of the job to the current directory``` ```# containing your input files.``` ```# This *has* to be somewhere in your Scratch space, or else your``` ```# job will go into the Eqw state.``` ```#$ -cwd``` ```module unload compilers/intel/11.1/072``` ```module unload mpi/qlogic/1.2.7/intel``` ```module unload mkl/10.2.5/035``` ```module load compilers/gnu/4.6.3``` ```module load mpi/openmpi/1.4.5/gnu.4.6.3``` ```module load gromacs/4.6.1/openmpi/gnu.4.6.3``` ```# Run GROMACS - replace with mdrun command line suitable for your job!``` ```gerun mdrun_mpi -v -stepout 10000``` ``` Category:Bash script pages Category:Legion","title":"Job Script"},{"location":"Wiki_Export/software/unedited/Running_Gaussian_09_on_Legion/","text":"Gaussian 09 (G09) revisions C.01 and the older A.02 are available on legion and currently can be used in either: Serial mode and shared memory parallel mode within single nodes using at most four processors when running on type W nodes or twelve processors when running on type X, Y or Z nodes. Module: gaussian/g09_a02/pgi Linda parallel execution mode across multiple nodes. Note: not all calculations are Linda parallel. From the Gaussian documentation \"HF, CIS=Direct, and DFT calculations on molecules are Linda parallel, including energies, optimizations and frequencies. TDDFT energies and MP2 energies and gradients are also Linda parallel. PBC calculations are not Linda parallel. The default for molecules larger than 65 atoms is to use the linear scaling algorithms (FMM). The linear scaling (FMM-based) algorithms are now Linda-parallel, so Linda parallel jobs on large molecules do not need to specify NoFMM, and will run faster with the default algorithms chosen by the program.\" Module: gaussian/g09_a02_linda/pgi Access to G09 is enabled by a module file and being a member of the appropriate reserved application group. Please email rc-support@ucl.ac.uk to get your userid added to the G09 group. All G09 jobs apart from small test jobs (4 cores and less than 5 minutes runtime) must be submitted as batch jobs. Before you can run G09 interactively you need to load the G09 module and run an initialisation script using: ``` module load gaussian/g09_c01_linda/pgi . $g09root/g09/bsd/g09.profile for revision C.01. To use the older version use the *gaussian/g09\\_a02\\_linda/pgi* module instead. You can use: module list to check that the module is loaded. Output should look similar to this: Currently Loaded Modulefiles: 1) ucl 6) nedit/5.6 2) compilers/intel/11.1/072 7) mrxvt/0.5.4 3) mkl/10.2.5/035 8) rcops/1.0 4) mpi/qlogic/1.2.7/intel 9) gaussian/g09_c01_linda/pgi 5) sge/6.2u3 You should now be able to run G09 using: g09 < myG09input > myG09output ``` for example. To submit batch jobs to the cluster you will need a runscript. Shared memory Gaussian jobs \u00a7 Here is a simple example G09 runscript for shared memory jobs including comments: ``` ```#!/bin/bash -l``` ```# Batch script to run a shared memory Gaussian 09 job on Legion with the``` ```# upgraded software stack under SGE.``` ```#``` ```# Aug 2012``` ```#``` ```# Based on openmp.sh by:``` ```#``` ```# Owain Kenway, Research Computing, 16/Sept/2010``` ```#$ -S /bin/bash``` ```# 1. Request 12 hours of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=12:0:0``` ```# 2. Request 4 gigabyte of RAM.``` ```#$ -l mem=4G``` ```# 3. Set the name of the job.``` ```#$ -N G09_job1``` ```# 4. Select 4 OpenMP threads (the most possible on Legion is 12).``` ```#$ -l thr=4``` ```# 5. Select the project that this job will run under.``` ```#$ -P ``` ```# 6. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```#``` ```# Note: this directory MUST exist before your job starts!``` ```#$ -wd /home/``` ```/Scratch/G09_output``` ```# Setup G09 runtime environment``` ```module load gaussian/g09_c01_linda/pgi``` ```source $g09root/g09/bsd/g09.profile``` ```mkdir -p $GAUSS_SCRDIR``` ```# Run g09 job``` ```echo \"GAUSS_SCRDIR = $GAUSS_SCRDIR\"``` ```echo \"\"``` ```echo \"Running: g09 < $g09infile > $g09outfile\"``` ```g09 < $g09infile > $g09outfile``` ``` This is available on Legion in: ``` /shared/ucl/apps/Gaussian/G09_C01_L/run-g09.sh ``` Please copy if you wish and edit it to suit your jobs. You will need to change the -P and -wd /home/ /Scratch/G09_output SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be: ``qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09.sh`` ``` where *Mydata.com* is the file containing your G09 commands and *MyOuput.out* is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive. ### Linda parallel Gaussian jobs Here is a simple example G09 runscript for Linda parallel jobs including comments: <table> <tbody> <tr class=\"odd\"> <td><p>```</p> <p>```#!/bin/bash -l```</p> <p>```# Batch script to run a Gaussian 09 job on Legion using Linda with the upgraded```<br /> ```# software stack under SGE.```<br /> ```#```<br /> ```# Aug 2012```<br /> ```#```<br /> ```# Based on the Qlogic MPI and Hybrid example scripts at```<br /> ```#```<br /> ```# ```<a href=\"http://www.ucl.ac.uk/isd/staff/research_services/research-computing/services/legion-upgrade/userguide/submissionscripts\">```http://www.ucl.ac.uk/isd/staff/research_services/research-computing/services/legion-upgrade/userguide/submissionscripts```</a></p> <p>```#$ -S /bin/bash```</p> <p>```# 2. Request 12 hours of wallclock time (format hours:minutes:seconds).```<br /> ```#$ -l h_rt=12:0:0```</p> <p>```# 3. Request 4 gigabyte of RAM.```<br /> ```#$ -l mem=4G```</p> <p>```# 4. Set the name of the job.```<br /> ```#$ -N G09l_job1```</p> <p>```# 5a. Select the QLogic parallel environment (qlc) and 2 processes (Linda ```<br /> ```# workers).```<br /> ```#$ -pe qlc 2```</p> <p>```# 5b. Select number of threads per Linda worker (value of NProcShared in your```<br /> ```# Gaussian input file.```<br /> ```#$ -l thr=4```</p> <p>```# 6. Select the project that this job will run under.```<br /> ```#$ -P ```<your_project_name></p> <p>```# 7. Set the working directory to somewhere in your scratch space. This is```<br /> ```# a necessary step with the upgraded software stack as compute nodes cannot```<br /> ```# write to $HOME.```<br /> ```#```<br /> ```# Note: this directory MUST exist before your job starts!```<br /> ```#$ -wd /home/```<your_UCL_id>```/Scratch/G09_output```</p> <p>```# 8. Run our G09 Linda job.```</p> <p>```# Setup G09 runtime environment```</p> <p>```module load gaussian/g09_c01_linda/pgi```<br /> ```source $g09root/g09/bsd/g09.profile```<br /> ```mkdir -p $GAUSS_SCRDIR```</p> <p>```# Pre-process G09 input file to include nodes alocated to job```</p> <p>```echo &quot;Running: lindaConv $g09infile $JOB_ID $TMPDIR/machines&quot;```<br /> ```echo ''```<br /> ```$lindaConv $g09infile $JOB_ID $TMPDIR/machines```</p> <p>```# Run g09 job```</p> <p>```echo &quot;GAUSS_SCRDIR = $GAUSS_SCRDIR&quot;```<br /> ```echo &quot;&quot;```<br /> ```echo &quot;Running: g09 &lt; job$JOB_ID.com &gt; $g09outfile&quot;```</p> <p>```# communication needs to be via ssh not the Linda default```<br /> ```export GAUSS_LFLAGS='-v -opt &quot;Tsnet.Node.lindarsharg: ssh&quot;'```</p> <p>```time g09 &lt; job$JOB_ID.com &gt; $g09outfile```</p> <p>```</p></td> </tr> </tbody> </table> This script is more complicated that the shared memory example as your Gaussian input file needs to be preprocessed to insert information about the nodes SGE has allocated to the job. This is available on Legion in: ``` `/shared/ucl/apps/Gaussian/G09_C01_L/run-g09-linda.sh` Please copy if you wish and edit it to suit your jobs. You will need to change the -P and -wd /home/ /Scratch/G09_output SGE directives and may need to change the memory, wallclock time, number of Linda workers (the -pe directive), number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be: ``qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09-linda.sh`` where Mydata.com is the file containing your G09 commands and MyOuput.out is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive. Submitting Long Gaussian Jobs \u00a7 It is possible to obtain permission to submit single node Gaussian jobs with wallclock times between 2 and 7 days. For details of how to gain access to the 7-day Gaussian queue see Requests for Additional Resources . As the 7-day queue is restricted to shared memory Gaussian jobs you will need to make some changes to your runscripts: Include the grid engine directive: #$ -ac app=g09 for Gaussian 09 jobs. If the directive is not present, normal job wallclock limits apply. The way Gaussian is launched needs to be modified as the new queues launch g09 via a new wrapper command. The new wrapper is G09 - note the capital G! It takes arguments for Gaussian command (standard input) and output (standard output) files so need to be used like so: G09 commands.in output.out where commands.in is the file containing your Gaussian commands and output.out is the file where standard output will appear. The G03 wrapper is used in the same way. Here is a simple Gaussian 09 runscript using the new '7-day' queue: ``` ```#!/bin/bash -l``` ```# Batch script to run a long Gaussian 09 shared memory job on Legion using ``` ```# the restricted '7-day' queue under SGE.``` ```#``` ```# Aug 2012``` ```#``` ```# Based on openmp.sh by:``` ```#``` ```# Owain Kenway, Research Computing, 16/Sept/2010``` ```#$ -S /bin/bash``` ```# 1. Request 168 hours of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=168:0:0``` ```# 2. Request 4 gigabyte of RAM.``` ```#$ -l mem=8G``` ```#$ -ac app=g09``` ```# 3. Set the name of the job.``` ```#$ -N G09_jobR``` ```# 4. Select 12 OpenMP threads (the most possible on Legion).``` ```#$ -l thr=12``` ```# 5. Select the project that this job will run under.``` ```#$ -P ``` ```# 6. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```#``` ```# Note: this directory MUST exist before your job starts!``` ```#$ -wd /home/``` ```/Scratch/G09_output``` ```# Run g09 job``` ```echo \"GAUSS_SCRDIR = $GAUSS_SCRDIR\"``` ```echo \"\"``` ```echo \"Running: G09 $g09infile $g09outfile\"``` ```time G09 $g09infile $g09outfile``` ``` This is available on Legion in: ``` /shared/ucl/apps/Gaussian/G09_C01_L/run-g09-res.sh Please copy if you wish and edit it to suit your jobs. You will need to change the *-P <your_project_name>* and *-wd /home/<your_UCL_id>/Scratch/G09\\_output* SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be: qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09-res.sh ``` where Mydata.com is the file containing your G09 commands and MyOuput.out is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive. Category:Bash script pages Category:Legion","title":"Running Gaussian 09 on Legion"},{"location":"Wiki_Export/software/unedited/Running_Gaussian_09_on_Legion/#shared-memory-gaussian-jobs","text":"Here is a simple example G09 runscript for shared memory jobs including comments: ``` ```#!/bin/bash -l``` ```# Batch script to run a shared memory Gaussian 09 job on Legion with the``` ```# upgraded software stack under SGE.``` ```#``` ```# Aug 2012``` ```#``` ```# Based on openmp.sh by:``` ```#``` ```# Owain Kenway, Research Computing, 16/Sept/2010``` ```#$ -S /bin/bash``` ```# 1. Request 12 hours of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=12:0:0``` ```# 2. Request 4 gigabyte of RAM.``` ```#$ -l mem=4G``` ```# 3. Set the name of the job.``` ```#$ -N G09_job1``` ```# 4. Select 4 OpenMP threads (the most possible on Legion is 12).``` ```#$ -l thr=4``` ```# 5. Select the project that this job will run under.``` ```#$ -P ``` ```# 6. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```#``` ```# Note: this directory MUST exist before your job starts!``` ```#$ -wd /home/``` ```/Scratch/G09_output``` ```# Setup G09 runtime environment``` ```module load gaussian/g09_c01_linda/pgi``` ```source $g09root/g09/bsd/g09.profile``` ```mkdir -p $GAUSS_SCRDIR``` ```# Run g09 job``` ```echo \"GAUSS_SCRDIR = $GAUSS_SCRDIR\"``` ```echo \"\"``` ```echo \"Running: g09 < $g09infile > $g09outfile\"``` ```g09 < $g09infile > $g09outfile``` ``` This is available on Legion in: ``` /shared/ucl/apps/Gaussian/G09_C01_L/run-g09.sh ``` Please copy if you wish and edit it to suit your jobs. You will need to change the -P and -wd /home/ /Scratch/G09_output SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be: ``qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09.sh`` ``` where *Mydata.com* is the file containing your G09 commands and *MyOuput.out* is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive. ### Linda parallel Gaussian jobs Here is a simple example G09 runscript for Linda parallel jobs including comments: <table> <tbody> <tr class=\"odd\"> <td><p>```</p> <p>```#!/bin/bash -l```</p> <p>```# Batch script to run a Gaussian 09 job on Legion using Linda with the upgraded```<br /> ```# software stack under SGE.```<br /> ```#```<br /> ```# Aug 2012```<br /> ```#```<br /> ```# Based on the Qlogic MPI and Hybrid example scripts at```<br /> ```#```<br /> ```# ```<a href=\"http://www.ucl.ac.uk/isd/staff/research_services/research-computing/services/legion-upgrade/userguide/submissionscripts\">```http://www.ucl.ac.uk/isd/staff/research_services/research-computing/services/legion-upgrade/userguide/submissionscripts```</a></p> <p>```#$ -S /bin/bash```</p> <p>```# 2. Request 12 hours of wallclock time (format hours:minutes:seconds).```<br /> ```#$ -l h_rt=12:0:0```</p> <p>```# 3. Request 4 gigabyte of RAM.```<br /> ```#$ -l mem=4G```</p> <p>```# 4. Set the name of the job.```<br /> ```#$ -N G09l_job1```</p> <p>```# 5a. Select the QLogic parallel environment (qlc) and 2 processes (Linda ```<br /> ```# workers).```<br /> ```#$ -pe qlc 2```</p> <p>```# 5b. Select number of threads per Linda worker (value of NProcShared in your```<br /> ```# Gaussian input file.```<br /> ```#$ -l thr=4```</p> <p>```# 6. Select the project that this job will run under.```<br /> ```#$ -P ```<your_project_name></p> <p>```# 7. Set the working directory to somewhere in your scratch space. This is```<br /> ```# a necessary step with the upgraded software stack as compute nodes cannot```<br /> ```# write to $HOME.```<br /> ```#```<br /> ```# Note: this directory MUST exist before your job starts!```<br /> ```#$ -wd /home/```<your_UCL_id>```/Scratch/G09_output```</p> <p>```# 8. Run our G09 Linda job.```</p> <p>```# Setup G09 runtime environment```</p> <p>```module load gaussian/g09_c01_linda/pgi```<br /> ```source $g09root/g09/bsd/g09.profile```<br /> ```mkdir -p $GAUSS_SCRDIR```</p> <p>```# Pre-process G09 input file to include nodes alocated to job```</p> <p>```echo &quot;Running: lindaConv $g09infile $JOB_ID $TMPDIR/machines&quot;```<br /> ```echo ''```<br /> ```$lindaConv $g09infile $JOB_ID $TMPDIR/machines```</p> <p>```# Run g09 job```</p> <p>```echo &quot;GAUSS_SCRDIR = $GAUSS_SCRDIR&quot;```<br /> ```echo &quot;&quot;```<br /> ```echo &quot;Running: g09 &lt; job$JOB_ID.com &gt; $g09outfile&quot;```</p> <p>```# communication needs to be via ssh not the Linda default```<br /> ```export GAUSS_LFLAGS='-v -opt &quot;Tsnet.Node.lindarsharg: ssh&quot;'```</p> <p>```time g09 &lt; job$JOB_ID.com &gt; $g09outfile```</p> <p>```</p></td> </tr> </tbody> </table> This script is more complicated that the shared memory example as your Gaussian input file needs to be preprocessed to insert information about the nodes SGE has allocated to the job. This is available on Legion in: ``` `/shared/ucl/apps/Gaussian/G09_C01_L/run-g09-linda.sh` Please copy if you wish and edit it to suit your jobs. You will need to change the -P and -wd /home/ /Scratch/G09_output SGE directives and may need to change the memory, wallclock time, number of Linda workers (the -pe directive), number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be: ``qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09-linda.sh`` where Mydata.com is the file containing your G09 commands and MyOuput.out is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.","title":"Shared memory Gaussian jobs"},{"location":"Wiki_Export/software/unedited/Running_Gaussian_09_on_Legion/#submitting-long-gaussian-jobs","text":"It is possible to obtain permission to submit single node Gaussian jobs with wallclock times between 2 and 7 days. For details of how to gain access to the 7-day Gaussian queue see Requests for Additional Resources . As the 7-day queue is restricted to shared memory Gaussian jobs you will need to make some changes to your runscripts: Include the grid engine directive: #$ -ac app=g09 for Gaussian 09 jobs. If the directive is not present, normal job wallclock limits apply. The way Gaussian is launched needs to be modified as the new queues launch g09 via a new wrapper command. The new wrapper is G09 - note the capital G! It takes arguments for Gaussian command (standard input) and output (standard output) files so need to be used like so: G09 commands.in output.out where commands.in is the file containing your Gaussian commands and output.out is the file where standard output will appear. The G03 wrapper is used in the same way. Here is a simple Gaussian 09 runscript using the new '7-day' queue: ``` ```#!/bin/bash -l``` ```# Batch script to run a long Gaussian 09 shared memory job on Legion using ``` ```# the restricted '7-day' queue under SGE.``` ```#``` ```# Aug 2012``` ```#``` ```# Based on openmp.sh by:``` ```#``` ```# Owain Kenway, Research Computing, 16/Sept/2010``` ```#$ -S /bin/bash``` ```# 1. Request 168 hours of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=168:0:0``` ```# 2. Request 4 gigabyte of RAM.``` ```#$ -l mem=8G``` ```#$ -ac app=g09``` ```# 3. Set the name of the job.``` ```#$ -N G09_jobR``` ```# 4. Select 12 OpenMP threads (the most possible on Legion).``` ```#$ -l thr=12``` ```# 5. Select the project that this job will run under.``` ```#$ -P ``` ```# 6. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```#``` ```# Note: this directory MUST exist before your job starts!``` ```#$ -wd /home/``` ```/Scratch/G09_output``` ```# Run g09 job``` ```echo \"GAUSS_SCRDIR = $GAUSS_SCRDIR\"``` ```echo \"\"``` ```echo \"Running: G09 $g09infile $g09outfile\"``` ```time G09 $g09infile $g09outfile``` ``` This is available on Legion in: ``` /shared/ucl/apps/Gaussian/G09_C01_L/run-g09-res.sh Please copy if you wish and edit it to suit your jobs. You will need to change the *-P <your_project_name>* and *-wd /home/<your_UCL_id>/Scratch/G09\\_output* SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be: qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09-res.sh ``` where Mydata.com is the file containing your G09 commands and MyOuput.out is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive. Category:Bash script pages Category:Legion","title":"Submitting Long Gaussian Jobs"},{"location":"Wiki_Export/software/unedited/Running_LAMMPS_on_Legion/","text":"The build of LAMMPS on Legion was build with double precision FFTW, the OpenMPI library and the Intel 13.0 compilers. It is therefore strongly recommended that you have these modules loaded when running it: sge/6.2u3 compilers/intel/13.0/028_cxx11 mpi/openmpi/1.4.5/intel.13.0 fftw/2.1.5/double/intel.13.0 lammps/7Jun13/openmpi/intel.13.0 The last four of those modules are not loaded by default and (will conflict with default loaded modules), but may be loaded in your job script. If you are using the default modules remember to unload them in your script before loading the modules above. An example job script for LAMMPS is shown below: ``` ```#!/bin/bash -l``` ```# Batch script to run an MPI parallel job on Legion with the upgraded software``` ```# stack under SGE with OpenMPI.``` ```# 1. Force bash as the executing shell.``` ```#$ -S /bin/bash``` ```# 2. Request one hour of wallclock time (format hours:minutes:seconds).``` ```#$ -l h_rt=1:00:00``` ```# 3. Request 1 gigabyte of RAM per process.``` ```#$ -l mem=1G``` ```# 4. Set the name of the job.``` ```#$ -N ExampleLAMMPS``` ```# 5. Select the QLogic parallel environment and 24 processes.``` ```#$ -pe openmpi 24``` ```# 6. Select the project that this job will run under.``` ```# Find ``` ``` by running the command \"groups\"``` ```#$ -P ``` ```# 7. Set the working directory to somewhere in your scratch space. This is``` ```# a necessary step with the upgraded software stack as compute nodes cannot``` ```# write to $HOME.``` ```# Replace \"``` ```\" with your UCL user ID.``` ```#$ -wd /home/``` ```/Scratch/lammps``` ```# 8. Load required modules ``` ```# If you have default modules loaded, uncomment lines below:``` ```# module remove default-modules``` ```# module load sge``` ```module load compilers/intel/13.0/028_cxx11``` ```module load mpi/openmpi/1.4.5/intel.13.0``` ```module load fftw/2.1.5/double/intel.13.0``` ```module load lammps/7Jun13/openmpi/intel.13.0``` ```# 9. Run our MPI job. Replace \"inputfile\" with the name of your LAMMPS input file.``` ```gerun `which lmp_legion` -in inputfile``` ``` Category:Bash script pages Category:Legion","title":"Running LAMMPS on Legion"},{"location":"Wiki_Export/software/unedited/Running_ROOT_on_Legion/","text":"ROOT has been installed on Legion primarily because it is required by a number of R add-on packages. It is also available for batch use in its own right and for short interactive runs (less than 15 minutes execution time) on the Login nodes. A number of versions are available including 5.34.14 and 5.34.09. You need to load the following modules to use ROOT: ``` module unload compilers/intel/11.1/072 module unload mpi/qlogic/1.2.7/intel module unload mkl/10.2.5/035 module add compilers/gnu/4.6.3 module add fftw/3.3.1/double/gnu.4.6.3 module load gsl/1.15/gnu.4.6.3 module load root/5.34.14/gnu.4.6.3 ROOT can now be run interactively using: root or in batch mode running a script: root -b -q myMacro.C > myMacro.out In the above example the ROOT script is read from file *myMacro.C* and output saved to file *myMacro.out*. Type: man root ``` for further details about the root command. Extensive documentation is available on the ROOT website . Category:Bash script pages Category:Legion","title":"Running ROOT on Legion"},{"location":"Wiki_Export/software/unedited/Running_TractoR_on_Legion/","text":"TractoR (Tractography with R) is an R application for reading, writing, analysing and visualising magnetic resonance images stored in Analyze, NIfTI and DICOM file formats. It also contains functions specifically designed for working with diffusion MRI and tractography, including a standard implementation of the neighbourhood tractography approach to white matter tract segmentation. TractoR is developed at UCL by Jonathan Clayden and colleagues. Two versions of TractoR are available on Legion - 2.4.2 and 2.2.1. TractoR packages can be used from within an R session. In addition a control script is provided that allows using TractoR without interacting with R. On Legion TractoR is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer interactive tests on the User Test Nodes. To use TractoR version 2.4.2 in either mode (within R or using the control script) you need to load the following modules: ``` module unload compilers/intel/11.1/072 module unload mpi/qlogic/1.2.7/intel module unload mkl/10.2.5/035 module load recommended/r module load fsl/5.0.2.2/gnu.4.6.3 module load tractor/2.4.2 source $FSLDIR/etc/fslconf/fsl.sh To use the older version 2.2.1 replace the FSL and TractoR modules in the above with: module load fsl/5.0.1/gnu.4.6.3 module load tractor/2.2.1 TractoR also uses the FMRIB Software Library (FSL) hence the need to source its setup script. You should now be able to run a TractoR command, for example: tractor list ``` which should display the list of available commands: `Starting TractoR environment...` `Experiment scripts found in /shared/ucl/apps/R/TractoR/tractor-2.2.1/share/experiments:` ` [1] age bedpost binarise camino2fsl` ` [5] caminofiles chfiletype clone contact` ` [9] dicomread dicomsort dicomtags dirviz` `[13] dpreproc extract fsl2camino gmap` `[17] gmean gradcheck gradread gradrotate` `[21] hnt-eval hnt-interpret hnt-ref hnt-viz` `[25] identify imageinfo imagestats list` `[29] mean mkroi morph mtrack` `[33] peek platform plotcorrections pnt-collate` `[37] pnt-data pnt-data-sge pnt-em pnt-eval` `[41] pnt-interpret pnt-prune pnt-ref pnt-train` `[45] pnt-viz proj ptrack rtrack` `[49] slice smooth status streamlines2trk` `[53] tensorfit track update upgrade` `[57] values view` Experiment completed with 0 warning(s) and 0 error(s) ``` Here is an example run script using the command script mode for submitting batch jobs to the cluster: <table> <tbody> <tr class=\"odd\"> <td><p>```</p> <p>```#!/bin/bash -l```</p> <p>```# Batch script to run an OpenMP threaded TractoR job on Legion with the upgraded```<br /> ```# software stack under SGE. ```</p> <p>```# This version works with the modules environment upgraded in Feb 2012.```</p> <p>```# TractoR Version 2.4.2```</p> <p>```# 1. Force bash as the executing shell.```<br /> ```#$ -S /bin/bash```</p> <p>```# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).```<br /> ```# Change this to suit your requirements.```<br /> ```#$ -l h_rt=0:10:0```</p> <p>```# 3. Request 1 gigabyte of RAM. Change this to suit your requirements.```<br /> ```#$ -l mem=1G```</p> <p>```# 4. Set the name of the job. You can change this if you wish.```<br /> ```#$ -N TractoR_job_1```</p> <p>```# 5. Select 12 threads (the most possible on Legion). ```<br /> ```#$ -l thr=12```</p> <p>```# 6. Select the project that this job will run under.```<br /> ```# Find ```<your_project_id>``` by running the command &quot;groups&quot;```<br /> ```#$ -P ```<your_project_id></p> <p>```# 7. Set the working directory to somewhere in your scratch space. This is```<br /> ```# a necessary step with the upgraded software stack as compute nodes cannot```<br /> ```# write to $HOME.```<br /> ```#```<br /> ```# NOTE: this directory must exist.```<br /> ```#```<br /> ```# Replace &quot;```<your_UCL_id>```&quot; with your UCL user ID :)```<br /> ```#$ -wd /home/```<your_UCL_id>```/Scratch/TractoR_output```</p> <p>```# 8. Load correct modules for TractoR and R```<br /> ```module unload compilers/intel/11.1/072```<br /> ```module unload mpi/qlogic/1.2.7/intel```<br /> ```module unload mkl/10.2.5/035```<br /> ```module load recommended/r```<br /> ```module load fsl/5.0.2.2/gnu.4.6.3```<br /> ```module load tractor/2.4.2```<br /> ```source $FSLDIR/etc/fslconf/fsl.sh```</p> <p>```# 9. Run TractoR commands - example from ```<br /> ```# ```<a href=\"http://www.tractor-mri.org.uk/HNT-tutorial\">```http://www.tractor-mri.org.uk/HNT-tutorial```</a><br /> ```cd $TMPDIR```<br /> ```cp -r $TRACTOR_HOME/tests/data/session-12dir/ .```<br /> ```mkdir tmp```<br /> ```tractor hnt-eval SessionList:session-12dir TractName:genu SearchWidth:7```<br /> ```echo ''```<br /> ```echo '--------------------------------------------'```<br /> ```echo ''```<br /> ```tractor hnt-viz SessionList:session-12dir TractName:genu ResultsName:results CreateVolumes:true```<br /> ```echo ''```<br /> ```echo '--------------------------------------------'```<br /> ```echo ''```<br /> ```tractor mean genu_session1 session-12dir Metric:FA AveragingMode:binary```<br /> ```echo ''```<br /> ```echo '--------------------------------------------'```<br /> ```echo ''```</p> <p>```# 10. Preferably, tar-up (archive) all output files onto the shared scratch area```<br /> ```# this will include the R_output file above.```<br /> ```tar zcvf $HOME/Scratch/TractoR_output/files_from_job_$JOB_ID.tgz $TMPDIR```</p> <p>```# Make sure you have given enough time for the copy to complete! ```</p> <p>```</p></td> </tr> </tbody> </table> A copy of this runscript is available on Legion in: ``` `/shared/ucl/apps/R/TractoR/run-TractoR.sh` ``` Please copy if you wish and edit it to suit your jobs. You will need to change the *-P <your_project_id>* and *-wd /home/<your_UCL_id>/Scratch/TractoR\\_output* grid engine directives and the TractoR commands. You may also need to change the *-l thr=12*, *-l mem=1G* and *-l h\\_rt=0:10:0* directives. The script can be submitted using the simplest form of the qsub command ie: ``` `qsub run-TractoR.sh` Output will be written to $TMPDIR and so will need to be copied back to your ~/Scratch directory - step 10 in the runscript. Category:Bash script pages Category:Legion","title":"Running TractoR on Legion"}]}