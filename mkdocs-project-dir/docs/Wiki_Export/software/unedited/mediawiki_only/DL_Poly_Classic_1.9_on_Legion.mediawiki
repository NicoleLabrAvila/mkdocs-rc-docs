---
title: DL Poly Classic on Legion
categories:
 - Legion

layout: application
---
{{Applications
|name=DL Poly Classic
|platform=Legion
|short=
DL_Poly Classic is the open source release of DL_Poly 2, a classical molecular dynamics package. 
|about=
DL_Poly Classic is the open source release of DL_Poly 2, a classical molecular dynamics package. 
|version=1.9
|link1 = [http://ccpforge.cse.rl.ac.uk/gf/project/dl_poly_classic/ DL Poly Classic at CCP Forge]
|moduleunload1=
|moduleunload2=
|moduleunload3=
|moduleunload4=
|moduleunload5=
|moduleunload6=
|moduleunload7=
|moduleunload8=
|moduleload1=
|moduleload2=
|moduleload3=
|moduleload4=
|moduleload5=
|moduleload6=
|moduleload7=
|moduleload8=
|setup-notes=
A build of DL_Poly classic (the open source version of DL_Poly 2) is available to users on Legion. It has been built with the default modules environment (Intel compilers and Intel MPI). If you wish to use the Java GUI, you will have to make sure that you have set up X11 forwarding and added a Java module to your environment. This has not been tested!

DL_Poly looks for its input files in the directory which its executable has been placed. Since this is not practical in a multi-user environment, it is necessary for a user to copy the DL_Poly executable to their working directory before running it. The script below does this as part of the job, as well as creating a job-specific working directory. In addition, many of the scripts provided with the DL_Poly distribution that live in the execute subdirectory have had the execute bit removed so that they cannot run (they would not work anyway as users do not have permission to write to this directory). 

Please read and understand the DL_Poly Classic license. A copy of it may be found on Legion in:

<code>
 /shared/ucl/apps/dl_poly/classic/1.9/intel-2015-update2/LICENCE.pdf
</code>

More advanced users may wish to create their own scripts and work-flows around the DL_Poly binary. This binary may be found in:

<code>
 /shared/ucl/apps/dl_poly/classic/1.9/intel-2015-update2/execute/DLPOLY.X
</code>

(or do a <code> module show</code> on the DL_POLY Classic module to see where the execute directory is.
|script=
<nowiki>
#!/bin/bash -l

# Batch script to run an MPI DL_POLY job on Legion with the upgraded 
# software stack under SGE. Updated Oct 2015.

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request thirty minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=0:30:00

# 3. Request 1 gigabyte of RAM per core.
#$ -l mem=1G

# 4. Set the name of the job.
#$ -N DL_POLY_JOB

# 5. Select the MPI parallel environment and 16 processors.
#$ -pe mpi 16

# 7. Set the working directory to somewhere in your scratch space.  This 
# is a necessary step as compute nodes cannot write to $HOME.  This should 
# be set to the directory where your DL_Poly input files are, and your output files
# will be written to the same directory.
#$ -wd /home/<your username>/Scratch/DLP_Job

# 8. Now we need to set up and run our DL_Poly job.  DL_Poly is a bit 
# odd, so we need to move files about to make things run.

dl_poly_work_dir=$SGE_O_WORKDIR/${JOB_NAME}_${JOB_ID}
dl_poly_executable=/shared/ucl/apps/dl_poly/classic/1.9/intel-2015-update2/execute/DL_POLY.X

# Make a working directory.
mkdir $dl_poly_work_dir

# Copy DL_Poly input files to temporary directory.
for a in CONTROL FIELD CONFIG TABLE TABEAM
do
   if [ -f $SGE_O_WORKDIR/$a ]; then
     echo Copying $SGE_O_WORKDIR/$a to $dl_poly_work_dir/$a
     cp $SGE_O_WORKDIR/$a $dl_poly_work_dir/$a
   else
     echo No $SGE_O_WORKDIR/$a found.
  fi
done

# Copy DL_Poly executable to temporary working directory.
echo Copying DL_Poly executable to $dl_poly_work_dir
cp $dl_poly_executable $dl_poly_work_dir

# Run it. Gerun is a wrapper for mpirun.
cd $dl_poly_work_dir
echo Running $dl_poly_work_dir/DLPOLY.X.
gerun $dl_poly_work_dir/DLPOLY.X

# Delete executable.
rm $dl_poly_work_dir/DLPOLY.X
</nowiki>
|script-path=
|script-notes=
|script2=
|script2-path=
|script2-notes=

}}