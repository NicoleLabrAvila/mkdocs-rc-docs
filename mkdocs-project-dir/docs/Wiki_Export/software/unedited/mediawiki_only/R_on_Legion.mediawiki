---
title: R
categories:
 - Applications
 - Software

layout: application
---
{{Applications

|name=R
|platform=Legion
|version=3.4.2 and 3.4.0
|short=
R (GNU S) is an integrated suite of software for data manipulation, analysis and graphical display with strong support for a wide range of statistical methods.
|about=
R versions 3.4.2 and 3.4.0 and Bioconductor 3.6, 3.5 are available on Legion, Grace and Aristotle and can be used in serial mode and shared memory parallel mode within single nodes using at most twelve processors. A wide range of additional R packages are also available. R packages Rmpi and snow are available for running multi-node R jobs. This version of R has been compiled with GCC 4.9.2 and the OpenBLAS high performance BLAS library. R is intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes.
|setup-intro=
|module-intro=Before you can use R interactively, you need to load the R module using: 
|moduleunload1=compilers
|moduleunload2=mpi
|moduleunload3=
|moduleunload4=
|moduleunload5=
|moduleunload6=
|moduleunload7=
|moduleunload8=
|moduleload1=r/recommended
|moduleload2=
|moduleload3=
|moduleload4=
|moduleload5=
|moduleload6=
|moduleload7=
|moduleload8=
|setup-notes=

The above will load R 3.4.2 and Bioconductor 3.6. If you want R 3.4.0 and Bioconductor 3.5 you can load:
<code>
 module load r/old
</code>

You can check that the modules are loaded using:
<code>
 module list
</code>
You should now be able to start R normally.
|script-title=
|script-intro=
To submit batch jobs to the cluster you will need a runscript. Here is a simple example R runscript for serial jobs including comments:
|script=
<nowiki>
#!/bin/bash -l

# Batch script to run a serial R job on Legion with the upgraded
# software stack under SGE. This version works with the modules
# environment upgraded in 2015.

# R Version 3.4.2

# 1. Request ten minutes of wallclock time (format hours:minutes:seconds).
#    Change this to suit your requirements.
#$ -l h_rt=0:10:0

# 2. Request 1 gigabyte of RAM. Change this to suit your requirements.
#$ -l mem=1G

# 3. Set the name of the job. You can change this if you wish.
#$ -N R_job_1

# 4. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to your $HOME.
#
# NOTE: this directory must exist.
#
# Replace "<your_UCL_id>" with your UCL user ID :)
#$ -wd /home/<your_UCL_id>/Scratch/R_output
 
# 5. Your work *must* be done in $TMPDIR 
cd $TMPDIR

# 6. Run your R program.
module unload compilers
module unload mpi
module load r/recommended

R --no-save < /home/username/Scratch/myR_job.R > myR_job.out

# 7. Preferably, tar-up (archive) all output files onto the shared scratch area
#    this will include the R_output file above.
tar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR

# Make sure you have given enough time for the copy to complete!
</nowiki>
|script-path=
|script-notes=
Please copy if you wish and edit it to suit your jobs. You will need to change the ''-wd /home/<your_UCL_id>/Scratch/R_output'' SGE directive and may need to change the memory, wallclock time and job name directives as well. A suitable qsub command to submit a single R job using this runscript would be:
<code>
 qsub run-R.sh
</code>

where ''myR_job.R'' is the file containing R commands and ''myR_job.out'' is the output file containing your results. In this example input, and your runscript files are in Scratch. The output file is saved in the tar archive produced by the last  command in the runscript and will be in $HOME/Scratch/R_output.
|script2-title=Shared memory parallel jobs
|script2-intro=
Here is an example runscript for running a shared memory parallel job:
|script2=
<nowiki>
#!/bin/bash -l

# Batch script to run an OpenMP threaded R job on Legion with the upgraded
# software stack under SGE. Using the forech packages foreach(...) %dopar%
# for example. 

# This version works with the modules environment upgraded in 2015.

# R Version 3.4.2

# 1. Request ten minutes of wallclock time (format hours:minutes:seconds).
#    Change this to suit your requirements.
#$ -l h_rt=0:10:0

# 2. Request 1 gigabyte of RAM. Change this to suit your requirements.
#$ -l mem=1G

# 3. Set the name of the job. You can change this if you wish.
#$ -N R_jobMC_2

# 4. Select 12 threads. The number of threads here
#    must equal the number of worker processes in the registerDoMC call in your
#    R program.
#$ -pe smp 12

# 5. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
#
# NOTE: this directory must exist.
#
# Replace "<your_UCL_id>" with your UCL user ID :)
#$ -wd /home/<your_UCL_id>/Scratch/R_output

# 6. Your work *must* be done in $TMPDIR 
cd $TMPDIR

# 7. Run your R program.
module unload compilers
module unload mpi
module load r/recommended

R --no-save < /home/username/Scratch/myR_job.R > myR_job.out

# 8. Preferably, tar-up (archive) all output files onto the shared scratch area
#    this will include the R_output file above.
tar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR

# Make sure you have given enough time for the copy to complete!
</nowiki>
|script2-path= 
|script2-notes=
Again you may take a copy and modify to suit your jobs. You will need to change the ''-pe smp 12'' directive to match the number of R worker processes, the ''-wd /home/<your_UCL_id>/Scratch/R_output'' SGE directive and may need to change the memory, wallclock time and job name directives as well. A suitable qsub command to submit a single R job using this runscript would be:
<code>
 qsub run-R-MC.sh
</code>
where ''myR_job.R'' is the file containing R commands and ''myR_job.out'' is the output file containing your results. In this example input, and your runscript files are in Scratch. The output file is saved in the directory specified by the ''-wd'' SGE directive.
|script3-title=Multi-node Parallel Jobs using Rmpi and snow 
|script3-intro=
Here is an example R runscript for multi-node parallel MPI jobs using the R packages Rmpi and snow:
|script3=
<nowiki>
#!/bin/bash -l

# Batch script to run an R MPI parallel job on Legion or Grace with the upgraded software
# stack under SGE with OpenMPI.

# R Version 3.4.2

# 1. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=0:10:0

# 2. Request 1 gigabyte of RAM per process.
#$ -l mem=1G

# 3. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)
#$ -l tmpfs=15G

# 4. Set the name of the job.
#$ -N snow_monte_carlo

# 5. Select the MPI parallel environment with 32 processes
#$ -pe mpi 32

# 6. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID :)
#$ -wd /home/<your_UCL_id>/Scratch/R_examples/snow

module unload compilers mpi
module load r/recommended

cp ~/R/Examples/snow_example.R .
cp ~/R/Examples/monte_carlo.R .

# 7. Run our MPI job.  GERun is a wrapper that launches MPI jobs on Legion  
gerun RMPISNOW < snow_example.R > snow.out.${JOB_ID}

</nowiki>
|script3-path=
|script3-notes=
Please copy if you wish and edit it to suit your jobs. You will need to change the ''-wd /home/<your_UCL_id>/Scratch/R_examples/snow'' SGE directive and may need to change the pe mpi,  memory, wallclock time and job name directives as well. A suitable qsub command to submit a single R job using this runscript would be:
<code>
 qsub run-R-snow.sh
</code>
The output file is saved in <code>$HOME/Scratch/R_examples/snow/snow.out.${JOB_ID}</code>. Here is an example R script using Rmpi and snow that can be used with the above runscript:
 <nowiki>
# Load the snow and random number package.
library(snow)
library(Rmpi)

# This example uses the already installed LEcuyers RNG library(rlecuyer)
library(rlecuyer)

# Set up our input/output
source('./monte_carlo.R')
sink('./monte_carlo_output.txt')

# Get a reference to our snow cluster that has been set up by the RMPISNOW
# script.
cl <- getMPIcluster ()

# Display info about each process in the cluster
print(clusterCall(cl, function() Sys.info()))

# Load the random number package on each R process
clusterEvalQ (cl, library (rlecuyer))

# Generate a seed for the pseudorandom number generator, unique to each
# processor in the cluster.

#Uncomment below line for default (unchanging) random number seed.
#clusterSetupRNG(cl, type = 'RNGstream')

#The lines below set up a time-based random number seed.  Note that 
#this only demonstrates the virtues of changing the seed; no guarantee
#is made that this seed is at all useful.  Comment out if you uncomment
#the above line.
s <- sum(strtoi(charToRaw(date()), base = 32))
clusterSetupRNGstream(cl, seed=rep(s,6))

#Choose which of the following blocks best fit your own needs.

# BLOCK 1
# Set up the input to our Monte Carlo function.
# Input is identical across the batch, only RNG seed has changed. 
# For this example, both clusters will roll one die. 

nrolls <- 2
print("Roll the dice once...")
output <- clusterCall(cl, monte_carlo, nrolls)
output
print("Roll the dice again...")
output <- clusterCall(cl, monte_carlo, nrolls)
output

# Output should show the results of two rolls of a six-sided die.

#BLOCK 2
# Input is different for each processor
print("Second example: coin flip plus 3 dice")
input <- array(1:2)  # Set up array of inputs, with each entry
input[1] <- 1        #   corresponding to one processor.
input[2] <- 3
parameters <- array(1:2)  # Set up inputs that will be used by each cluster.
parameters[1] <- 2        #   These will be passed to monte_carlo as its
parameters[2] <- 6        #   second argument.
output <- clusterApply(cl, input, monte_carlo, parameters)

# Output should show the results of a coin flip and the roll of three 
# six-sided die.

# Output the output.
output

inputStrings <- array(1:2)
inputStrings[1] <- 'abc'
inputStrings[2] <- 'def'
output <- clusterApply(cl, inputStrings, paste, 'foo')
output

#clusterEvalQ(cl, sinkWorkerOutput("snow_monte_carlo.out"))

# Clean up the cluster and release the relevant resources.
stopCluster(cl)
sink()

mpi.quit()
</nowiki>

This example is based on the one from https://www.sharcnet.ca/help/index.php/Using_R_and_MPI

==Using your own local R packages==

There are several ways to modify your R library path so you can pick up local packages that you have installed in your own space.

The easiest way is to add them to the R_LIBS environment variable (insert the correct path):
<code>
 export R_LIBS=/your/local/R/library/path:$R_LIBS
</code>
Setting that in your terminal will let you install them from inside R and should be put in your jobscript (or your .bashrc) when you submit a job using those libraries. This appends your directory to $R_LIBS rather than overwriting it so the system libraries can still be found.

You can also change the library path for a session from within R:
<code>
 .libPaths(c('~/MyRlibs',.libPaths()))
</code>
This puts your local directory at the beginning of R's search path, and means that install.packages() will automatically put packages there and the library() function will find libraries in your local directory.

To install, after setting your library path:

From inside R, you can do
<code>
 install.packages('package_name', repos="http://cran.r-project.org")
</code>

Or if you have downloaded the tar file, you can do
<code>
 R CMD INSTALL -l /home/username/your_R_libs_directory package.tar.gz
</code>

If you want to keep some libraries separate, you can have multiple colon-separated paths in your $R_LIBS and specify which one you want to install into with R CMD INSTALL.

===BioConductor===

If you are installing extra packages for BioConductor, check that you are using the same version that Legion has at [[R_packages_available_on_Legion#Bioconductor_Packages_installed_on_RC_Systems]]

Eg. you can find the [http://www.bioconductor.org/packages/3.6/BiocViews.html#___Software BioConductor 3.6 package downloads here].

|coda-title=
|coda=
R can be used in more complicated ways including for example using one R job to control the submission of other R jobs or submitting 1000s of jobs using the SGE Array job facility. This is beyond the scope of this introductory how to.

}}