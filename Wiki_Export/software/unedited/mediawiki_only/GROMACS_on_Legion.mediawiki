---
title: GROMACS
categories:
 - Applications

layout: application
---
{{Applications

|name=GROMACS
|platform=Legion
|short=
GROMACS is an open source molecular dynamics package. Both double and single precision builds of GROMACS are provided.
|about=
GROMACS is an open source molecular dynamics package. Both double and single precision builds of GROMACS are provided. 

==GROMACS versions==
There are several versions of GROMACS available on the Research Computing software stack, built with different module combinations. Type <code>module avail gromacs</code> to see them all. You can see the prerequisite modules with <code>module show <modulename></code>. Some are built with Plumed and matheval support.

Which executable you should run depends on the problem you wish to solve.  For both single and double precision version builds, serial binaries and an MPI binary for mdrun (mdrun_mpi or gmx_mpi depending on version) are provided. Double precision binaries have a _d suffix (so gmx_d, mdrun_mpi_d, gmx_mpi_d etc).
 
|version=5.0.4, 5.1.1, 5.1.3, 2016.3, 2016.4, 2018
|link1= [http://www.gromacs.org/ GROMACS web site]

{{Script
|script-title=GROMACS 5.1.1
|script-intro=GROMACS 5.1.1 was built with the Intel 2015 compilers, MKL and Intel MPI.
|script-code=
<nowiki>
#!/bin/bash -l

# Batch script to run an MPI parallel GROMACS job with the upgraded software
# stack under SGE with Intel MPI.

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=0:10:0

# 3. Request 1 gigabyte of RAM per process.
#$ -l mem=1G

# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)
#$ -l tmpfs=15G

# 5. Set the name of the job.
#$ -N GROMACS_1_16

# 6. Select the MPI parallel environment and 16 processes.
#$ -pe mpi 16

# 7. Set the working directory of the job to the current directory
#     containing your input files.
#    This *has* to be somewhere in your Scratch space, or else your
#     job will go into the Eqw state.
#$ -cwd

# Using the default compiler and MPI
module load gromacs/5.1.1/intel-2015-update2

# Run GROMACS - replace with mdrun command line suitable for your job!

gerun mdrun_mpi -v -stepout 10000
</nowiki>
}}

{{Script
|script-title=GROMACS 5.1.3 with Plumed
|script-intro=GROMACS 5.1.3 was built with Plumed 2.2.3 and matheval support, the Intel 2015 compilers, OpenBLAS and Intel MPI. (Plumed did not work correctly with MKL).
|script-code=
<nowiki>
#!/bin/bash -l

# Batch script to run an MPI parallel GROMACS job with the upgraded software
# stack under SGE with Intel MPI.

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=0:10:0

# 3. Request 1 gigabyte of RAM per process.
#$ -l mem=1G

# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)
#$ -l tmpfs=15G

# 5. Set the name of the job.
#$ -N GROMACS_1_16_plumed

# 6. Select the MPI parallel environment and 16 processes.
#$ -pe mpi 16

# 7. Set the working directory of the job to the current directory
#     containing your input files.
#    This *has* to be somewhere in your Scratch space, or else your
#     job will go into the Eqw state.
#$ -cwd

# Using default compilers and MPI
module load libmatheval
module load flex
module load openblas/0.2.14/intel-2015-update2
module load plumed/2.1.2/intel-2015-update2
module load gromacs/5.1.3/plumed/intel-2015-update2

# Run GROMACS - replace with mdrun command line suitable for your job!

gerun gmx_mpi -v -stepout 10000
</nowiki>
}}

==Passing in options to GROMACS non-interactively==

Some GROMACS executables like trjconv normally take interactive input. You can't do this in a jobscript, so you need to pass in the input you would normally type in. There are several ways of doing this, mentioned at
[http://www.gromacs.org/Documentation/How-tos/Using_Commands_in_Scripts GROMACS Documentation - Using Commands in Scripts]. The simplest is to echo the input in and keep your gmx options as they would normally be. If the inputs you would normally type were 3 and 3, then you can do this:

<code>
<nowiki>
 echo 3 3 | gmx whatevercommand -options
</nowiki>
</code>

==Checkpoint and restart==

GROMACS has built-in checkpoint and restart ability, so you can use this if your runs will not complete in the maximum 48hr wallclock time.

Have a look at the GROMACS manual for full details, as there are more options than mentioned here.

You can tell GROMACS to write a checkpoint file when it is approaching the maximum wallclock time available, and then exit.

In this case, we had asked for 48hrs wallclock. This tells GROMACS to start from the last checkpoint if there is one, and write a new checkpoint just before it reaches 47 hrs runtime.

<code>
 gerun mdrun_mpi -cpi -maxh 47 <options>
</code>

The next job you submit with the same script will carry on from the checkpoint the last job wrote. You could use job dependencies to submit two identical jobs at the same time and have one dependent on the other, so it won't start until the first finishes - have a look at <code>man qsub</code> for the <code>-hold_jid</code> option.

Alternatively, you can tell it to write checkpoints at set intervals and get it to automatically resubmit your job if it runs out of time. Modify your jobscript to include the below. It will write checkpoints every 15 mins if you don't specify a time.

<code>
 # This SGE option means a signal is sent to the job shortly before the scheduler kills it.
 #$ -notify

 # This catches the signal and resubmits your job to the queue after it has run out of time.
 # When your job has properly finished, that is a signal 0 so it won't be restarted again.
 trap 'exit 99' SIGUSR2

 # load your modules as normal here
 ...

 # Write checkpoints every 120 mins, start from checkpoint if there is one.
 gerun mdrun_mpi -cpi -cpt 120 <options>

</code>

}}